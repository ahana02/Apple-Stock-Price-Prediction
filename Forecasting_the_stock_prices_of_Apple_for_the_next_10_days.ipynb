{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhcG3iw0JYlx",
        "outputId": "4a1bec6f-e565-4b16-b2da-6db06b9464ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting autots\n",
            "  Downloading AutoTS-0.5.2-py3-none-any.whl (674 kB)\n",
            "\u001b[K     |████████████████████████████████| 674 kB 29.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: statsmodels>=0.10.* in /usr/local/lib/python3.8/dist-packages (from autots) (0.12.2)\n",
            "Requirement already satisfied: pandas>=0.25.* in /usr/local/lib/python3.8/dist-packages (from autots) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from autots) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.20.* in /usr/local/lib/python3.8/dist-packages (from autots) (1.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.25.*->autots) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.25.*->autots) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25.*->autots) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.*->autots) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.*->autots) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.*->autots) (1.7.3)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.8/dist-packages (from statsmodels>=0.10.*->autots) (0.5.3)\n",
            "Installing collected packages: autots\n",
            "Successfully installed autots-0.5.2\n"
          ]
        }
      ],
      "source": [
        "%pip install autots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nJRNSusUMbRt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from autots import AutoTS\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "W11UB-CbM0tP",
        "outputId": "fd955005-d2d5-4681-fd3f-d4bbd18755db"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7f0d367e-e117-4ba4-9d17-c7836b5fd544\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-05-03</td>\n",
              "      <td>132.039993</td>\n",
              "      <td>134.070007</td>\n",
              "      <td>131.830002</td>\n",
              "      <td>132.539993</td>\n",
              "      <td>131.756897</td>\n",
              "      <td>75135100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-05-04</td>\n",
              "      <td>131.190002</td>\n",
              "      <td>131.490005</td>\n",
              "      <td>126.699997</td>\n",
              "      <td>127.849998</td>\n",
              "      <td>127.094612</td>\n",
              "      <td>137564700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-05-05</td>\n",
              "      <td>129.199997</td>\n",
              "      <td>130.449997</td>\n",
              "      <td>127.970001</td>\n",
              "      <td>128.100006</td>\n",
              "      <td>127.343140</td>\n",
              "      <td>84000900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-05-06</td>\n",
              "      <td>127.889999</td>\n",
              "      <td>129.750000</td>\n",
              "      <td>127.129997</td>\n",
              "      <td>129.740005</td>\n",
              "      <td>128.973465</td>\n",
              "      <td>78128300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-05-07</td>\n",
              "      <td>130.850006</td>\n",
              "      <td>131.259995</td>\n",
              "      <td>129.479996</td>\n",
              "      <td>130.210007</td>\n",
              "      <td>129.660553</td>\n",
              "      <td>78973300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7f0d367e-e117-4ba4-9d17-c7836b5fd544')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7f0d367e-e117-4ba4-9d17-c7836b5fd544 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7f0d367e-e117-4ba4-9d17-c7836b5fd544');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         Date        Open        High         Low       Close   Adj Close  \\\n",
              "0  2021-05-03  132.039993  134.070007  131.830002  132.539993  131.756897   \n",
              "1  2021-05-04  131.190002  131.490005  126.699997  127.849998  127.094612   \n",
              "2  2021-05-05  129.199997  130.449997  127.970001  128.100006  127.343140   \n",
              "3  2021-05-06  127.889999  129.750000  127.129997  129.740005  128.973465   \n",
              "4  2021-05-07  130.850006  131.259995  129.479996  130.210007  129.660553   \n",
              "\n",
              "      Volume  \n",
              "0   75135100  \n",
              "1  137564700  \n",
              "2   84000900  \n",
              "3   78128300  \n",
              "4   78973300  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df=pd.read_csv('AAPL.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "NS82v-H1PiF-",
        "outputId": "641d761f-4047-4493-a65a-62d4ae242e77"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-49118ab3-1092-476a-8621-72fb4d98f2b8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-05-03</td>\n",
              "      <td>132.539993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-05-04</td>\n",
              "      <td>127.849998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-05-05</td>\n",
              "      <td>128.100006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-05-06</td>\n",
              "      <td>129.740005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-05-07</td>\n",
              "      <td>130.210007</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49118ab3-1092-476a-8621-72fb4d98f2b8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-49118ab3-1092-476a-8621-72fb4d98f2b8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-49118ab3-1092-476a-8621-72fb4d98f2b8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         Date       Close\n",
              "0  2021-05-03  132.539993\n",
              "1  2021-05-04  127.849998\n",
              "2  2021-05-05  128.100006\n",
              "3  2021-05-06  129.740005\n",
              "4  2021-05-07  130.210007"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1=df[['Date','Close']]\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrQQHWcNzILs",
        "outputId": "892ac52a-e4a3-47ad-9651-242f8b433343"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Close        1.000000\n",
            "Adj Close    0.999963\n",
            "Low          0.995383\n",
            "High         0.995246\n",
            "Open         0.989744\n",
            "Volume       0.258599\n",
            "Name: Close, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "correlation = df.corr()\n",
        "print(correlation[\"Close\"].sort_values(ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Xr9TkUGJUEEb"
      },
      "outputs": [],
      "source": [
        "df1[\"Date\"] = pd.to_datetime(df1.Date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "65gJWkmjUv83",
        "outputId": "0a0e8f4d-c76a-4069-c276-b8e48d4d3309"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "df1[\"Close\"].plot(figsize=(12, 8), title=\"Apple Stock Prices\", fontsize=20, label=\"Close Price\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl2BlnrcU9bg",
        "outputId": "7882e0bd-5a15-42ad-e983-71be88e9a23d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inferred frequency is: B\n",
            "Old data dropped by `drop_data_older_than_periods`.\n",
            "Model Number: 1 with model ARIMA in generation 0 of 5\n",
            "Model Number: 2 with model ARIMA in generation 0 of 5\n",
            "Model Number: 3 with model ARIMA in generation 0 of 5\n",
            "Model Number: 4 with model AverageValueNaive in generation 0 of 5\n",
            "Model Number: 5 with model AverageValueNaive in generation 0 of 5\n",
            "Model Number: 6 with model AverageValueNaive in generation 0 of 5\n",
            "Model Number: 7 with model DatepartRegression in generation 0 of 5\n",
            "Model Number: 8 with model DatepartRegression in generation 0 of 5\n",
            "Model Number: 9 with model DatepartRegression in generation 0 of 5\n",
            "Model Number: 10 with model DatepartRegression in generation 0 of 5\n",
            "Epoch 1/50\n",
            "6/6 [==============================] - 6s 5ms/step - loss: 0.3830\n",
            "Epoch 2/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3794\n",
            "Epoch 3/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3699\n",
            "Epoch 4/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3643\n",
            "Epoch 5/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3567\n",
            "Epoch 6/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3507\n",
            "Epoch 7/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3382\n",
            "Epoch 8/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3309\n",
            "Epoch 9/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3264\n",
            "Epoch 10/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3205\n",
            "Epoch 11/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.3157\n",
            "Epoch 12/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3155\n",
            "Epoch 13/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3100\n",
            "Epoch 14/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3132\n",
            "Epoch 15/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3069\n",
            "Epoch 16/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.3031\n",
            "Epoch 17/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3061\n",
            "Epoch 18/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3066\n",
            "Epoch 19/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3044\n",
            "Epoch 20/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2987\n",
            "Epoch 21/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.3006\n",
            "Epoch 22/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2954\n",
            "Epoch 23/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2937\n",
            "Epoch 24/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2930\n",
            "Epoch 25/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.2890\n",
            "Epoch 26/50\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.2893\n",
            "Epoch 27/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2875\n",
            "Epoch 28/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2862\n",
            "Epoch 29/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.2829\n",
            "Epoch 30/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2802\n",
            "Epoch 31/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2826\n",
            "Epoch 32/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2850\n",
            "Epoch 33/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2804\n",
            "Epoch 34/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.2849\n",
            "Epoch 35/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2722\n",
            "Epoch 36/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2763\n",
            "Epoch 37/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2753\n",
            "Epoch 38/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2745\n",
            "Epoch 39/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2725\n",
            "Epoch 40/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2656\n",
            "Epoch 41/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2650\n",
            "Epoch 42/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2636\n",
            "Epoch 43/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2599\n",
            "Epoch 44/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2570\n",
            "Epoch 45/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2583\n",
            "Epoch 46/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2565\n",
            "Epoch 47/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2454\n",
            "Epoch 48/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2497\n",
            "Epoch 49/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2470\n",
            "Epoch 50/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2335\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Model Number: 11 with model ETS in generation 0 of 5\n",
            "Model Number: 12 with model ETS in generation 0 of 5\n",
            "Model Number: 13 with model GLM in generation 0 of 5\n",
            "Model Number: 14 with model GLM in generation 0 of 5\n",
            "Model Number: 15 with model GLS in generation 0 of 5\n",
            "Model Number: 16 with model GLS in generation 0 of 5\n",
            "Model Number: 17 with model GluonTS in generation 0 of 5\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 17: GluonTS\n",
            "Model Number: 18 with model GluonTS in generation 0 of 5\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 18: GluonTS\n",
            "Model Number: 19 with model GluonTS in generation 0 of 5\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 19: GluonTS\n",
            "Model Number: 20 with model GluonTS in generation 0 of 5\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 20: GluonTS\n",
            "Model Number: 21 with model GluonTS in generation 0 of 5\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 21: GluonTS\n",
            "Model Number: 22 with model LastValueNaive in generation 0 of 5\n",
            "Model Number: 23 with model LastValueNaive in generation 0 of 5\n",
            "Model Number: 24 with model LastValueNaive in generation 0 of 5\n",
            "Model Number: 25 with model LastValueNaive in generation 0 of 5\n",
            "Model Number: 26 with model SeasonalNaive in generation 0 of 5\n",
            "Model Number: 27 with model SeasonalNaive in generation 0 of 5\n",
            "Model Number: 28 with model SeasonalNaive in generation 0 of 5\n",
            "Model Number: 29 with model UnobservedComponents in generation 0 of 5\n",
            "Model Number: 30 with model UnobservedComponents in generation 0 of 5\n",
            "Model Number: 31 with model UnobservedComponents in generation 0 of 5\n",
            "Model Number: 32 with model VAR in generation 0 of 5\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 32: VAR\n",
            "Model Number: 33 with model VAR in generation 0 of 5\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 33: VAR\n",
            "Model Number: 34 with model VECM in generation 0 of 5\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 34: VECM\n",
            "Model Number: 35 with model VECM in generation 0 of 5\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 35: VECM\n",
            "Model Number: 36 with model WindowRegression in generation 0 of 5\n",
            "Model Number: 37 with model ConstantNaive in generation 0 of 5\n",
            "Model Number: 38 with model FBProphet in generation 0 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/a9ywlrtc.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/b7zt4bya.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=43230', 'data', 'file=/tmp/tmp11_dwuur/a9ywlrtc.json', 'init=/tmp/tmp11_dwuur/b7zt4bya.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_modeldthh33yc/prophet_model-20221223063403.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "06:34:03 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "06:34:03 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 39 with model GluonTS in generation 0 of 5\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 39: GluonTS\n",
            "Model Number: 40 with model MultivariateRegression in generation 0 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=-2)]: Done 200 out of 200 | elapsed:    0.3s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 41 with model MultivariateRegression in generation 0 of 5\n",
            "Template Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 41: MultivariateRegression\n",
            "Model Number: 42 with model DatepartRegression in generation 0 of 5\n",
            "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 42: DatepartRegression\n",
            "Model Number: 43 with model SeasonalNaive in generation 0 of 5\n",
            "Model Number: 44 with model DatepartRegression in generation 0 of 5\n",
            "Model Number: 45 with model UnobservedComponents in generation 0 of 5\n",
            "Model Number: 46 with model UnobservedComponents in generation 0 of 5\n",
            "Model Number: 47 with model ETS in generation 0 of 5\n",
            "Model Number: 48 with model VECM in generation 0 of 5\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 48: VECM\n",
            "Model Number: 49 with model ARDL in generation 0 of 5\n",
            "Template Eval Error: ImportError(\"cannot import name 'ARDL' from 'statsmodels.tsa.api' (/usr/local/lib/python3.8/dist-packages/statsmodels/tsa/api.py)\") in model 49: ARDL\n",
            "Model Number: 50 with model MultivariateMotif in generation 0 of 5\n",
            "Model Number: 51 with model MultivariateMotif in generation 0 of 5\n",
            "Model Number: 52 with model UnivariateMotif in generation 0 of 5\n",
            "Model Number: 53 with model UnivariateMotif in generation 0 of 5\n",
            "Model Number: 54 with model SectionalMotif in generation 0 of 5\n",
            "Model Number: 55 with model SectionalMotif in generation 0 of 5\n",
            "Model Number: 56 with model MultivariateRegression in generation 0 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/w2if7vq_.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/ecrx5oc7.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=62827', 'data', 'file=/tmp/tmp11_dwuur/w2if7vq_.json', 'init=/tmp/tmp11_dwuur/ecrx5oc7.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_modelc_rmlee_/prophet_model-20221223063406.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "06:34:06 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 57 with model FBProphet in generation 0 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "06:34:06 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 58 with model SeasonalNaive in generation 0 of 5\n",
            "Model Number: 59 with model DatepartRegression in generation 0 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 60 with model NVAR in generation 0 of 5\n",
            "Model Number: 61 with model Theta in generation 0 of 5\n",
            "Model Number: 62 with model UnivariateRegression in generation 0 of 5\n",
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float32').\") in model 62: UnivariateRegression\n",
            "Model Number: 63 with model ARCH in generation 0 of 5\n",
            "Template Eval Error: ImportError('`arch` package must be installed from pip') in model 63: ARCH\n",
            "Model Number: 64 with model ConstantNaive in generation 0 of 5\n",
            "Model Number: 65 with model LastValueNaive in generation 0 of 5\n",
            "Model Number: 66 with model AverageValueNaive in generation 0 of 5\n",
            "Model Number: 67 with model GLS in generation 0 of 5\n",
            "Model Number: 68 with model SeasonalNaive in generation 0 of 5\n",
            "Model Number: 69 with model GLM in generation 0 of 5\n",
            "Template Eval Error: ValueError('regression_type=user and no future_regressor passed') in model 69: GLM\n",
            "Model Number: 70 with model ETS in generation 0 of 5\n",
            "ETS error ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\n",
            "ETS failed on Close with ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 71 with model FBProphet in generation 0 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/a5vbkckc.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/pix1k551.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=93867', 'data', 'file=/tmp/tmp11_dwuur/a5vbkckc.json', 'init=/tmp/tmp11_dwuur/pix1k551.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_modelmzhk37xt/prophet_model-20221223063407.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "06:34:07 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "06:34:08 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 72 with model GluonTS in generation 0 of 5\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 72: GluonTS\n",
            "Model Number: 73 with model UnobservedComponents in generation 0 of 5\n",
            "Model Number: 74 with model VAR in generation 0 of 5\n",
            "No anomalies detected.\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 74: VAR\n",
            "Model Number: 75 with model VECM in generation 0 of 5\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 75: VECM\n",
            "Model Number: 76 with model ARIMA in generation 0 of 5\n",
            "Model Number: 77 with model WindowRegression in generation 0 of 5\n",
            "Model Number: 78 with model DatepartRegression in generation 0 of 5\n",
            "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 78: DatepartRegression\n",
            "Model Number: 79 with model UnivariateRegression in generation 0 of 5\n",
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float32').\") in model 79: UnivariateRegression\n",
            "Model Number: 80 with model MultivariateRegression in generation 0 of 5\n",
            "Model Number: 81 with model UnivariateMotif in generation 0 of 5\n",
            "No anomalies detected.\n",
            "Model Number: 82 with model MultivariateMotif in generation 0 of 5\n",
            "Model Number: 83 with model SectionalMotif in generation 0 of 5\n",
            "Model Number: 84 with model NVAR in generation 0 of 5\n",
            "Model Number: 85 with model Theta in generation 0 of 5\n",
            "Model Number: 86 with model ARDL in generation 0 of 5\n",
            "Template Eval Error: ImportError(\"cannot import name 'ARDL' from 'statsmodels.tsa.api' (/usr/local/lib/python3.8/dist-packages/statsmodels/tsa/api.py)\") in model 86: ARDL\n",
            "Model Number: 87 with model ARCH in generation 0 of 5\n",
            "Template Eval Error: ImportError('`arch` package must be installed from pip') in model 87: ARCH\n",
            "Model Number: 88 with model MetricMotif in generation 0 of 5\n",
            "Model Number: 89 with model MultivariateMotif in generation 0 of 5\n",
            "Model Number: 90 with model VAR in generation 0 of 5\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 90: VAR\n",
            "Model Number: 91 with model DatepartRegression in generation 0 of 5\n",
            "Template Eval Error: ValueError('Model DatepartRegression returned NaN for one or more series. fail_on_forecast_nan=True') in model 91: DatepartRegression\n",
            "Model Number: 92 with model VAR in generation 0 of 5\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 92: VAR\n",
            "Model Number: 93 with model UnivariateMotif in generation 0 of 5\n",
            "Model Number: 94 with model UnivariateRegression in generation 0 of 5\n",
            "Model Number: 95 with model AverageValueNaive in generation 0 of 5\n",
            "Model Number: 96 with model VAR in generation 0 of 5\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 96: VAR\n",
            "Model Number: 97 with model DatepartRegression in generation 0 of 5\n",
            "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 97: DatepartRegression\n",
            "Model Number: 98 with model MetricMotif in generation 0 of 5\n",
            "Model Number: 99 with model VAR in generation 0 of 5\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 99: VAR\n",
            "Model Number: 100 with model GLM in generation 0 of 5\n",
            "Model Number: 101 with model ARCH in generation 0 of 5\n",
            "Template Eval Error: ImportError('`arch` package must be installed from pip') in model 101: ARCH\n",
            "Model Number: 102 with model VAR in generation 0 of 5\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 102: VAR\n",
            "Model Number: 103 with model SeasonalNaive in generation 0 of 5\n",
            "Model Number: 104 with model UnobservedComponents in generation 0 of 5\n",
            "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 104: UnobservedComponents\n",
            "Model Number: 105 with model ConstantNaive in generation 0 of 5\n",
            "Model Number: 106 with model ETS in generation 0 of 5\n",
            "ETS error ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\n",
            "ETS failed on Close with ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\n",
            "Model Number: 107 with model ConstantNaive in generation 0 of 5\n",
            "Model Number: 108 with model VAR in generation 0 of 5\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 108: VAR\n",
            "Model Number: 109 with model ARDL in generation 0 of 5\n",
            "Template Eval Error: ImportError(\"cannot import name 'ARDL' from 'statsmodels.tsa.api' (/usr/local/lib/python3.8/dist-packages/statsmodels/tsa/api.py)\") in model 109: ARDL\n",
            "Model Number: 110 with model AverageValueNaive in generation 0 of 5\n",
            "Model Number: 111 with model NVAR in generation 0 of 5\n",
            "Model Number: 112 with model FBProphet in generation 0 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/da5yct6x.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/5j7b6555.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=76118', 'data', 'file=/tmp/tmp11_dwuur/da5yct6x.json', 'init=/tmp/tmp11_dwuur/5j7b6555.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_modeln1pk9f6o/prophet_model-20221223063414.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "06:34:14 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "06:34:14 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 113 with model SeasonalNaive in generation 0 of 5\n",
            "Model Number: 114 with model ETS in generation 0 of 5\n",
            "Model Number: 115 with model VAR in generation 0 of 5\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 115: VAR\n",
            "Model Number: 116 with model MultivariateMotif in generation 0 of 5\n",
            "Model Number: 117 with model GLM in generation 0 of 5\n",
            "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.') in model 117: GLM\n",
            "Model Number: 118 with model VECM in generation 0 of 5\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 118: VECM\n",
            "Model Number: 119 with model LastValueNaive in generation 0 of 5\n",
            "Model Number: 120 with model ARCH in generation 0 of 5\n",
            "Template Eval Error: ImportError('`arch` package must be installed from pip') in model 120: ARCH\n",
            "Model Number: 121 with model MultivariateMotif in generation 0 of 5\n",
            "Model Number: 122 with model ETS in generation 0 of 5\n",
            "Model Number: 123 with model Theta in generation 0 of 5\n",
            "Model Number: 124 with model MultivariateMotif in generation 0 of 5\n",
            "Model Number: 125 with model UnivariateMotif in generation 0 of 5\n",
            "Model Number: 126 with model GLM in generation 0 of 5\n",
            "Model Number: 127 with model ARIMA in generation 0 of 5\n",
            "Template Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 127: ARIMA\n",
            "Model Number: 128 with model VAR in generation 0 of 5\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 128: VAR\n",
            "Model Number: 129 with model Theta in generation 0 of 5\n",
            "Model Number: 130 with model SeasonalNaive in generation 0 of 5\n",
            "Model Number: 131 with model SeasonalNaive in generation 0 of 5\n",
            "Model Number: 132 with model ARCH in generation 0 of 5\n",
            "Template Eval Error: ImportError('`arch` package must be installed from pip') in model 132: ARCH\n",
            "Model Number: 133 with model VECM in generation 0 of 5\n",
            "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 133: VECM\n",
            "Model Number: 134 with model VAR in generation 0 of 5\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 134: VAR\n",
            "Model Number: 135 with model MetricMotif in generation 0 of 5\n",
            "Model Number: 136 with model VECM in generation 0 of 5\n",
            "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 136: VECM\n",
            "Model Number: 137 with model ETS in generation 0 of 5\n",
            "ETS error ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\n",
            "ETS failed on Close with ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\n",
            "Model Number: 138 with model SeasonalNaive in generation 0 of 5\n",
            "Model Number: 139 with model NVAR in generation 0 of 5\n",
            "Model Number: 140 with model SeasonalNaive in generation 0 of 5\n",
            "Model Number: 141 with model UnobservedComponents in generation 0 of 5\n",
            "Model Number: 142 with model LastValueNaive in generation 0 of 5\n",
            "Model Number: 143 with model VAR in generation 0 of 5\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 143: VAR\n",
            "Model Number: 144 with model UnivariateMotif in generation 0 of 5\n",
            "Model Number: 145 with model NVAR in generation 0 of 5\n",
            "Model Number: 146 with model VAR in generation 0 of 5\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 146: VAR\n",
            "Model Number: 147 with model GLS in generation 0 of 5\n",
            "Model Number: 148 with model Theta in generation 0 of 5\n",
            "Model Number: 149 with model UnivariateMotif in generation 0 of 5\n",
            "Model Number: 150 with model UnivariateMotif in generation 0 of 5\n",
            "Model Number: 151 with model MetricMotif in generation 0 of 5\n",
            "Model Number: 152 with model SectionalMotif in generation 0 of 5\n",
            "Template Eval Error: ValueError(\"regression_type=='User' but no future_regressor supplied\") in model 152: SectionalMotif\n",
            "Model Number: 153 with model ARCH in generation 0 of 5\n",
            "Template Eval Error: ImportError('`arch` package must be installed from pip') in model 153: ARCH\n",
            "Model Number: 154 with model ARDL in generation 0 of 5\n",
            "Template Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 154: ARDL\n",
            "Model Number: 155 with model SeasonalNaive in generation 0 of 5\n",
            "Model Number: 156 with model MultivariateMotif in generation 0 of 5\n",
            "Model Number: 157 with model ARCH in generation 0 of 5\n",
            "Template Eval Error: ImportError('`arch` package must be installed from pip') in model 157: ARCH\n",
            "Model Number: 158 with model MetricMotif in generation 0 of 5\n",
            "Model Number: 159 with model UnivariateMotif in generation 0 of 5\n",
            "Model Number: 160 with model MultivariateMotif in generation 0 of 5\n",
            "Model Number: 161 with model UnivariateMotif in generation 0 of 5\n",
            "Model Number: 162 with model VECM in generation 0 of 5\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 162: VECM\n",
            "Model Number: 163 with model MultivariateMotif in generation 0 of 5\n",
            "Model Number: 164 with model UnivariateMotif in generation 0 of 5\n",
            "Model Number: 165 with model ARIMA in generation 0 of 5\n",
            "Template Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 165: ARIMA\n",
            "Model Number: 166 with model AverageValueNaive in generation 0 of 5\n",
            "Model Number: 167 with model MultivariateMotif in generation 0 of 5\n",
            "Model Number: 168 with model Theta in generation 0 of 5\n",
            "Model Number: 169 with model GLM in generation 0 of 5\n",
            "Model Number: 170 with model UnivariateMotif in generation 0 of 5\n",
            "Model Number: 171 with model ARDL in generation 0 of 5\n",
            "Template Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 171: ARDL\n",
            "Model Number: 172 with model ARCH in generation 0 of 5\n",
            "Template Eval Error: ImportError('`arch` package must be installed from pip') in model 172: ARCH\n",
            "Model Number: 173 with model GLM in generation 0 of 5\n",
            "No anomalies detected.\n",
            "Model Number: 174 with model ARDL in generation 0 of 5\n",
            "Template Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 174: ARDL\n",
            "Model Number: 175 with model GluonTS in generation 0 of 5\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 175: GluonTS\n",
            "Model Number: 176 with model ARIMA in generation 0 of 5\n",
            "Model Number: 177 with model ARIMA in generation 0 of 5\n",
            "Model Number: 178 with model ConstantNaive in generation 0 of 5\n",
            "Model Number: 179 with model GLM in generation 0 of 5\n",
            "Template Eval Error: PerfectSeparationError('Perfect separation detected, results not available') in model 179: GLM\n",
            "Model Number: 180 with model UnobservedComponents in generation 0 of 5\n",
            "Model Number: 181 with model SeasonalNaive in generation 0 of 5\n",
            "Model Number: 182 with model LastValueNaive in generation 0 of 5\n",
            "Model Number: 183 with model DatepartRegression in generation 0 of 5\n",
            "Model Number: 184 with model ARIMA in generation 0 of 5\n",
            "Template Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 184: ARIMA\n",
            "Model Number: 185 with model MultivariateRegression in generation 0 of 5\n",
            "Model Number: 186 with model AverageValueNaive in generation 0 of 5\n",
            "Model Number: 187 with model ETS in generation 0 of 5\n",
            "Model Number: 188 with model ETS in generation 0 of 5\n",
            "ETS failed on Close with ValueError('could not broadcast input array from shape (190,) into shape (364,)')\n",
            "New Generation: 1 of 5\n",
            "Model Number: 189 with model MultivariateRegression in generation 1 of 5\n",
            "Model Number: 190 with model LastValueNaive in generation 1 of 5\n",
            "Model Number: 191 with model DatepartRegression in generation 1 of 5\n",
            "Model Number: 192 with model AverageValueNaive in generation 1 of 5\n",
            "Model Number: 193 with model LastValueNaive in generation 1 of 5\n",
            "Template Eval Error: Exception('Transformer BTCD failed on fit') in model 193: LastValueNaive\n",
            "Model Number: 194 with model LastValueNaive in generation 1 of 5\n",
            "Model Number: 195 with model UnobservedComponents in generation 1 of 5\n",
            "Model Number: 196 with model MultivariateMotif in generation 1 of 5\n",
            "Model Number: 197 with model SectionalMotif in generation 1 of 5\n",
            "Model Number: 198 with model UnobservedComponents in generation 1 of 5\n",
            "Model Number: 199 with model MultivariateMotif in generation 1 of 5\n",
            "Model Number: 200 with model DatepartRegression in generation 1 of 5\n",
            "Model Number: 201 with model Theta in generation 1 of 5\n",
            "Model Number: 202 with model MultivariateMotif in generation 1 of 5\n",
            "Model Number: 203 with model ConstantNaive in generation 1 of 5\n",
            "Model Number: 204 with model AverageValueNaive in generation 1 of 5\n",
            "Model Number: 205 with model UnobservedComponents in generation 1 of 5\n",
            "Model Number: 206 with model MultivariateRegression in generation 1 of 5\n",
            "Template Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 206: MultivariateRegression\n",
            "Model Number: 207 with model MultivariateMotif in generation 1 of 5\n",
            "Model Number: 208 with model Theta in generation 1 of 5\n",
            "Model Number: 209 with model MultivariateMotif in generation 1 of 5\n",
            "Model Number: 210 with model AverageValueNaive in generation 1 of 5\n",
            "Model Number: 211 with model ARIMA in generation 1 of 5\n",
            "Model Number: 212 with model AverageValueNaive in generation 1 of 5\n",
            "Model Number: 213 with model UnivariateMotif in generation 1 of 5\n",
            "Model Number: 214 with model ARIMA in generation 1 of 5\n",
            "Model Number: 215 with model AverageValueNaive in generation 1 of 5\n",
            "Model Number: 216 with model UnobservedComponents in generation 1 of 5\n",
            "Template Eval Error: ValueError('Model UnobservedComponents returned NaN for one or more series. fail_on_forecast_nan=True') in model 216: UnobservedComponents\n",
            "Model Number: 217 with model Theta in generation 1 of 5\n",
            "Model Number: 218 with model UnobservedComponents in generation 1 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/dev7p4_q.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/vkepw9px.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=41968', 'data', 'file=/tmp/tmp11_dwuur/dev7p4_q.json', 'init=/tmp/tmp11_dwuur/vkepw9px.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_modelwmfgv0hk/prophet_model-20221223063429.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "06:34:29 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 219 with model FBProphet in generation 1 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "06:34:29 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 220 with model UnivariateRegression in generation 1 of 5\n",
            "Model Number: 221 with model SectionalMotif in generation 1 of 5\n",
            "Model Number: 222 with model GLM in generation 1 of 5\n",
            "Model Number: 223 with model LastValueNaive in generation 1 of 5\n",
            "Model Number: 224 with model ETS in generation 1 of 5\n",
            "ETS failed on Close with ValueError('could not broadcast input array from shape (190,) into shape (364,)')\n",
            "Model Number: 225 with model SectionalMotif in generation 1 of 5\n",
            "Model Number: 226 with model GLS in generation 1 of 5\n",
            "Model Number: 227 with model Theta in generation 1 of 5\n",
            "Model Number: 228 with model MultivariateMotif in generation 1 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/2nl5s0je.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/i55eb4r6.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=98869', 'data', 'file=/tmp/tmp11_dwuur/2nl5s0je.json', 'init=/tmp/tmp11_dwuur/i55eb4r6.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_modellce6noc0/prophet_model-20221223063431.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "06:34:31 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "06:34:31 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 229 with model FBProphet in generation 1 of 5\n",
            "Model Number: 230 with model GLM in generation 1 of 5\n",
            "Model Number: 231 with model SeasonalNaive in generation 1 of 5\n",
            "Model Number: 232 with model UnivariateMotif in generation 1 of 5\n",
            "Model Number: 233 with model WindowRegression in generation 1 of 5\n",
            "Model Number: 234 with model DatepartRegression in generation 1 of 5\n",
            "Model Number: 235 with model WindowRegression in generation 1 of 5\n",
            "Model Number: 236 with model UnivariateMotif in generation 1 of 5\n",
            "Model Number: 237 with model GLS in generation 1 of 5\n",
            "Model Number: 238 with model LastValueNaive in generation 1 of 5\n",
            "Model Number: 239 with model ARIMA in generation 1 of 5\n",
            "Model Number: 240 with model DatepartRegression in generation 1 of 5\n",
            "Epoch 1/50\n",
            "23/23 [==============================] - 4s 5ms/step - loss: 12.3044\n",
            "Epoch 2/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 12.2633\n",
            "Epoch 3/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 12.1760\n",
            "Epoch 4/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 11.9681\n",
            "Epoch 5/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 11.5062\n",
            "Epoch 6/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 11.0048\n",
            "Epoch 7/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 10.3831\n",
            "Epoch 8/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 10.0483\n",
            "Epoch 9/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 9.7141\n",
            "Epoch 10/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 9.3703\n",
            "Epoch 11/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 8.8786\n",
            "Epoch 12/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 8.6252\n",
            "Epoch 13/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 8.1894\n",
            "Epoch 14/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 8.0519\n",
            "Epoch 15/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 7.8385\n",
            "Epoch 16/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.4846\n",
            "Epoch 17/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 7.3321\n",
            "Epoch 18/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.2466\n",
            "Epoch 19/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 7.1161\n",
            "Epoch 20/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 6.8798\n",
            "Epoch 21/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 6.8020\n",
            "Epoch 22/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 6.6201\n",
            "Epoch 23/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.6294\n",
            "Epoch 24/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 6.4717\n",
            "Epoch 25/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 6.4710\n",
            "Epoch 26/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 6.3803\n",
            "Epoch 27/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 6.4203\n",
            "Epoch 28/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.3446\n",
            "Epoch 29/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 6.5448\n",
            "Epoch 30/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 6.3688\n",
            "Epoch 31/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.2668\n",
            "Epoch 32/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.3085\n",
            "Epoch 33/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.2273\n",
            "Epoch 34/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.2326\n",
            "Epoch 35/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.2963\n",
            "Epoch 36/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.2099\n",
            "Epoch 37/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.3286\n",
            "Epoch 38/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.2366\n",
            "Epoch 39/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.1462\n",
            "Epoch 40/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 6.3102\n",
            "Epoch 41/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.3042\n",
            "Epoch 42/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.3347\n",
            "Epoch 43/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 6.1153\n",
            "Epoch 44/50\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 6.1240\n",
            "Epoch 45/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.1725\n",
            "Epoch 46/50\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 5.9904\n",
            "Epoch 47/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.1180\n",
            "Epoch 48/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.2469\n",
            "Epoch 49/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.0867\n",
            "Epoch 50/50\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.2085\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "Model Number: 241 with model ETS in generation 1 of 5\n",
            "ETS error ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\n",
            "ETS failed on Close with ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\n",
            "Model Number: 242 with model Theta in generation 1 of 5\n",
            "Model Number: 243 with model UnivariateMotif in generation 1 of 5\n",
            "Model Number: 244 with model AverageValueNaive in generation 1 of 5\n",
            "Model Number: 245 with model SeasonalNaive in generation 1 of 5\n",
            "Model Number: 246 with model ConstantNaive in generation 1 of 5\n",
            "Model Number: 247 with model UnobservedComponents in generation 1 of 5\n",
            "Model Number: 248 with model MultivariateRegression in generation 1 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=-2)]: Done 200 out of 200 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 249 with model MultivariateMotif in generation 1 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 250 with model DatepartRegression in generation 1 of 5\n",
            "Epoch 1/50\n",
            "6/6 [==============================] - 5s 5ms/step - loss: 0.3827\n",
            "Epoch 2/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3785\n",
            "Epoch 3/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3694\n",
            "Epoch 4/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3637\n",
            "Epoch 5/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3564\n",
            "Epoch 6/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.3502\n",
            "Epoch 7/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3384\n",
            "Epoch 8/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.3310\n",
            "Epoch 9/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.3270\n",
            "Epoch 10/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3210\n",
            "Epoch 11/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.3165\n",
            "Epoch 12/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3165\n",
            "Epoch 13/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3106\n",
            "Epoch 14/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.3141\n",
            "Epoch 15/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.3070\n",
            "Epoch 16/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3053\n",
            "Epoch 17/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.3072\n",
            "Epoch 18/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3085\n",
            "Epoch 19/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3052\n",
            "Epoch 20/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2994\n",
            "Epoch 21/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.3011\n",
            "Epoch 22/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.2968\n",
            "Epoch 23/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2944\n",
            "Epoch 24/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2951\n",
            "Epoch 25/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2900\n",
            "Epoch 26/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2907\n",
            "Epoch 27/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2892\n",
            "Epoch 28/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2871\n",
            "Epoch 29/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2834\n",
            "Epoch 30/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.2803\n",
            "Epoch 31/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2824\n",
            "Epoch 32/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2865\n",
            "Epoch 33/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2810\n",
            "Epoch 34/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.2856\n",
            "Epoch 35/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2729\n",
            "Epoch 36/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2751\n",
            "Epoch 37/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.2759\n",
            "Epoch 38/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2751\n",
            "Epoch 39/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2738\n",
            "Epoch 40/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2670\n",
            "Epoch 41/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2667\n",
            "Epoch 42/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2644\n",
            "Epoch 43/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2580\n",
            "Epoch 44/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2569\n",
            "Epoch 45/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2585\n",
            "Epoch 46/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2554\n",
            "Epoch 47/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2436\n",
            "Epoch 48/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2493\n",
            "Epoch 49/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2462\n",
            "Epoch 50/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2315\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Model Number: 251 with model GLS in generation 1 of 5\n",
            "Model Number: 252 with model LastValueNaive in generation 1 of 5\n",
            "Model Number: 253 with model ETS in generation 1 of 5\n",
            "Model Number: 254 with model Theta in generation 1 of 5\n",
            "Model Number: 255 with model ETS in generation 1 of 5\n",
            "ETS failed on Close with ValueError('could not broadcast input array from shape (184,) into shape (364,)')\n",
            "Model Number: 256 with model NVAR in generation 1 of 5\n",
            "Model Number: 257 with model Theta in generation 1 of 5\n",
            "Model Number: 258 with model UnivariateMotif in generation 1 of 5\n",
            "Model Number: 259 with model ARIMA in generation 1 of 5\n",
            "Model Number: 260 with model GLM in generation 1 of 5\n",
            "Template Eval Error: ValueError('regression_type=user and no future_regressor passed') in model 260: GLM\n",
            "Model Number: 261 with model MultivariateRegression in generation 1 of 5\n",
            "Template Eval Error: LightGBMError('[gamma]: at least one target label is negative') in model 261: MultivariateRegression\n",
            "Model Number: 262 with model MetricMotif in generation 1 of 5\n",
            "Model Number: 263 with model ARIMA in generation 1 of 5\n",
            "Model Number: 264 with model AverageValueNaive in generation 1 of 5\n",
            "Model Number: 265 with model NVAR in generation 1 of 5\n",
            "Model Number: 266 with model MultivariateMotif in generation 1 of 5\n",
            "Model Number: 267 with model ARIMA in generation 1 of 5\n",
            "Model Number: 268 with model LastValueNaive in generation 1 of 5\n",
            "Model Number: 269 with model SectionalMotif in generation 1 of 5\n",
            "Model Number: 270 with model LastValueNaive in generation 1 of 5\n",
            "Model Number: 271 with model ETS in generation 1 of 5\n",
            "Template Eval Error: Exception('Transformer Cointegration failed on fit') in model 271: ETS\n",
            "Model Number: 272 with model UnivariateMotif in generation 1 of 5\n",
            "Model Number: 273 with model MultivariateRegression in generation 1 of 5\n",
            "Template Eval Error: LightGBMError('[gamma]: at least one target label is negative') in model 273: MultivariateRegression\n",
            "Model Number: 274 with model MetricMotif in generation 1 of 5\n",
            "Template Eval Error: ValueError(\"Model returned NaN due to a preprocessing transformer {'fillna': 'zero', 'transformations': {'0': 'Log', '1': 'PowerTransformer', '2': 'DifferencedTransformer', '3': 'AlignLastValue'}, 'transformation_params': {'0': {}, '1': {}, '2': {}, '3': {'rows': 1, 'lag': 1, 'method': 'multiplicative', 'strength': 1.0, 'first_value_only': False}}}. fail_on_forecast_nan=True\") in model 274: MetricMotif\n",
            "Model Number: 275 with model UnobservedComponents in generation 1 of 5\n",
            "Model Number: 276 with model UnivariateMotif in generation 1 of 5\n",
            "Model Number: 277 with model UnivariateMotif in generation 1 of 5\n",
            "Model Number: 278 with model SeasonalNaive in generation 1 of 5\n",
            "Model Number: 279 with model UnivariateMotif in generation 1 of 5\n",
            "Model Number: 280 with model ARIMA in generation 1 of 5\n",
            "Model Number: 281 with model NVAR in generation 1 of 5\n",
            "Model Number: 282 with model UnivariateMotif in generation 1 of 5\n",
            "Model Number: 283 with model DatepartRegression in generation 1 of 5\n",
            "Model Number: 284 with model LastValueNaive in generation 1 of 5\n",
            "Model Number: 285 with model Theta in generation 1 of 5\n",
            "Model Number: 286 with model ETS in generation 1 of 5\n",
            "Template Eval Error: Exception('Transformer Cointegration failed on fit') in model 286: ETS\n",
            "Model Number: 287 with model MetricMotif in generation 1 of 5\n",
            "Model Number: 288 with model MultivariateMotif in generation 1 of 5\n",
            "Model Number: 289 with model GLM in generation 1 of 5\n",
            "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.') in model 289: GLM\n",
            "Model Number: 290 with model AverageValueNaive in generation 1 of 5\n",
            "Model Number: 291 with model NVAR in generation 1 of 5\n",
            "Model Number: 292 with model MetricMotif in generation 1 of 5\n",
            "Model Number: 293 with model SectionalMotif in generation 1 of 5\n",
            "Model Number: 294 with model UnivariateMotif in generation 1 of 5\n",
            "Model Number: 295 with model SeasonalNaive in generation 1 of 5\n",
            "Model Number: 296 with model LastValueNaive in generation 1 of 5\n",
            "Model Number: 297 with model DatepartRegression in generation 1 of 5\n",
            "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 297: DatepartRegression\n",
            "Model Number: 298 with model AverageValueNaive in generation 1 of 5\n",
            "Model Number: 299 with model ARIMA in generation 1 of 5\n",
            "Model Number: 300 with model LastValueNaive in generation 1 of 5\n",
            "Model Number: 301 with model LastValueNaive in generation 1 of 5\n",
            "Model Number: 302 with model UnivariateMotif in generation 1 of 5\n",
            "Model Number: 303 with model SeasonalNaive in generation 1 of 5\n",
            "Model Number: 304 with model MetricMotif in generation 1 of 5\n",
            "Model Number: 305 with model GLM in generation 1 of 5\n",
            "Template Eval Error: ValueError('regression_type=user and no future_regressor passed') in model 305: GLM\n",
            "Model Number: 306 with model Theta in generation 1 of 5\n",
            "Model Number: 307 with model MetricMotif in generation 1 of 5\n",
            "Model Number: 308 with model ETS in generation 1 of 5\n",
            "ETS failed on Close with ValueError('operands could not be broadcast together with shapes (0,) (190,) ')\n",
            "Model Number: 309 with model GLM in generation 1 of 5\n",
            "Model Number: 310 with model SeasonalNaive in generation 1 of 5\n",
            "Model Number: 311 with model UnobservedComponents in generation 1 of 5\n",
            "Model Number: 312 with model MultivariateMotif in generation 1 of 5\n",
            "Model Number: 313 with model NVAR in generation 1 of 5\n",
            "New Generation: 2 of 5\n",
            "Model Number: 314 with model UnivariateMotif in generation 2 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/cee09ix4.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/z7bo5rmt.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=60725', 'data', 'file=/tmp/tmp11_dwuur/cee09ix4.json', 'init=/tmp/tmp11_dwuur/z7bo5rmt.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_modelw8zg7xa5/prophet_model-20221223063530.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "06:35:30 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 315 with model FBProphet in generation 2 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "06:35:30 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 316 with model UnivariateMotif in generation 2 of 5\n",
            "Model Number: 317 with model AverageValueNaive in generation 2 of 5\n",
            "Model Number: 318 with model ETS in generation 2 of 5\n",
            "ETS failed on Close with ValueError('could not broadcast input array from shape (190,) into shape (364,)')\n",
            "Model Number: 319 with model NVAR in generation 2 of 5\n",
            "Model Number: 320 with model SectionalMotif in generation 2 of 5\n",
            "Model Number: 321 with model MultivariateMotif in generation 2 of 5\n",
            "Model Number: 322 with model MultivariateMotif in generation 2 of 5\n",
            "Template Eval Error: ValueError(\"Model returned NaN due to a preprocessing transformer {'fillna': 'zero', 'transformations': {'0': 'QuantileTransformer', '1': 'AlignLastValue', '2': 'AlignLastValue'}, 'transformation_params': {'0': {'output_distribution': 'uniform', 'n_quantiles': 100}, '1': {'rows': 1, 'lag': 1, 'method': 'multiplicative', 'strength': 1.0, 'first_value_only': False}, '2': {'rows': 1, 'lag': 1, 'method': 'additive', 'strength': 1.0, 'first_value_only': False}}}. fail_on_forecast_nan=True\") in model 322: MultivariateMotif\n",
            "Model Number: 323 with model ARIMA in generation 2 of 5\n",
            "Model Number: 324 with model SeasonalNaive in generation 2 of 5\n",
            "Model Number: 325 with model MetricMotif in generation 2 of 5\n",
            "Model Number: 326 with model GLM in generation 2 of 5\n",
            "Model Number: 327 with model LastValueNaive in generation 2 of 5\n",
            "Model Number: 328 with model UnobservedComponents in generation 2 of 5\n",
            "Model Number: 329 with model DatepartRegression in generation 2 of 5\n",
            "Model Number: 330 with model LastValueNaive in generation 2 of 5\n",
            "Model Number: 331 with model SeasonalNaive in generation 2 of 5\n",
            "Model Number: 332 with model MultivariateMotif in generation 2 of 5\n",
            "Model Number: 333 with model LastValueNaive in generation 2 of 5\n",
            "Model Number: 334 with model AverageValueNaive in generation 2 of 5\n",
            "Model Number: 335 with model GLM in generation 2 of 5\n",
            "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.') in model 335: GLM\n",
            "Model Number: 336 with model Theta in generation 2 of 5\n",
            "Model Number: 337 with model UnivariateMotif in generation 2 of 5\n",
            "Model Number: 338 with model ARIMA in generation 2 of 5\n",
            "Model Number: 339 with model UnivariateMotif in generation 2 of 5\n",
            "Model Number: 340 with model MultivariateMotif in generation 2 of 5\n",
            "Model Number: 341 with model MultivariateMotif in generation 2 of 5\n",
            "Model Number: 342 with model ARIMA in generation 2 of 5\n",
            "Model Number: 343 with model NVAR in generation 2 of 5\n",
            "Model Number: 344 with model MultivariateRegression in generation 2 of 5\n",
            "Model Number: 345 with model SeasonalNaive in generation 2 of 5\n",
            "Model Number: 346 with model SectionalMotif in generation 2 of 5\n",
            "Model Number: 347 with model Theta in generation 2 of 5\n",
            "Model Number: 348 with model UnivariateMotif in generation 2 of 5\n",
            "Model Number: 349 with model ETS in generation 2 of 5\n",
            "Model Number: 350 with model GLS in generation 2 of 5\n",
            "Model Number: 351 with model GLM in generation 2 of 5\n",
            "Model Number: 352 with model Theta in generation 2 of 5\n",
            "Model Number: 353 with model WindowRegression in generation 2 of 5\n",
            "Model Number: 354 with model ARIMA in generation 2 of 5\n",
            "Template Eval Error: Exception('Transformer BTCD failed on fit') in model 354: ARIMA\n",
            "Model Number: 355 with model AverageValueNaive in generation 2 of 5\n",
            "Model Number: 356 with model ARIMA in generation 2 of 5\n",
            "Model Number: 357 with model AverageValueNaive in generation 2 of 5\n",
            "Model Number: 358 with model MetricMotif in generation 2 of 5\n",
            "Model Number: 359 with model SectionalMotif in generation 2 of 5\n",
            "Model Number: 360 with model Theta in generation 2 of 5\n",
            "Model Number: 361 with model LastValueNaive in generation 2 of 5\n",
            "Template Eval Error: ValueError(\"Model returned NaN due to a preprocessing transformer {'fillna': 'zero', 'transformations': {'0': 'AlignLastValue', '1': 'MaxAbsScaler'}, 'transformation_params': {'0': {'rows': 1, 'lag': 1, 'method': 'multiplicative', 'strength': 0.2, 'first_value_only': False}, '1': {}}}. fail_on_forecast_nan=True\") in model 361: LastValueNaive\n",
            "Model Number: 362 with model UnivariateMotif in generation 2 of 5\n",
            "Model Number: 363 with model NVAR in generation 2 of 5\n",
            "Model Number: 364 with model Theta in generation 2 of 5\n",
            "Model Number: 365 with model MultivariateMotif in generation 2 of 5\n",
            "Model Number: 366 with model AverageValueNaive in generation 2 of 5\n",
            "Model Number: 367 with model MetricMotif in generation 2 of 5\n",
            "Model Number: 368 with model ConstantNaive in generation 2 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/ixhfxe1c.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/kv8tvp3q.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=98107', 'data', 'file=/tmp/tmp11_dwuur/ixhfxe1c.json', 'init=/tmp/tmp11_dwuur/kv8tvp3q.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_model0dj7kfu1/prophet_model-20221223063540.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "06:35:40 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 369 with model FBProphet in generation 2 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "06:35:40 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 370 with model ARIMA in generation 2 of 5\n",
            "Model Number: 371 with model AverageValueNaive in generation 2 of 5\n",
            "Model Number: 372 with model Theta in generation 2 of 5\n",
            "Model Number: 373 with model ETS in generation 2 of 5\n",
            "ETS failed on Close with ValueError('could not broadcast input array from shape (184,) into shape (420,)')\n",
            "Model Number: 374 with model ARIMA in generation 2 of 5\n",
            "Model Number: 375 with model UnobservedComponents in generation 2 of 5\n",
            "Model Number: 376 with model MultivariateMotif in generation 2 of 5\n",
            "Model Number: 377 with model UnobservedComponents in generation 2 of 5\n",
            "Model Number: 378 with model AverageValueNaive in generation 2 of 5\n",
            "Model Number: 379 with model LastValueNaive in generation 2 of 5\n",
            "Model Number: 380 with model SeasonalNaive in generation 2 of 5\n",
            "Model Number: 381 with model UnobservedComponents in generation 2 of 5\n",
            "Model Number: 382 with model NVAR in generation 2 of 5\n",
            "Model Number: 383 with model MultivariateRegression in generation 2 of 5\n",
            "Model Number: 384 with model SeasonalNaive in generation 2 of 5\n",
            "Model Number: 385 with model ARIMA in generation 2 of 5\n",
            "Template Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 385: ARIMA\n",
            "Model Number: 386 with model MultivariateMotif in generation 2 of 5\n",
            "Model Number: 387 with model MultivariateRegression in generation 2 of 5\n",
            "Model Number: 388 with model SectionalMotif in generation 2 of 5\n",
            "Model Number: 389 with model LastValueNaive in generation 2 of 5\n",
            "Model Number: 390 with model MetricMotif in generation 2 of 5\n",
            "Model Number: 391 with model DatepartRegression in generation 2 of 5\n",
            "Model Number: 392 with model GLS in generation 2 of 5\n",
            "Model Number: 393 with model LastValueNaive in generation 2 of 5\n",
            "Model Number: 394 with model MultivariateMotif in generation 2 of 5\n",
            "Model Number: 395 with model SeasonalNaive in generation 2 of 5\n",
            "Model Number: 396 with model Theta in generation 2 of 5\n",
            "Model Number: 397 with model ARIMA in generation 2 of 5\n",
            "Model Number: 398 with model UnobservedComponents in generation 2 of 5\n",
            "Model Number: 399 with model WindowRegression in generation 2 of 5\n",
            "Model Number: 400 with model MultivariateMotif in generation 2 of 5\n",
            "Model Number: 401 with model MultivariateRegression in generation 2 of 5\n",
            "Model Number: 402 with model SeasonalNaive in generation 2 of 5\n",
            "Model Number: 403 with model SeasonalNaive in generation 2 of 5\n",
            "Model Number: 404 with model MetricMotif in generation 2 of 5\n",
            "Model Number: 405 with model UnivariateMotif in generation 2 of 5\n",
            "Model Number: 406 with model NVAR in generation 2 of 5\n",
            "Model Number: 407 with model SectionalMotif in generation 2 of 5\n",
            "Model Number: 408 with model ARIMA in generation 2 of 5\n",
            "Model Number: 409 with model MultivariateMotif in generation 2 of 5\n",
            "Model Number: 410 with model Theta in generation 2 of 5\n",
            "Model Number: 411 with model UnivariateMotif in generation 2 of 5\n",
            "Model Number: 412 with model NVAR in generation 2 of 5\n",
            "Model Number: 413 with model ARIMA in generation 2 of 5\n",
            "Model Number: 414 with model GLS in generation 2 of 5\n",
            "Model Number: 415 with model ETS in generation 2 of 5\n",
            "ETS error ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\n",
            "ETS failed on Close with ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\n",
            "Model Number: 416 with model Theta in generation 2 of 5\n",
            "Model Number: 417 with model NVAR in generation 2 of 5\n",
            "Model Number: 418 with model FBProphet in generation 2 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/1owr9n7q.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/f6pix8_p.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=2170', 'data', 'file=/tmp/tmp11_dwuur/1owr9n7q.json', 'init=/tmp/tmp11_dwuur/f6pix8_p.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_model3rumfqra/prophet_model-20221223063552.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "06:35:52 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "06:35:52 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Template Eval Error: Exception('Transformer DatepartRegression failed on inverse') in model 418: FBProphet\n",
            "Model Number: 419 with model MultivariateMotif in generation 2 of 5\n",
            "Model Number: 420 with model SectionalMotif in generation 2 of 5\n",
            "Model Number: 421 with model LastValueNaive in generation 2 of 5\n",
            "Model Number: 422 with model UnivariateMotif in generation 2 of 5\n",
            "Model Number: 423 with model LastValueNaive in generation 2 of 5\n",
            "Model Number: 424 with model UnobservedComponents in generation 2 of 5\n",
            "Model Number: 425 with model NVAR in generation 2 of 5\n",
            "Model Number: 426 with model LastValueNaive in generation 2 of 5\n",
            "Model Number: 427 with model UnivariateMotif in generation 2 of 5\n",
            "Model Number: 428 with model UnivariateMotif in generation 2 of 5\n",
            "Model Number: 429 with model AverageValueNaive in generation 2 of 5\n",
            "Model Number: 430 with model DatepartRegression in generation 2 of 5\n",
            "Epoch 1/50\n",
            "6/6 [==============================] - 5s 13ms/step - loss: nan\n",
            "Epoch 2/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 3/50\n",
            "6/6 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 4/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 5/50\n",
            "6/6 [==============================] - 0s 13ms/step - loss: nan\n",
            "Epoch 6/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 7/50\n",
            "6/6 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 8/50\n",
            "6/6 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 9/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 10/50\n",
            "6/6 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 11/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 12/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 13/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 14/50\n",
            "6/6 [==============================] - 0s 13ms/step - loss: nan\n",
            "Epoch 15/50\n",
            "6/6 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 16/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 17/50\n",
            "6/6 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 18/50\n",
            "6/6 [==============================] - 0s 13ms/step - loss: nan\n",
            "Epoch 19/50\n",
            "6/6 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 20/50\n",
            "6/6 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 21/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 22/50\n",
            "6/6 [==============================] - 0s 13ms/step - loss: nan\n",
            "Epoch 23/50\n",
            "6/6 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 24/50\n",
            "6/6 [==============================] - 0s 13ms/step - loss: nan\n",
            "Epoch 25/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 26/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: nan\n",
            "Epoch 27/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 28/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: nan\n",
            "Epoch 29/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: nan\n",
            "Epoch 30/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 31/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 32/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 33/50\n",
            "6/6 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 34/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 35/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: nan\n",
            "Epoch 36/50\n",
            "6/6 [==============================] - 0s 13ms/step - loss: nan\n",
            "Epoch 37/50\n",
            "6/6 [==============================] - 0s 13ms/step - loss: nan\n",
            "Epoch 38/50\n",
            "6/6 [==============================] - 0s 13ms/step - loss: nan\n",
            "Epoch 39/50\n",
            "6/6 [==============================] - 0s 13ms/step - loss: nan\n",
            "Epoch 40/50\n",
            "6/6 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 41/50\n",
            "6/6 [==============================] - 0s 13ms/step - loss: nan\n",
            "Epoch 42/50\n",
            "6/6 [==============================] - 0s 13ms/step - loss: nan\n",
            "Epoch 43/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 44/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 45/50\n",
            "6/6 [==============================] - 0s 13ms/step - loss: nan\n",
            "Epoch 46/50\n",
            "6/6 [==============================] - 0s 13ms/step - loss: nan\n",
            "Epoch 47/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 48/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 49/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 50/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Model Number: 431 with model NVAR in generation 2 of 5\n",
            "Model Number: 432 with model DatepartRegression in generation 2 of 5\n",
            "Model Number: 433 with model DatepartRegression in generation 2 of 5\n",
            "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 433: DatepartRegression\n",
            "Model Number: 434 with model AverageValueNaive in generation 2 of 5\n",
            "Model Number: 435 with model DatepartRegression in generation 2 of 5\n",
            "Model Number: 436 with model LastValueNaive in generation 2 of 5\n",
            "Model Number: 437 with model WindowRegression in generation 2 of 5\n",
            "Model Number: 438 with model ETS in generation 2 of 5\n",
            "New Generation: 3 of 5\n",
            "Model Number: 439 with model SectionalMotif in generation 3 of 5\n",
            "Model Number: 440 with model MetricMotif in generation 3 of 5\n",
            "Model Number: 441 with model ETS in generation 3 of 5\n",
            "Model Number: 442 with model AverageValueNaive in generation 3 of 5\n",
            "Model Number: 443 with model AverageValueNaive in generation 3 of 5\n",
            "Model Number: 444 with model GLM in generation 3 of 5\n",
            "Model Number: 445 with model GLM in generation 3 of 5\n",
            "Model Number: 446 with model SeasonalNaive in generation 3 of 5\n",
            "Model Number: 447 with model ARIMA in generation 3 of 5\n",
            "Model Number: 448 with model NVAR in generation 3 of 5\n",
            "Model Number: 449 with model WindowRegression in generation 3 of 5\n",
            "Model Number: 450 with model WindowRegression in generation 3 of 5\n",
            "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 450: WindowRegression\n",
            "Model Number: 451 with model GLM in generation 3 of 5\n",
            "Template Eval Error: ValueError('NaN, inf or invalid value detected in weights, estimation infeasible.') in model 451: GLM\n",
            "Model Number: 452 with model UnivariateRegression in generation 3 of 5\n",
            "Model Number: 453 with model DatepartRegression in generation 3 of 5\n",
            "Model Number: 454 with model AverageValueNaive in generation 3 of 5\n",
            "Model Number: 455 with model SeasonalNaive in generation 3 of 5\n",
            "Model Number: 456 with model UnobservedComponents in generation 3 of 5\n",
            "Model Number: 457 with model GLS in generation 3 of 5\n",
            "Model Number: 458 with model LastValueNaive in generation 3 of 5\n",
            "Model Number: 459 with model SectionalMotif in generation 3 of 5\n",
            "Model Number: 460 with model GLS in generation 3 of 5\n",
            "Model Number: 461 with model AverageValueNaive in generation 3 of 5\n",
            "Model Number: 462 with model LastValueNaive in generation 3 of 5\n",
            "Model Number: 463 with model ETS in generation 3 of 5\n",
            "Model Number: 464 with model SectionalMotif in generation 3 of 5\n",
            "Model Number: 465 with model ETS in generation 3 of 5\n",
            "Model Number: 466 with model GLS in generation 3 of 5\n",
            "Model Number: 467 with model Theta in generation 3 of 5\n",
            "Model Number: 468 with model UnobservedComponents in generation 3 of 5\n",
            "Model Number: 469 with model LastValueNaive in generation 3 of 5\n",
            "Model Number: 470 with model NVAR in generation 3 of 5\n",
            "Model Number: 471 with model WindowRegression in generation 3 of 5\n",
            "Template Eval Error: Exception('Transformer Detrend failed on fit') in model 471: WindowRegression\n",
            "Model Number: 472 with model ETS in generation 3 of 5\n",
            "Model Number: 473 with model ARIMA in generation 3 of 5\n",
            "Model Number: 474 with model MultivariateMotif in generation 3 of 5\n",
            "Model Number: 475 with model MultivariateMotif in generation 3 of 5\n",
            "Model Number: 476 with model UnivariateMotif in generation 3 of 5\n",
            "Model Number: 477 with model SeasonalNaive in generation 3 of 5\n",
            "Model Number: 478 with model UnivariateMotif in generation 3 of 5\n",
            "Model Number: 479 with model Theta in generation 3 of 5\n",
            "Model Number: 480 with model LastValueNaive in generation 3 of 5\n",
            "Model Number: 481 with model UnobservedComponents in generation 3 of 5\n",
            "Model Number: 482 with model GLM in generation 3 of 5\n",
            "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.') in model 482: GLM\n",
            "Model Number: 483 with model SeasonalNaive in generation 3 of 5\n",
            "Model Number: 484 with model DatepartRegression in generation 3 of 5\n",
            "Model Number: 485 with model NVAR in generation 3 of 5\n",
            "Model Number: 486 with model UnivariateMotif in generation 3 of 5\n",
            "Model Number: 487 with model AverageValueNaive in generation 3 of 5\n",
            "Model Number: 488 with model ETS in generation 3 of 5\n",
            "Model Number: 489 with model SectionalMotif in generation 3 of 5\n",
            "Template Eval Error: Exception('Transformer Cointegration failed on fit') in model 489: SectionalMotif\n",
            "Model Number: 490 with model ETS in generation 3 of 5\n",
            "Model Number: 491 with model ETS in generation 3 of 5\n",
            "ETS failed on Close with ValueError('could not broadcast input array from shape (190,) into shape (420,)')\n",
            "Model Number: 492 with model MultivariateRegression in generation 3 of 5\n",
            "Model Number: 493 with model Theta in generation 3 of 5\n",
            "Model Number: 494 with model Theta in generation 3 of 5\n",
            "Model Number: 495 with model UnobservedComponents in generation 3 of 5\n",
            "Model Number: 496 with model ARIMA in generation 3 of 5\n",
            "Model Number: 497 with model SeasonalNaive in generation 3 of 5\n",
            "Model Number: 498 with model LastValueNaive in generation 3 of 5\n",
            "Model Number: 499 with model MultivariateRegression in generation 3 of 5\n",
            "Model Number: 500 with model MultivariateMotif in generation 3 of 5\n",
            "Model Number: 501 with model DatepartRegression in generation 3 of 5\n",
            "Model Number: 502 with model UnivariateMotif in generation 3 of 5\n",
            "Template Eval Error: ValueError('window shape cannot be larger than input array shape') in model 502: UnivariateMotif\n",
            "Model Number: 503 with model ConstantNaive in generation 3 of 5\n",
            "Model Number: 504 with model ARIMA in generation 3 of 5\n",
            "Model Number: 505 with model UnobservedComponents in generation 3 of 5\n",
            "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 505: UnobservedComponents\n",
            "Model Number: 506 with model SeasonalNaive in generation 3 of 5\n",
            "Model Number: 507 with model MultivariateMotif in generation 3 of 5\n",
            "Model Number: 508 with model GLM in generation 3 of 5\n",
            "Template Eval Error: ValueError('regression_type=user and no future_regressor passed') in model 508: GLM\n",
            "Model Number: 509 with model GLS in generation 3 of 5\n",
            "Model Number: 510 with model UnivariateMotif in generation 3 of 5\n",
            "Model Number: 511 with model UnivariateMotif in generation 3 of 5\n",
            "Model Number: 512 with model UnivariateMotif in generation 3 of 5\n",
            "Model Number: 513 with model UnivariateMotif in generation 3 of 5\n",
            "Model Number: 514 with model Theta in generation 3 of 5\n",
            "Model Number: 515 with model SeasonalNaive in generation 3 of 5\n",
            "Model Number: 516 with model MultivariateMotif in generation 3 of 5\n",
            "Model Number: 517 with model SeasonalNaive in generation 3 of 5\n",
            "Model Number: 518 with model MultivariateRegression in generation 3 of 5\n",
            "Model Number: 519 with model AverageValueNaive in generation 3 of 5\n",
            "Model Number: 520 with model AverageValueNaive in generation 3 of 5\n",
            "Model Number: 521 with model UnivariateMotif in generation 3 of 5\n",
            "Model Number: 522 with model Theta in generation 3 of 5\n",
            "Model Number: 523 with model UnivariateMotif in generation 3 of 5\n",
            "Model Number: 524 with model ConstantNaive in generation 3 of 5\n",
            "Model Number: 525 with model Theta in generation 3 of 5\n",
            "Model Number: 526 with model FBProphet in generation 3 of 5\n",
            "Template Eval Error: Exception('Transformer Cointegration failed on fit') in model 526: FBProphet\n",
            "Model Number: 527 with model SeasonalNaive in generation 3 of 5\n",
            "Model Number: 528 with model UnivariateRegression in generation 3 of 5\n",
            "Model Number: 529 with model AverageValueNaive in generation 3 of 5\n",
            "Model Number: 530 with model AverageValueNaive in generation 3 of 5\n",
            "Model Number: 531 with model LastValueNaive in generation 3 of 5\n",
            "Model Number: 532 with model LastValueNaive in generation 3 of 5\n",
            "Model Number: 533 with model NVAR in generation 3 of 5\n",
            "Model Number: 534 with model MultivariateRegression in generation 3 of 5\n",
            "Model Number: 535 with model ETS in generation 3 of 5\n",
            "Model Number: 536 with model MultivariateRegression in generation 3 of 5\n",
            "Model Number: 537 with model ARIMA in generation 3 of 5\n",
            "Model Number: 538 with model LastValueNaive in generation 3 of 5\n",
            "Model Number: 539 with model MultivariateMotif in generation 3 of 5\n",
            "Model Number: 540 with model DatepartRegression in generation 3 of 5\n",
            "Epoch 1/200\n",
            "6/6 [==============================] - 5s 35ms/step - loss: 0.1802\n",
            "Epoch 2/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0848\n",
            "Epoch 3/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0844\n",
            "Epoch 4/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0868\n",
            "Epoch 5/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0900\n",
            "Epoch 6/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1096\n",
            "Epoch 7/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0870\n",
            "Epoch 8/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0969\n",
            "Epoch 9/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0880\n",
            "Epoch 10/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0867\n",
            "Epoch 11/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.0890\n",
            "Epoch 12/200\n",
            "6/6 [==============================] - 0s 43ms/step - loss: 0.0897\n",
            "Epoch 13/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0869\n",
            "Epoch 14/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0928\n",
            "Epoch 15/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0905\n",
            "Epoch 16/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0896\n",
            "Epoch 17/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0909\n",
            "Epoch 18/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0872\n",
            "Epoch 19/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0854\n",
            "Epoch 20/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0870\n",
            "Epoch 21/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0954\n",
            "Epoch 22/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0874\n",
            "Epoch 23/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0868\n",
            "Epoch 24/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0876\n",
            "Epoch 25/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0859\n",
            "Epoch 26/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0920\n",
            "Epoch 27/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0846\n",
            "Epoch 28/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0861\n",
            "Epoch 29/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0880\n",
            "Epoch 30/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0865\n",
            "Epoch 31/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0876\n",
            "Epoch 32/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0889\n",
            "Epoch 33/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0862\n",
            "Epoch 34/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0875\n",
            "Epoch 35/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0870\n",
            "Epoch 36/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0874\n",
            "Epoch 37/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0884\n",
            "Epoch 38/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0866\n",
            "Epoch 39/200\n",
            "6/6 [==============================] - 0s 46ms/step - loss: 0.0846\n",
            "Epoch 40/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0861\n",
            "Epoch 41/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0878\n",
            "Epoch 42/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0875\n",
            "Epoch 43/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0888\n",
            "Epoch 44/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0844\n",
            "Epoch 45/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0921\n",
            "Epoch 46/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0876\n",
            "Epoch 47/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0872\n",
            "Epoch 48/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0852\n",
            "Epoch 49/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0887\n",
            "Epoch 50/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0858\n",
            "Epoch 51/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.0875\n",
            "Epoch 52/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0847\n",
            "Epoch 53/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0850\n",
            "Epoch 54/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0887\n",
            "Epoch 55/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0867\n",
            "Epoch 56/200\n",
            "6/6 [==============================] - 0s 42ms/step - loss: 0.0892\n",
            "Epoch 57/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0875\n",
            "Epoch 58/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0901\n",
            "Epoch 59/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0871\n",
            "Epoch 60/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0843\n",
            "Epoch 61/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0839\n",
            "Epoch 62/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0855\n",
            "Epoch 63/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0846\n",
            "Epoch 64/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0814\n",
            "Epoch 65/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0843\n",
            "Epoch 66/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0817\n",
            "Epoch 67/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0766\n",
            "Epoch 68/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0858\n",
            "Epoch 69/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0784\n",
            "Epoch 70/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0779\n",
            "Epoch 71/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0703\n",
            "Epoch 72/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0860\n",
            "Epoch 73/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0696\n",
            "Epoch 74/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0703\n",
            "Epoch 75/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0720\n",
            "Epoch 76/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0595\n",
            "Epoch 77/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0645\n",
            "Epoch 78/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0615\n",
            "Epoch 79/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0576\n",
            "Epoch 80/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0498\n",
            "Epoch 81/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.0764\n",
            "Epoch 82/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0545\n",
            "Epoch 83/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0625\n",
            "Epoch 84/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0455\n",
            "Epoch 85/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0605\n",
            "Epoch 86/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0466\n",
            "Epoch 87/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0398\n",
            "Epoch 88/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0506\n",
            "Epoch 89/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0506\n",
            "Epoch 90/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0483\n",
            "Epoch 91/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0545\n",
            "Epoch 92/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0477\n",
            "Epoch 93/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0469\n",
            "Epoch 94/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0499\n",
            "Epoch 95/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0525\n",
            "Epoch 96/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0381\n",
            "Epoch 97/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0397\n",
            "Epoch 98/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0525\n",
            "Epoch 99/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0456\n",
            "Epoch 100/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0360\n",
            "Epoch 101/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0358\n",
            "Epoch 102/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0421\n",
            "Epoch 103/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0403\n",
            "Epoch 104/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0403\n",
            "Epoch 105/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0419\n",
            "Epoch 106/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0436\n",
            "Epoch 107/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0343\n",
            "Epoch 108/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0376\n",
            "Epoch 109/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0321\n",
            "Epoch 110/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0369\n",
            "Epoch 111/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0411\n",
            "Epoch 112/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0497\n",
            "Epoch 113/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0294\n",
            "Epoch 114/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0415\n",
            "Epoch 115/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0316\n",
            "Epoch 116/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0352\n",
            "Epoch 117/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0442\n",
            "Epoch 118/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0356\n",
            "Epoch 119/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0267\n",
            "Epoch 120/200\n",
            "6/6 [==============================] - 0s 41ms/step - loss: 0.0359\n",
            "Epoch 121/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0271\n",
            "Epoch 122/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0402\n",
            "Epoch 123/200\n",
            "6/6 [==============================] - 0s 41ms/step - loss: 0.0304\n",
            "Epoch 124/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0402\n",
            "Epoch 125/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0382\n",
            "Epoch 126/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0311\n",
            "Epoch 127/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.0277\n",
            "Epoch 128/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0420\n",
            "Epoch 129/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0273\n",
            "Epoch 130/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0336\n",
            "Epoch 131/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0296\n",
            "Epoch 132/200\n",
            "6/6 [==============================] - 0s 41ms/step - loss: 0.0276\n",
            "Epoch 133/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0327\n",
            "Epoch 134/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0309\n",
            "Epoch 135/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0262\n",
            "Epoch 136/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0260\n",
            "Epoch 137/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0349\n",
            "Epoch 138/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0243\n",
            "Epoch 139/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0385\n",
            "Epoch 140/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0241\n",
            "Epoch 141/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0251\n",
            "Epoch 142/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0371\n",
            "Epoch 143/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0241\n",
            "Epoch 144/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0292\n",
            "Epoch 145/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0256\n",
            "Epoch 146/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.0307\n",
            "Epoch 147/200\n",
            "6/6 [==============================] - 0s 42ms/step - loss: 0.0292\n",
            "Epoch 148/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.0212\n",
            "Epoch 149/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0265\n",
            "Epoch 150/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0243\n",
            "Epoch 151/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0353\n",
            "Epoch 152/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.0212\n",
            "Epoch 153/200\n",
            "6/6 [==============================] - 0s 42ms/step - loss: 0.0297\n",
            "Epoch 154/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0264\n",
            "Epoch 155/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0213\n",
            "Epoch 156/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0308\n",
            "Epoch 157/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0298\n",
            "Epoch 158/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0278\n",
            "Epoch 159/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0294\n",
            "Epoch 160/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0223\n",
            "Epoch 161/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0290\n",
            "Epoch 162/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0221\n",
            "Epoch 163/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0217\n",
            "Epoch 164/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0290\n",
            "Epoch 165/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0208\n",
            "Epoch 166/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0239\n",
            "Epoch 167/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0326\n",
            "Epoch 168/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0197\n",
            "Epoch 169/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0195\n",
            "Epoch 170/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0245\n",
            "Epoch 171/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0233\n",
            "Epoch 172/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.0234\n",
            "Epoch 173/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0250\n",
            "Epoch 174/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0261\n",
            "Epoch 175/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0275\n",
            "Epoch 176/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0187\n",
            "Epoch 177/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0294\n",
            "Epoch 178/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0212\n",
            "Epoch 179/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0254\n",
            "Epoch 180/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0198\n",
            "Epoch 181/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0197\n",
            "Epoch 182/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0294\n",
            "Epoch 183/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0176\n",
            "Epoch 184/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0245\n",
            "Epoch 185/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0226\n",
            "Epoch 186/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0225\n",
            "Epoch 187/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0266\n",
            "Epoch 188/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0195\n",
            "Epoch 189/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0264\n",
            "Epoch 190/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0198\n",
            "Epoch 191/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0261\n",
            "Epoch 192/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0177\n",
            "Epoch 193/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0210\n",
            "Epoch 194/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0216\n",
            "Epoch 195/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0249\n",
            "Epoch 196/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0204\n",
            "Epoch 197/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0189\n",
            "Epoch 198/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0261\n",
            "Epoch 199/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0205\n",
            "Epoch 200/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0290\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd06b0190d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "Model Number: 541 with model UnivariateMotif in generation 3 of 5\n",
            "Model Number: 542 with model UnivariateMotif in generation 3 of 5\n",
            "Model Number: 543 with model ETS in generation 3 of 5\n",
            "ETS failed on Close with ValueError('could not broadcast input array from shape (190,) into shape (420,)')\n",
            "Model Number: 544 with model WindowRegression in generation 3 of 5\n",
            "Model Number: 545 with model UnobservedComponents in generation 3 of 5\n",
            "Model Number: 546 with model GLS in generation 3 of 5\n",
            "Model Number: 547 with model WindowRegression in generation 3 of 5\n",
            "Model Number: 548 with model ARIMA in generation 3 of 5\n",
            "Model Number: 549 with model NVAR in generation 3 of 5\n",
            "Model Number: 550 with model UnobservedComponents in generation 3 of 5\n",
            "Model Number: 551 with model MultivariateRegression in generation 3 of 5\n",
            "Model Number: 552 with model ARIMA in generation 3 of 5\n",
            "Model Number: 553 with model MultivariateRegression in generation 3 of 5\n",
            "Model Number: 554 with model Theta in generation 3 of 5\n",
            "Model Number: 555 with model DatepartRegression in generation 3 of 5\n",
            "Model Number: 556 with model ARIMA in generation 3 of 5\n",
            "Model Number: 557 with model MultivariateMotif in generation 3 of 5\n",
            "Model Number: 558 with model MetricMotif in generation 3 of 5\n",
            "Model Number: 559 with model AverageValueNaive in generation 3 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/y9nt5bdq.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 560 with model FBProphet in generation 3 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/d7rxfqae.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=15967', 'data', 'file=/tmp/tmp11_dwuur/y9nt5bdq.json', 'init=/tmp/tmp11_dwuur/d7rxfqae.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_modeleiuz3wrl/prophet_model-20221223063719.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "06:37:19 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "06:37:19 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 561 with model GLS in generation 3 of 5\n",
            "Model Number: 562 with model UnivariateMotif in generation 3 of 5\n",
            "Model Number: 563 with model ARIMA in generation 3 of 5\n",
            "New Generation: 4 of 5\n",
            "Model Number: 564 with model ConstantNaive in generation 4 of 5\n",
            "Model Number: 565 with model GLS in generation 4 of 5\n",
            "Model Number: 566 with model ConstantNaive in generation 4 of 5\n",
            "Model Number: 567 with model SectionalMotif in generation 4 of 5\n",
            "Model Number: 568 with model MultivariateMotif in generation 4 of 5\n",
            "Model Number: 569 with model AverageValueNaive in generation 4 of 5\n",
            "Model Number: 570 with model MultivariateRegression in generation 4 of 5\n",
            "Template Eval Error: Exception('Transformer BTCD failed on fit') in model 570: MultivariateRegression\n",
            "Model Number: 571 with model SeasonalNaive in generation 4 of 5\n",
            "Model Number: 572 with model LastValueNaive in generation 4 of 5\n",
            "Model Number: 573 with model ARIMA in generation 4 of 5\n",
            "Model Number: 574 with model SectionalMotif in generation 4 of 5\n",
            "Model Number: 575 with model ARIMA in generation 4 of 5\n",
            "Model Number: 576 with model ETS in generation 4 of 5\n",
            "ETS failed on Close with ValueError('could not broadcast input array from shape (190,) into shape (420,)')\n",
            "Model Number: 577 with model UnivariateMotif in generation 4 of 5\n",
            "Model Number: 578 with model UnivariateMotif in generation 4 of 5\n",
            "Model Number: 579 with model Theta in generation 4 of 5\n",
            "Model Number: 580 with model Theta in generation 4 of 5\n",
            "Model Number: 581 with model UnivariateMotif in generation 4 of 5\n",
            "Model Number: 582 with model UnivariateMotif in generation 4 of 5\n",
            "Model Number: 583 with model LastValueNaive in generation 4 of 5\n",
            "Model Number: 584 with model UnivariateMotif in generation 4 of 5\n",
            "Model Number: 585 with model SectionalMotif in generation 4 of 5\n",
            "Model Number: 586 with model WindowRegression in generation 4 of 5\n",
            "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 586: WindowRegression\n",
            "Model Number: 587 with model SeasonalNaive in generation 4 of 5\n",
            "Model Number: 588 with model ETS in generation 4 of 5\n",
            "ETS failed on Close with ValueError('could not broadcast input array from shape (184,) into shape (364,)')\n",
            "Model Number: 589 with model SectionalMotif in generation 4 of 5\n",
            "Model Number: 590 with model ARIMA in generation 4 of 5\n",
            "Model Number: 591 with model UnobservedComponents in generation 4 of 5\n",
            "Model Number: 592 with model LastValueNaive in generation 4 of 5\n",
            "Model Number: 593 with model MultivariateRegression in generation 4 of 5\n",
            "Model Number: 594 with model LastValueNaive in generation 4 of 5\n",
            "Model Number: 595 with model GLS in generation 4 of 5\n",
            "Model Number: 596 with model DatepartRegression in generation 4 of 5\n",
            "Model Number: 597 with model MultivariateRegression in generation 4 of 5\n",
            "Model Number: 598 with model UnivariateRegression in generation 4 of 5\n",
            "Model Number: 599 with model MultivariateRegression in generation 4 of 5\n",
            "Model Number: 600 with model MultivariateRegression in generation 4 of 5\n",
            "Model Number: 601 with model LastValueNaive in generation 4 of 5\n",
            "Model Number: 602 with model UnivariateMotif in generation 4 of 5\n",
            "Model Number: 603 with model ETS in generation 4 of 5\n",
            "Model Number: 604 with model DatepartRegression in generation 4 of 5\n",
            "Epoch 1/200\n",
            "6/6 [==============================] - 5s 34ms/step - loss: 0.3449\n",
            "Epoch 2/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3455\n",
            "Epoch 3/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.3450\n",
            "Epoch 4/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.3445\n",
            "Epoch 5/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3457\n",
            "Epoch 6/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3465\n",
            "Epoch 7/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.3444\n",
            "Epoch 8/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3459\n",
            "Epoch 9/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3448\n",
            "Epoch 10/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.3458\n",
            "Epoch 11/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.3446\n",
            "Epoch 12/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3449\n",
            "Epoch 13/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.3454\n",
            "Epoch 14/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.3451\n",
            "Epoch 15/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.3446\n",
            "Epoch 16/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.3450\n",
            "Epoch 17/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.3447\n",
            "Epoch 18/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3452\n",
            "Epoch 19/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3444\n",
            "Epoch 20/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.3446\n",
            "Epoch 21/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3452\n",
            "Epoch 22/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.3444\n",
            "Epoch 23/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.3444\n",
            "Epoch 24/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.3443\n",
            "Epoch 25/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3472\n",
            "Epoch 26/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.3445\n",
            "Epoch 27/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.3441\n",
            "Epoch 28/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.3445\n",
            "Epoch 29/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3443\n",
            "Epoch 30/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.3441\n",
            "Epoch 31/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.3446\n",
            "Epoch 32/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3445\n",
            "Epoch 33/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3442\n",
            "Epoch 34/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.3446\n",
            "Epoch 35/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3444\n",
            "Epoch 36/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3449\n",
            "Epoch 37/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.3442\n",
            "Epoch 38/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.3442\n",
            "Epoch 39/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3437\n",
            "Epoch 40/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3445\n",
            "Epoch 41/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3440\n",
            "Epoch 42/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.3448\n",
            "Epoch 43/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.3443\n",
            "Epoch 44/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3451\n",
            "Epoch 45/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.3446\n",
            "Epoch 46/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3433\n",
            "Epoch 47/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3421\n",
            "Epoch 48/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.3417\n",
            "Epoch 49/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.3351\n",
            "Epoch 50/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3272\n",
            "Epoch 51/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3318\n",
            "Epoch 52/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.2908\n",
            "Epoch 53/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.2775\n",
            "Epoch 54/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.2444\n",
            "Epoch 55/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.2692\n",
            "Epoch 56/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.2824\n",
            "Epoch 57/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.2320\n",
            "Epoch 58/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.2268\n",
            "Epoch 59/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.2084\n",
            "Epoch 60/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.2013\n",
            "Epoch 61/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.2036\n",
            "Epoch 62/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.1947\n",
            "Epoch 63/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.1835\n",
            "Epoch 64/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.1682\n",
            "Epoch 65/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.2033\n",
            "Epoch 66/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.1817\n",
            "Epoch 67/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.1527\n",
            "Epoch 68/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1833\n",
            "Epoch 69/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.1498\n",
            "Epoch 70/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.1595\n",
            "Epoch 71/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1440\n",
            "Epoch 72/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1559\n",
            "Epoch 73/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.1689\n",
            "Epoch 74/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1285\n",
            "Epoch 75/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.1756\n",
            "Epoch 76/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1376\n",
            "Epoch 77/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.1397\n",
            "Epoch 78/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1353\n",
            "Epoch 79/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.1371\n",
            "Epoch 80/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.1461\n",
            "Epoch 81/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1243\n",
            "Epoch 82/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1670\n",
            "Epoch 83/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.1183\n",
            "Epoch 84/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1146\n",
            "Epoch 85/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1203\n",
            "Epoch 86/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1361\n",
            "Epoch 87/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1124\n",
            "Epoch 88/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1348\n",
            "Epoch 89/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1165\n",
            "Epoch 90/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1131\n",
            "Epoch 91/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.1114\n",
            "Epoch 92/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.1257\n",
            "Epoch 93/200\n",
            "6/6 [==============================] - 0s 41ms/step - loss: 0.1077\n",
            "Epoch 94/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.1176\n",
            "Epoch 95/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1258\n",
            "Epoch 96/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1043\n",
            "Epoch 97/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1092\n",
            "Epoch 98/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.1049\n",
            "Epoch 99/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.1191\n",
            "Epoch 100/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0941\n",
            "Epoch 101/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1027\n",
            "Epoch 102/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0942\n",
            "Epoch 103/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.1079\n",
            "Epoch 104/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0871\n",
            "Epoch 105/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0980\n",
            "Epoch 106/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1098\n",
            "Epoch 107/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1064\n",
            "Epoch 108/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0956\n",
            "Epoch 109/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0887\n",
            "Epoch 110/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0845\n",
            "Epoch 111/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0849\n",
            "Epoch 112/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1004\n",
            "Epoch 113/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1013\n",
            "Epoch 114/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0834\n",
            "Epoch 115/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0863\n",
            "Epoch 116/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0946\n",
            "Epoch 117/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0866\n",
            "Epoch 118/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.1031\n",
            "Epoch 119/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0715\n",
            "Epoch 120/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0949\n",
            "Epoch 121/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0800\n",
            "Epoch 122/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0933\n",
            "Epoch 123/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1204\n",
            "Epoch 124/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0933\n",
            "Epoch 125/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0768\n",
            "Epoch 126/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0842\n",
            "Epoch 127/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0866\n",
            "Epoch 128/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0767\n",
            "Epoch 129/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0783\n",
            "Epoch 130/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0884\n",
            "Epoch 131/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.0748\n",
            "Epoch 132/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0716\n",
            "Epoch 133/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0847\n",
            "Epoch 134/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0801\n",
            "Epoch 135/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0832\n",
            "Epoch 136/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0649\n",
            "Epoch 137/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0794\n",
            "Epoch 138/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0695\n",
            "Epoch 139/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0717\n",
            "Epoch 140/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0705\n",
            "Epoch 141/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0617\n",
            "Epoch 142/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0779\n",
            "Epoch 143/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0887\n",
            "Epoch 144/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0685\n",
            "Epoch 145/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0648\n",
            "Epoch 146/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0702\n",
            "Epoch 147/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0696\n",
            "Epoch 148/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0667\n",
            "Epoch 149/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0641\n",
            "Epoch 150/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0701\n",
            "Epoch 151/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0638\n",
            "Epoch 152/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0666\n",
            "Epoch 153/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0696\n",
            "Epoch 154/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0756\n",
            "Epoch 155/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0609\n",
            "Epoch 156/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0534\n",
            "Epoch 157/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0893\n",
            "Epoch 158/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0563\n",
            "Epoch 159/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0553\n",
            "Epoch 160/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0608\n",
            "Epoch 161/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0664\n",
            "Epoch 162/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0663\n",
            "Epoch 163/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0546\n",
            "Epoch 164/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0591\n",
            "Epoch 165/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0519\n",
            "Epoch 166/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0532\n",
            "Epoch 167/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.0725\n",
            "Epoch 168/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0489\n",
            "Epoch 169/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0608\n",
            "Epoch 170/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0562\n",
            "Epoch 171/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0524\n",
            "Epoch 172/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0489\n",
            "Epoch 173/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0799\n",
            "Epoch 174/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0492\n",
            "Epoch 175/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0484\n",
            "Epoch 176/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0555\n",
            "Epoch 177/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0503\n",
            "Epoch 178/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0558\n",
            "Epoch 179/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0479\n",
            "Epoch 180/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0604\n",
            "Epoch 181/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0563\n",
            "Epoch 182/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0436\n",
            "Epoch 183/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0381\n",
            "Epoch 184/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.0484\n",
            "Epoch 185/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0545\n",
            "Epoch 186/200\n",
            "6/6 [==============================] - 0s 42ms/step - loss: 0.0527\n",
            "Epoch 187/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0473\n",
            "Epoch 188/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0429\n",
            "Epoch 189/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0544\n",
            "Epoch 190/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0485\n",
            "Epoch 191/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0351\n",
            "Epoch 192/200\n",
            "6/6 [==============================] - 0s 43ms/step - loss: 0.0356\n",
            "Epoch 193/200\n",
            "6/6 [==============================] - 0s 59ms/step - loss: 0.0769\n",
            "Epoch 194/200\n",
            "6/6 [==============================] - 0s 63ms/step - loss: 0.0479\n",
            "Epoch 195/200\n",
            "6/6 [==============================] - 0s 61ms/step - loss: 0.0371\n",
            "Epoch 196/200\n",
            "6/6 [==============================] - 0s 60ms/step - loss: 0.0345\n",
            "Epoch 197/200\n",
            "6/6 [==============================] - 0s 60ms/step - loss: 0.0611\n",
            "Epoch 198/200\n",
            "6/6 [==============================] - 0s 58ms/step - loss: 0.0436\n",
            "Epoch 199/200\n",
            "6/6 [==============================] - 0s 55ms/step - loss: 0.0366\n",
            "Epoch 200/200\n",
            "6/6 [==============================] - 0s 60ms/step - loss: 0.0686\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd068f8dd30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "Model Number: 605 with model MultivariateMotif in generation 4 of 5\n",
            "Model Number: 606 with model ETS in generation 4 of 5\n",
            "ETS error ValueError('Can only dampen the trend component')\n",
            "ETS failed on Close with ValueError('Can only dampen the trend component')\n",
            "Model Number: 607 with model SeasonalNaive in generation 4 of 5\n",
            "Model Number: 608 with model DatepartRegression in generation 4 of 5\n",
            "Model Number: 609 with model MultivariateMotif in generation 4 of 5\n",
            "Model Number: 610 with model Theta in generation 4 of 5\n",
            "Model Number: 611 with model MultivariateRegression in generation 4 of 5\n",
            "Template Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 611: MultivariateRegression\n",
            "Model Number: 612 with model MultivariateMotif in generation 4 of 5\n",
            "Model Number: 613 with model ARIMA in generation 4 of 5\n",
            "Model Number: 614 with model LastValueNaive in generation 4 of 5\n",
            "Model Number: 615 with model ETS in generation 4 of 5\n",
            "ETS failed on Close with ValueError('could not broadcast input array from shape (190,) into shape (364,)')\n",
            "Model Number: 616 with model UnivariateMotif in generation 4 of 5\n",
            "Model Number: 617 with model ARIMA in generation 4 of 5\n",
            "Model Number: 618 with model UnivariateMotif in generation 4 of 5\n",
            "Model Number: 619 with model WindowRegression in generation 4 of 5\n",
            "Model Number: 620 with model NVAR in generation 4 of 5\n",
            "Model Number: 621 with model DatepartRegression in generation 4 of 5\n",
            "Model Number: 622 with model SectionalMotif in generation 4 of 5\n",
            "Model Number: 623 with model LastValueNaive in generation 4 of 5\n",
            "Model Number: 624 with model LastValueNaive in generation 4 of 5\n",
            "Model Number: 625 with model MetricMotif in generation 4 of 5\n",
            "Model Number: 626 with model GLS in generation 4 of 5\n",
            "Model Number: 627 with model MultivariateRegression in generation 4 of 5\n",
            "Model Number: 628 with model NVAR in generation 4 of 5\n",
            "Model Number: 629 with model Theta in generation 4 of 5\n",
            "Template Eval Error: Exception('Transformer Detrend failed on fit') in model 629: Theta\n",
            "Model Number: 630 with model ETS in generation 4 of 5\n",
            "Model Number: 631 with model ARIMA in generation 4 of 5\n",
            "Model Number: 632 with model ARIMA in generation 4 of 5\n",
            "Template Eval Error: Exception('Transformer Detrend failed on fit') in model 632: ARIMA\n",
            "Model Number: 633 with model Theta in generation 4 of 5\n",
            "Model Number: 634 with model MultivariateMotif in generation 4 of 5\n",
            "Model Number: 635 with model UnobservedComponents in generation 4 of 5\n",
            "Model Number: 636 with model ETS in generation 4 of 5\n",
            "Model Number: 637 with model MultivariateMotif in generation 4 of 5\n",
            "Model Number: 638 with model MultivariateMotif in generation 4 of 5\n",
            "Model Number: 639 with model MultivariateMotif in generation 4 of 5\n",
            "Model Number: 640 with model MetricMotif in generation 4 of 5\n",
            "Model Number: 641 with model MultivariateMotif in generation 4 of 5\n",
            "Model Number: 642 with model UnivariateMotif in generation 4 of 5\n",
            "Model Number: 643 with model Theta in generation 4 of 5\n",
            "Model Number: 644 with model UnivariateMotif in generation 4 of 5\n",
            "Model Number: 645 with model NVAR in generation 4 of 5\n",
            "Model Number: 646 with model Theta in generation 4 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 647 with model FBProphet in generation 4 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/4not2vdg.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/0q8yid_e.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=35149', 'data', 'file=/tmp/tmp11_dwuur/4not2vdg.json', 'init=/tmp/tmp11_dwuur/0q8yid_e.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_model894wnwzs/prophet_model-20221223063824.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "06:38:24 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "06:38:24 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 648 with model SeasonalNaive in generation 4 of 5\n",
            "Model Number: 649 with model LastValueNaive in generation 4 of 5\n",
            "Model Number: 650 with model UnobservedComponents in generation 4 of 5\n",
            "Model Number: 651 with model MultivariateRegression in generation 4 of 5\n",
            "Model Number: 652 with model SectionalMotif in generation 4 of 5\n",
            "Model Number: 653 with model UnobservedComponents in generation 4 of 5\n",
            "Model Number: 654 with model LastValueNaive in generation 4 of 5\n",
            "Model Number: 655 with model SeasonalNaive in generation 4 of 5\n",
            "Model Number: 656 with model ETS in generation 4 of 5\n",
            "Model Number: 657 with model GLM in generation 4 of 5\n",
            "Model Number: 658 with model AverageValueNaive in generation 4 of 5\n",
            "Model Number: 659 with model GLS in generation 4 of 5\n",
            "Model Number: 660 with model UnobservedComponents in generation 4 of 5\n",
            "Model Number: 661 with model DatepartRegression in generation 4 of 5\n",
            "Epoch 1/50\n",
            "5/5 [==============================] - 8s 383ms/step - loss: 6349283.5000 - val_loss: 1202418.6250\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - 1s 244ms/step - loss: 4076224.5000 - val_loss: 4115883.0000\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - 1s 235ms/step - loss: 5042167.5000 - val_loss: 3494353.0000\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - 1s 237ms/step - loss: 4330290.0000 - val_loss: 1548783.0000\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - 1s 240ms/step - loss: 4023862.2500 - val_loss: 1505290.8750\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - 1s 239ms/step - loss: 4052398.7500 - val_loss: 1907666.5000\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - 1s 248ms/step - loss: 5067331.5000 - val_loss: 746150.8125\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - 1s 237ms/step - loss: 4742263.5000 - val_loss: 959495.1875\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - 1s 239ms/step - loss: 4042785.7500 - val_loss: 1597685.7500\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - 1s 240ms/step - loss: 4095140.2500 - val_loss: 181873.7812\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - 1s 239ms/step - loss: 4083417.7500 - val_loss: 4875078.0000\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - 1s 239ms/step - loss: 4163271.5000 - val_loss: 2211345.2500\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - 1s 247ms/step - loss: 4281907.5000 - val_loss: 2211991.2500\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - 1s 238ms/step - loss: 4609148.0000 - val_loss: 393766.0000\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - 1s 238ms/step - loss: 4003165.5000 - val_loss: 4959878.5000\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - 1s 238ms/step - loss: 4097021.0000 - val_loss: 2235555.0000\n",
            "Epoch 17/50\n",
            "5/5 [==============================] - 1s 235ms/step - loss: 3603406.0000 - val_loss: 4802636.0000\n",
            "Epoch 18/50\n",
            "5/5 [==============================] - 1s 241ms/step - loss: 4480734.0000 - val_loss: 992523.8125\n",
            "Epoch 19/50\n",
            "5/5 [==============================] - 1s 240ms/step - loss: 4131898.0000 - val_loss: 148216.1250\n",
            "Epoch 20/50\n",
            "5/5 [==============================] - 1s 239ms/step - loss: 3840005.0000 - val_loss: 2247053.5000\n",
            "Epoch 21/50\n",
            "5/5 [==============================] - 1s 238ms/step - loss: 4072003.7500 - val_loss: 1473598.3750\n",
            "Epoch 22/50\n",
            "5/5 [==============================] - 1s 240ms/step - loss: 3694072.7500 - val_loss: 3274984.5000\n",
            "Epoch 23/50\n",
            "5/5 [==============================] - 1s 242ms/step - loss: 4419633.0000 - val_loss: 376482.9375\n",
            "Epoch 24/50\n",
            "5/5 [==============================] - 1s 242ms/step - loss: 3688006.0000 - val_loss: 1769209.8750\n",
            "Epoch 25/50\n",
            "5/5 [==============================] - 1s 245ms/step - loss: 3662475.7500 - val_loss: 2486796.0000\n",
            "Epoch 26/50\n",
            "5/5 [==============================] - 1s 245ms/step - loss: 3986983.2500 - val_loss: 531240.0000\n",
            "Epoch 27/50\n",
            "5/5 [==============================] - 1s 245ms/step - loss: 4285307.5000 - val_loss: 3164176.7500\n",
            "Epoch 28/50\n",
            "5/5 [==============================] - 1s 246ms/step - loss: 4210664.5000 - val_loss: 1220369.0000\n",
            "Epoch 29/50\n",
            "5/5 [==============================] - 1s 250ms/step - loss: 3760988.2500 - val_loss: 2851685.5000\n",
            "1/1 [==============================] - 1s 653ms/step\n",
            "Template Eval Error: ValueError('Model DatepartRegression returned NaN for one or more series. fail_on_forecast_nan=True') in model 661: DatepartRegression\n",
            "Model Number: 662 with model Theta in generation 4 of 5\n",
            "Model Number: 663 with model NVAR in generation 4 of 5\n",
            "Model Number: 664 with model NVAR in generation 4 of 5\n",
            "Model Number: 665 with model SectionalMotif in generation 4 of 5\n",
            "Model Number: 666 with model AverageValueNaive in generation 4 of 5\n",
            "Model Number: 667 with model UnivariateRegression in generation 4 of 5\n",
            "Model Number: 668 with model MultivariateMotif in generation 4 of 5\n",
            "Model Number: 669 with model AverageValueNaive in generation 4 of 5\n",
            "Model Number: 670 with model ETS in generation 4 of 5\n",
            "Model Number: 671 with model WindowRegression in generation 4 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/jmk96dzx.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/rbaw5adt.json\n",
            "DEBUG:cmdstanpy:idx 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 672 with model FBProphet in generation 4 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=24199', 'data', 'file=/tmp/tmp11_dwuur/jmk96dzx.json', 'init=/tmp/tmp11_dwuur/rbaw5adt.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_modelv634n414/prophet_model-20221223063914.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "06:39:14 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "06:39:15 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 673 with model SeasonalNaive in generation 4 of 5\n",
            "Model Number: 674 with model NVAR in generation 4 of 5\n",
            "Model Number: 675 with model Theta in generation 4 of 5\n",
            "Model Number: 676 with model NVAR in generation 4 of 5\n",
            "Model Number: 677 with model GLS in generation 4 of 5\n",
            "Model Number: 678 with model SectionalMotif in generation 4 of 5\n",
            "Model Number: 679 with model SeasonalNaive in generation 4 of 5\n",
            "Model Number: 680 with model LastValueNaive in generation 4 of 5\n",
            "Model Number: 681 with model ETS in generation 4 of 5\n",
            "Model Number: 682 with model FBProphet in generation 4 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/s5a1tfkm.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/1yqeq6si.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=36923', 'data', 'file=/tmp/tmp11_dwuur/s5a1tfkm.json', 'init=/tmp/tmp11_dwuur/1yqeq6si.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_models6f686jg/prophet_model-20221223063918.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "06:39:18 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "06:39:18 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 683 with model NVAR in generation 4 of 5\n",
            "Model Number: 684 with model UnivariateMotif in generation 4 of 5\n",
            "Model Number: 685 with model SectionalMotif in generation 4 of 5\n",
            "Model Number: 686 with model UnivariateMotif in generation 4 of 5\n",
            "Model Number: 687 with model MetricMotif in generation 4 of 5\n",
            "Model Number: 688 with model Theta in generation 4 of 5\n",
            "New Generation: 5 of 5\n",
            "Model Number: 689 with model MultivariateRegression in generation 5 of 5\n",
            "Template Eval Error: Exception('Transformer Cointegration failed on fit') in model 689: MultivariateRegression\n",
            "Model Number: 690 with model SectionalMotif in generation 5 of 5\n",
            "Template Eval Error: ValueError(\"regression_type=='User' but no future_regressor supplied\") in model 690: SectionalMotif\n",
            "Model Number: 691 with model MultivariateMotif in generation 5 of 5\n",
            "Model Number: 692 with model AverageValueNaive in generation 5 of 5\n",
            "Model Number: 693 with model UnivariateMotif in generation 5 of 5\n",
            "Model Number: 694 with model SeasonalNaive in generation 5 of 5\n",
            "Model Number: 695 with model ETS in generation 5 of 5\n",
            "Model Number: 696 with model WindowRegression in generation 5 of 5\n",
            "Model Number: 697 with model Theta in generation 5 of 5\n",
            "Model Number: 698 with model SectionalMotif in generation 5 of 5\n",
            "Model Number: 699 with model MultivariateRegression in generation 5 of 5\n",
            "Model Number: 700 with model LastValueNaive in generation 5 of 5\n",
            "Model Number: 701 with model UnivariateRegression in generation 5 of 5\n",
            "Model Number: 702 with model MultivariateMotif in generation 5 of 5\n",
            "Model Number: 703 with model Theta in generation 5 of 5\n",
            "Model Number: 704 with model AverageValueNaive in generation 5 of 5\n",
            "Model Number: 705 with model SeasonalNaive in generation 5 of 5\n",
            "Model Number: 706 with model MetricMotif in generation 5 of 5\n",
            "Model Number: 707 with model UnivariateMotif in generation 5 of 5\n",
            "Model Number: 708 with model MultivariateMotif in generation 5 of 5\n",
            "Model Number: 709 with model AverageValueNaive in generation 5 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 710 with model FBProphet in generation 5 of 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/ubt1num_.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/l9w5ndgx.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=56724', 'data', 'file=/tmp/tmp11_dwuur/ubt1num_.json', 'init=/tmp/tmp11_dwuur/l9w5ndgx.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_modelzual2fvt/prophet_model-20221223063921.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "06:39:21 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "06:39:21 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Number: 711 with model GLM in generation 5 of 5\n",
            "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.') in model 711: GLM\n",
            "Model Number: 712 with model ETS in generation 5 of 5\n",
            "ETS failed on Close with ValueError('could not broadcast input array from shape (190,) into shape (364,)')\n",
            "Model Number: 713 with model ConstantNaive in generation 5 of 5\n",
            "Model Number: 714 with model ETS in generation 5 of 5\n",
            "Model Number: 715 with model GLS in generation 5 of 5\n",
            "Model Number: 716 with model Theta in generation 5 of 5\n",
            "Model Number: 717 with model Theta in generation 5 of 5\n",
            "Model Number: 718 with model UnivariateMotif in generation 5 of 5\n",
            "Model Number: 719 with model ETS in generation 5 of 5\n",
            "Model Number: 720 with model UnobservedComponents in generation 5 of 5\n",
            "Model Number: 721 with model UnivariateMotif in generation 5 of 5\n",
            "Model Number: 722 with model UnivariateMotif in generation 5 of 5\n",
            "Model Number: 723 with model GLM in generation 5 of 5\n",
            "Template Eval Error: ValueError('NaN, inf or invalid value detected in weights, estimation infeasible.') in model 723: GLM\n",
            "Model Number: 724 with model ETS in generation 5 of 5\n",
            "Model Number: 725 with model Theta in generation 5 of 5\n",
            "Model Number: 726 with model MultivariateMotif in generation 5 of 5\n",
            "Model Number: 727 with model Theta in generation 5 of 5\n",
            "Template Eval Error: Exception('Transformer BTCD failed on fit') in model 727: Theta\n",
            "Model Number: 728 with model AverageValueNaive in generation 5 of 5\n",
            "Model Number: 729 with model NVAR in generation 5 of 5\n",
            "Model Number: 730 with model UnobservedComponents in generation 5 of 5\n",
            "Model Number: 731 with model MultivariateRegression in generation 5 of 5\n",
            "Model Number: 732 with model UnobservedComponents in generation 5 of 5\n",
            "Model Number: 733 with model FBProphet in generation 5 of 5\n",
            "Template Eval Error: Exception('Transformer Detrend failed on fit') in model 733: FBProphet\n",
            "Model Number: 734 with model MultivariateMotif in generation 5 of 5\n",
            "Model Number: 735 with model WindowRegression in generation 5 of 5\n",
            "Model Number: 736 with model SeasonalNaive in generation 5 of 5\n",
            "Model Number: 737 with model LastValueNaive in generation 5 of 5\n",
            "Model Number: 738 with model UnivariateMotif in generation 5 of 5\n",
            "Model Number: 739 with model MultivariateRegression in generation 5 of 5\n",
            "Model Number: 740 with model ARIMA in generation 5 of 5\n",
            "Model Number: 741 with model NVAR in generation 5 of 5\n",
            "Model Number: 742 with model ARIMA in generation 5 of 5\n",
            "Model Number: 743 with model ARIMA in generation 5 of 5\n",
            "Model Number: 744 with model LastValueNaive in generation 5 of 5\n",
            "Model Number: 745 with model LastValueNaive in generation 5 of 5\n",
            "Model Number: 746 with model WindowRegression in generation 5 of 5\n",
            "Model Number: 747 with model Theta in generation 5 of 5\n",
            "Model Number: 748 with model Theta in generation 5 of 5\n",
            "Template Eval Error: Exception('Transformer BTCD failed on fit') in model 748: Theta\n",
            "Model Number: 749 with model DatepartRegression in generation 5 of 5\n",
            "Template Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 749: DatepartRegression\n",
            "Model Number: 750 with model GLM in generation 5 of 5\n",
            "Model Number: 751 with model ARIMA in generation 5 of 5\n",
            "Model Number: 752 with model ARIMA in generation 5 of 5\n",
            "Model Number: 753 with model DatepartRegression in generation 5 of 5\n",
            "Template Eval Error: ValueError('Model DatepartRegression returned NaN for one or more series. fail_on_forecast_nan=True') in model 753: DatepartRegression\n",
            "Model Number: 754 with model SectionalMotif in generation 5 of 5\n",
            "Model Number: 755 with model NVAR in generation 5 of 5\n",
            "Model Number: 756 with model AverageValueNaive in generation 5 of 5\n",
            "Model Number: 757 with model UnobservedComponents in generation 5 of 5\n",
            "Model Number: 758 with model MultivariateMotif in generation 5 of 5\n",
            "Model Number: 759 with model GLS in generation 5 of 5\n",
            "Model Number: 760 with model UnivariateMotif in generation 5 of 5\n",
            "Model Number: 761 with model ConstantNaive in generation 5 of 5\n",
            "Model Number: 762 with model MultivariateMotif in generation 5 of 5\n",
            "Model Number: 763 with model ConstantNaive in generation 5 of 5\n",
            "Model Number: 764 with model SeasonalNaive in generation 5 of 5\n",
            "Model Number: 765 with model ETS in generation 5 of 5\n",
            "ETS failed on Close with ValueError('could not broadcast input array from shape (190,) into shape (364,)')\n",
            "Model Number: 766 with model GLS in generation 5 of 5\n",
            "Template Eval Error: Exception('Transformer Cointegration failed on fit') in model 766: GLS\n",
            "Model Number: 767 with model MultivariateMotif in generation 5 of 5\n",
            "Model Number: 768 with model ARIMA in generation 5 of 5\n",
            "Model Number: 769 with model MultivariateMotif in generation 5 of 5\n",
            "Model Number: 770 with model SectionalMotif in generation 5 of 5\n",
            "Model Number: 771 with model AverageValueNaive in generation 5 of 5\n",
            "Model Number: 772 with model NVAR in generation 5 of 5\n",
            "Model Number: 773 with model ARIMA in generation 5 of 5\n",
            "Model Number: 774 with model LastValueNaive in generation 5 of 5\n",
            "Model Number: 775 with model NVAR in generation 5 of 5\n",
            "Model Number: 776 with model Theta in generation 5 of 5\n",
            "Model Number: 777 with model MultivariateRegression in generation 5 of 5\n",
            "Template Eval Error: Exception('Transformer Detrend failed on fit') in model 777: MultivariateRegression\n",
            "Model Number: 778 with model LastValueNaive in generation 5 of 5\n",
            "Model Number: 779 with model SectionalMotif in generation 5 of 5\n",
            "Model Number: 780 with model MetricMotif in generation 5 of 5\n",
            "Model Number: 781 with model UnivariateMotif in generation 5 of 5\n",
            "Model Number: 782 with model UnivariateMotif in generation 5 of 5\n",
            "Model Number: 783 with model DatepartRegression in generation 5 of 5\n",
            "Template Eval Error: ValueError('X has 26 features, but AdaBoostRegressor is expecting 36 features as input.') in model 783: DatepartRegression\n",
            "Model Number: 784 with model NVAR in generation 5 of 5\n",
            "Model Number: 785 with model MultivariateMotif in generation 5 of 5\n",
            "Model Number: 786 with model UnivariateMotif in generation 5 of 5\n",
            "Template Eval Error: Exception('Transformer Cointegration failed on fit') in model 786: UnivariateMotif\n",
            "Model Number: 787 with model NVAR in generation 5 of 5\n",
            "Model Number: 788 with model ETS in generation 5 of 5\n",
            "ETS error ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\n",
            "ETS failed on Close with ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\n",
            "Model Number: 789 with model Ensemble in generation 6 of Ensembles\n",
            "Model Number: 790 with model Ensemble in generation 6 of Ensembles\n",
            "Model Number: 791 with model Ensemble in generation 6 of Ensembles\n",
            "Model Number: 792 with model Ensemble in generation 6 of Ensembles\n",
            "Model Number: 793 with model Ensemble in generation 6 of Ensembles\n",
            "Model Number: 794 with model Ensemble in generation 6 of Ensembles\n",
            "Model Number: 795 with model Ensemble in generation 6 of Ensembles\n",
            "Model Number: 796 with model Ensemble in generation 6 of Ensembles\n",
            "Validation Round: 1\n",
            "Model Number: 1 of 119 with model Ensemble for Validation 1\n",
            "📈 1 - Ensemble with avg smape 1.29: \n",
            "Model Number: 2 of 119 with model Ensemble for Validation 1\n",
            "📈 2 - Ensemble with avg smape 1.28: \n",
            "Model Number: 3 of 119 with model Ensemble for Validation 1\n",
            "3 - Ensemble with avg smape 1.3: \n",
            "Model Number: 4 of 119 with model NVAR for Validation 1\n",
            "📈 4 - NVAR with avg smape 1.24: \n",
            "Model Number: 5 of 119 with model NVAR for Validation 1\n",
            "5 - NVAR with avg smape 1.24: \n",
            "Model Number: 6 of 119 with model NVAR for Validation 1\n",
            "6 - NVAR with avg smape 1.75: \n",
            "Model Number: 7 of 119 with model NVAR for Validation 1\n",
            "7 - NVAR with avg smape 1.28: \n",
            "Model Number: 8 of 119 with model Ensemble for Validation 1\n",
            "8 - Ensemble with avg smape 1.53: \n",
            "Model Number: 9 of 119 with model Ensemble for Validation 1\n",
            "9 - Ensemble with avg smape 1.55: \n",
            "Model Number: 10 of 119 with model NVAR for Validation 1\n",
            "10 - NVAR with avg smape 1.28: \n",
            "Model Number: 11 of 119 with model Ensemble for Validation 1\n",
            "11 - Ensemble with avg smape 1.27: \n",
            "Model Number: 12 of 119 with model NVAR for Validation 1\n",
            "12 - NVAR with avg smape 5.4: \n",
            "Model Number: 13 of 119 with model UnivariateMotif for Validation 1\n",
            "📈 13 - UnivariateMotif with avg smape 1.19: \n",
            "Model Number: 14 of 119 with model UnivariateMotif for Validation 1\n",
            "14 - UnivariateMotif with avg smape 1.31: \n",
            "Model Number: 15 of 119 with model UnivariateMotif for Validation 1\n",
            "15 - UnivariateMotif with avg smape 5.1: \n",
            "Model Number: 16 of 119 with model UnivariateMotif for Validation 1\n",
            "16 - UnivariateMotif with avg smape 1.46: \n",
            "Model Number: 17 of 119 with model UnivariateMotif for Validation 1\n",
            "17 - UnivariateMotif with avg smape 4.89: \n",
            "Model Number: 18 of 119 with model UnivariateMotif for Validation 1\n",
            "📈 18 - UnivariateMotif with avg smape 1.14: \n",
            "Model Number: 19 of 119 with model MultivariateMotif for Validation 1\n",
            "19 - MultivariateMotif with avg smape 1.5: \n",
            "Model Number: 20 of 119 with model ARIMA for Validation 1\n",
            "20 - ARIMA with avg smape 2.64: \n",
            "Model Number: 21 of 119 with model MultivariateRegression for Validation 1\n",
            "21 - MultivariateRegression with avg smape 1.19: \n",
            "Model Number: 22 of 119 with model ARIMA for Validation 1\n",
            "22 - ARIMA with avg smape 5.84: \n",
            "Model Number: 23 of 119 with model ARIMA for Validation 1\n",
            "23 - ARIMA with avg smape 5.92: \n",
            "Model Number: 24 of 119 with model LastValueNaive for Validation 1\n",
            "24 - LastValueNaive with avg smape 1.78: \n",
            "Model Number: 25 of 119 with model LastValueNaive for Validation 1\n",
            "25 - LastValueNaive with avg smape 1.7: \n",
            "Model Number: 26 of 119 with model LastValueNaive for Validation 1\n",
            "26 - LastValueNaive with avg smape 1.56: \n",
            "Model Number: 27 of 119 with model LastValueNaive for Validation 1\n",
            "27 - LastValueNaive with avg smape 1.58: \n",
            "Model Number: 28 of 119 with model MultivariateMotif for Validation 1\n",
            "28 - MultivariateMotif with avg smape 2.09: \n",
            "Model Number: 29 of 119 with model MultivariateMotif for Validation 1\n",
            "29 - MultivariateMotif with avg smape 1.37: \n",
            "Model Number: 30 of 119 with model AverageValueNaive for Validation 1\n",
            "30 - AverageValueNaive with avg smape 15.77: \n",
            "Model Number: 31 of 119 with model SectionalMotif for Validation 1\n",
            "31 - SectionalMotif with avg smape 2.16: \n",
            "Model Number: 32 of 119 with model LastValueNaive for Validation 1\n",
            "32 - LastValueNaive with avg smape 1.74: \n",
            "Model Number: 33 of 119 with model LastValueNaive for Validation 1\n",
            "33 - LastValueNaive with avg smape 1.64: \n",
            "Model Number: 34 of 119 with model AverageValueNaive for Validation 1\n",
            "34 - AverageValueNaive with avg smape 5.69: \n",
            "Model Number: 35 of 119 with model MultivariateMotif for Validation 1\n",
            "35 - MultivariateMotif with avg smape 1.15: \n",
            "Model Number: 36 of 119 with model MultivariateMotif for Validation 1\n",
            "36 - MultivariateMotif with avg smape 1.43: \n",
            "Model Number: 37 of 119 with model MultivariateMotif for Validation 1\n",
            "37 - MultivariateMotif with avg smape 1.42: \n",
            "Model Number: 38 of 119 with model ETS for Validation 1\n",
            "ETS failed on Close with ValueError('could not broadcast input array from shape (175,) into shape (420,)')\n",
            "38 - ETS with avg smape 3.23: \n",
            "Model Number: 39 of 119 with model SeasonalNaive for Validation 1\n",
            "39 - SeasonalNaive with avg smape 4.52: \n",
            "Model Number: 40 of 119 with model ARIMA for Validation 1\n",
            "40 - ARIMA with avg smape 2.35: \n",
            "Model Number: 41 of 119 with model ARIMA for Validation 1\n",
            "41 - ARIMA with avg smape 2.35: \n",
            "Model Number: 42 of 119 with model ARIMA for Validation 1\n",
            "42 - ARIMA with avg smape 1.79: \n",
            "Model Number: 43 of 119 with model ETS for Validation 1\n",
            "ETS failed on Close with ValueError('could not broadcast input array from shape (180,) into shape (364,)')\n",
            "43 - ETS with avg smape 2.8: \n",
            "Model Number: 44 of 119 with model Theta for Validation 1\n",
            "44 - Theta with avg smape 3.59: \n",
            "Model Number: 45 of 119 with model SectionalMotif for Validation 1\n",
            "45 - SectionalMotif with avg smape 2.99: \n",
            "Model Number: 46 of 119 with model MultivariateRegression for Validation 1\n",
            "46 - MultivariateRegression with avg smape 4.13: \n",
            "Model Number: 47 of 119 with model SeasonalNaive for Validation 1\n",
            "47 - SeasonalNaive with avg smape 1.95: \n",
            "Model Number: 48 of 119 with model AverageValueNaive for Validation 1\n",
            "48 - AverageValueNaive with avg smape 7.52: \n",
            "Model Number: 49 of 119 with model Theta for Validation 1\n",
            "49 - Theta with avg smape 2.58: \n",
            "Model Number: 50 of 119 with model Theta for Validation 1\n",
            "50 - Theta with avg smape 3.2: \n",
            "Model Number: 51 of 119 with model SectionalMotif for Validation 1\n",
            "51 - SectionalMotif with avg smape 2.71: \n",
            "Model Number: 52 of 119 with model AverageValueNaive for Validation 1\n",
            "52 - AverageValueNaive with avg smape 7.67: \n",
            "Model Number: 53 of 119 with model SectionalMotif for Validation 1\n",
            "53 - SectionalMotif with avg smape 1.73: \n",
            "Model Number: 54 of 119 with model AverageValueNaive for Validation 1\n",
            "54 - AverageValueNaive with avg smape 7.12: \n",
            "Model Number: 55 of 119 with model Theta for Validation 1\n",
            "55 - Theta with avg smape 4.35: \n",
            "Model Number: 56 of 119 with model ETS for Validation 1\n",
            "56 - ETS with avg smape 1.65: \n",
            "Model Number: 57 of 119 with model AverageValueNaive for Validation 1\n",
            "57 - AverageValueNaive with avg smape 1.36: \n",
            "Model Number: 58 of 119 with model GLS for Validation 1\n",
            "58 - GLS with avg smape 2.71: \n",
            "Model Number: 59 of 119 with model Theta for Validation 1\n",
            "59 - Theta with avg smape 2.87: \n",
            "Model Number: 60 of 119 with model Theta for Validation 1\n",
            "60 - Theta with avg smape 2.8: \n",
            "Model Number: 61 of 119 with model SeasonalNaive for Validation 1\n",
            "61 - SeasonalNaive with avg smape 1.93: \n",
            "Model Number: 62 of 119 with model UnobservedComponents for Validation 1\n",
            "62 - UnobservedComponents with avg smape 2.38: \n",
            "Model Number: 63 of 119 with model SectionalMotif for Validation 1\n",
            "63 - SectionalMotif with avg smape 2.84: \n",
            "Model Number: 64 of 119 with model WindowRegression for Validation 1\n",
            "64 - WindowRegression with avg smape 2.0: \n",
            "Model Number: 65 of 119 with model DatepartRegression for Validation 1\n",
            "65 - DatepartRegression with avg smape 1.6: \n",
            "Model Number: 66 of 119 with model ETS for Validation 1\n",
            "66 - ETS with avg smape 3.22: \n",
            "Model Number: 67 of 119 with model UnobservedComponents for Validation 1\n",
            "67 - UnobservedComponents with avg smape 2.71: \n",
            "Model Number: 68 of 119 with model GLS for Validation 1\n",
            "68 - GLS with avg smape 5.49: \n",
            "Model Number: 69 of 119 with model DatepartRegression for Validation 1\n",
            "Epoch 1/200\n",
            "6/6 [==============================] - 6s 34ms/step - loss: 0.1963\n",
            "Epoch 2/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0903\n",
            "Epoch 3/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0977\n",
            "Epoch 4/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0941\n",
            "Epoch 5/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0862\n",
            "Epoch 6/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0877\n",
            "Epoch 7/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0893\n",
            "Epoch 8/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0972\n",
            "Epoch 9/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0882\n",
            "Epoch 10/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0854\n",
            "Epoch 11/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0892\n",
            "Epoch 12/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0904\n",
            "Epoch 13/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0846\n",
            "Epoch 14/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0897\n",
            "Epoch 15/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.0921\n",
            "Epoch 16/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0943\n",
            "Epoch 17/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0865\n",
            "Epoch 18/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0893\n",
            "Epoch 19/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0848\n",
            "Epoch 20/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0859\n",
            "Epoch 21/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0910\n",
            "Epoch 22/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0972\n",
            "Epoch 23/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0860\n",
            "Epoch 24/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.0906\n",
            "Epoch 25/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0853\n",
            "Epoch 26/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0845\n",
            "Epoch 27/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0971\n",
            "Epoch 28/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0866\n",
            "Epoch 29/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.0893\n",
            "Epoch 30/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0845\n",
            "Epoch 31/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0847\n",
            "Epoch 32/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0860\n",
            "Epoch 33/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0880\n",
            "Epoch 34/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0883\n",
            "Epoch 35/200\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 0.0887\n",
            "Epoch 36/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0880\n",
            "Epoch 37/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0867\n",
            "Epoch 38/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0892\n",
            "Epoch 39/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0878\n",
            "Epoch 40/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0860\n",
            "Epoch 41/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0869\n",
            "Epoch 42/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0882\n",
            "Epoch 43/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0846\n",
            "Epoch 44/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0894\n",
            "Epoch 45/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0871\n",
            "Epoch 46/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0916\n",
            "Epoch 47/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0876\n",
            "Epoch 48/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0906\n",
            "Epoch 49/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0876\n",
            "Epoch 50/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0868\n",
            "Epoch 51/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0853\n",
            "Epoch 52/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0898\n",
            "Epoch 53/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0882\n",
            "Epoch 54/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0867\n",
            "Epoch 55/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0857\n",
            "Epoch 56/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0890\n",
            "Epoch 57/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0857\n",
            "Epoch 58/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0833\n",
            "Epoch 59/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0855\n",
            "Epoch 60/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0857\n",
            "Epoch 61/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0838\n",
            "Epoch 62/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0840\n",
            "Epoch 63/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0903\n",
            "Epoch 64/200\n",
            "6/6 [==============================] - 0s 42ms/step - loss: 0.0840\n",
            "Epoch 65/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0852\n",
            "Epoch 66/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0817\n",
            "Epoch 67/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0855\n",
            "Epoch 68/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0827\n",
            "Epoch 69/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0787\n",
            "Epoch 70/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0832\n",
            "Epoch 71/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0766\n",
            "Epoch 72/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0859\n",
            "Epoch 73/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0694\n",
            "Epoch 74/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0727\n",
            "Epoch 75/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0739\n",
            "Epoch 76/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0610\n",
            "Epoch 77/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0748\n",
            "Epoch 78/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0534\n",
            "Epoch 79/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0705\n",
            "Epoch 80/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0495\n",
            "Epoch 81/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0680\n",
            "Epoch 82/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0572\n",
            "Epoch 83/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0565\n",
            "Epoch 84/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0576\n",
            "Epoch 85/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0614\n",
            "Epoch 86/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0575\n",
            "Epoch 87/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0465\n",
            "Epoch 88/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0667\n",
            "Epoch 89/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0426\n",
            "Epoch 90/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0565\n",
            "Epoch 91/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0635\n",
            "Epoch 92/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0498\n",
            "Epoch 93/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0360\n",
            "Epoch 94/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0429\n",
            "Epoch 95/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0436\n",
            "Epoch 96/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0441\n",
            "Epoch 97/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0481\n",
            "Epoch 98/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0348\n",
            "Epoch 99/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0441\n",
            "Epoch 100/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0381\n",
            "Epoch 101/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0341\n",
            "Epoch 102/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0476\n",
            "Epoch 103/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0402\n",
            "Epoch 104/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0427\n",
            "Epoch 105/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0433\n",
            "Epoch 106/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0356\n",
            "Epoch 107/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0308\n",
            "Epoch 108/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0464\n",
            "Epoch 109/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0316\n",
            "Epoch 110/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0346\n",
            "Epoch 111/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0269\n",
            "Epoch 112/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0451\n",
            "Epoch 113/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0271\n",
            "Epoch 114/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0389\n",
            "Epoch 115/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0448\n",
            "Epoch 116/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0279\n",
            "Epoch 117/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0336\n",
            "Epoch 118/200\n",
            "6/6 [==============================] - 0s 42ms/step - loss: 0.0311\n",
            "Epoch 119/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0331\n",
            "Epoch 120/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0398\n",
            "Epoch 121/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0288\n",
            "Epoch 122/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0258\n",
            "Epoch 123/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0369\n",
            "Epoch 124/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0386\n",
            "Epoch 125/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0293\n",
            "Epoch 126/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0287\n",
            "Epoch 127/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0323\n",
            "Epoch 128/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0228\n",
            "Epoch 129/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0366\n",
            "Epoch 130/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0267\n",
            "Epoch 131/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0301\n",
            "Epoch 132/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0265\n",
            "Epoch 133/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0297\n",
            "Epoch 134/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0323\n",
            "Epoch 135/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0233\n",
            "Epoch 136/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0359\n",
            "Epoch 137/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0200\n",
            "Epoch 138/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0296\n",
            "Epoch 139/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0219\n",
            "Epoch 140/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0210\n",
            "Epoch 141/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0309\n",
            "Epoch 142/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0248\n",
            "Epoch 143/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0280\n",
            "Epoch 144/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0372\n",
            "Epoch 145/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0226\n",
            "Epoch 146/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0261\n",
            "Epoch 147/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0264\n",
            "Epoch 148/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0203\n",
            "Epoch 149/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0197\n",
            "Epoch 150/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0268\n",
            "Epoch 151/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0358\n",
            "Epoch 152/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0232\n",
            "Epoch 153/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0229\n",
            "Epoch 154/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0260\n",
            "Epoch 155/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0236\n",
            "Epoch 156/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0242\n",
            "Epoch 157/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0178\n",
            "Epoch 158/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0295\n",
            "Epoch 159/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0176\n",
            "Epoch 160/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0190\n",
            "Epoch 161/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0287\n",
            "Epoch 162/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0172\n",
            "Epoch 163/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0223\n",
            "Epoch 164/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0275\n",
            "Epoch 165/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.0284\n",
            "Epoch 166/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0174\n",
            "Epoch 167/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0225\n",
            "Epoch 168/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0216\n",
            "Epoch 169/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.0185\n",
            "Epoch 170/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.0291\n",
            "Epoch 171/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0245\n",
            "Epoch 172/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0197\n",
            "Epoch 173/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0244\n",
            "Epoch 174/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0231\n",
            "Epoch 175/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0154\n",
            "Epoch 176/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0244\n",
            "Epoch 177/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0192\n",
            "Epoch 178/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0206\n",
            "Epoch 179/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0216\n",
            "Epoch 180/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0148\n",
            "Epoch 181/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0324\n",
            "Epoch 182/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0163\n",
            "Epoch 183/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0173\n",
            "Epoch 184/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0200\n",
            "Epoch 185/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0193\n",
            "Epoch 186/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0195\n",
            "Epoch 187/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0263\n",
            "Epoch 188/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0143\n",
            "Epoch 189/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0212\n",
            "Epoch 190/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0180\n",
            "Epoch 191/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0189\n",
            "Epoch 192/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0251\n",
            "Epoch 193/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0183\n",
            "Epoch 194/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0188\n",
            "Epoch 195/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0190\n",
            "Epoch 196/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0203\n",
            "Epoch 197/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0192\n",
            "Epoch 198/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0133\n",
            "Epoch 199/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0267\n",
            "Epoch 200/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0149\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "69 - DatepartRegression with avg smape 8.49: \n",
            "Model Number: 70 of 119 with model SeasonalNaive for Validation 1\n",
            "70 - SeasonalNaive with avg smape 8.27: \n",
            "Model Number: 71 of 119 with model SectionalMotif for Validation 1\n",
            "71 - SectionalMotif with avg smape 2.29: \n",
            "Model Number: 72 of 119 with model MultivariateRegression for Validation 1\n",
            "72 - MultivariateRegression with avg smape 1.88: \n",
            "Model Number: 73 of 119 with model DatepartRegression for Validation 1\n",
            "73 - DatepartRegression with avg smape 2.99: \n",
            "Model Number: 74 of 119 with model MultivariateRegression for Validation 1\n",
            "74 - MultivariateRegression with avg smape 2.71: \n",
            "Model Number: 75 of 119 with model UnobservedComponents for Validation 1\n",
            "75 - UnobservedComponents with avg smape 2.63: \n",
            "Model Number: 76 of 119 with model SeasonalNaive for Validation 1\n",
            "76 - SeasonalNaive with avg smape 1.96: \n",
            "Model Number: 77 of 119 with model MultivariateRegression for Validation 1\n",
            "77 - MultivariateRegression with avg smape 2.71: \n",
            "Model Number: 78 of 119 with model UnobservedComponents for Validation 1\n",
            "78 - UnobservedComponents with avg smape 3.03: \n",
            "Model Number: 79 of 119 with model UnobservedComponents for Validation 1\n",
            "79 - UnobservedComponents with avg smape 3.03: \n",
            "Model Number: 80 of 119 with model UnobservedComponents for Validation 1\n",
            "80 - UnobservedComponents with avg smape 2.71: \n",
            "Model Number: 81 of 119 with model WindowRegression for Validation 1\n",
            "81 - WindowRegression with avg smape 2.71: \n",
            "Model Number: 82 of 119 with model WindowRegression for Validation 1\n",
            "82 - WindowRegression with avg smape 3.08: \n",
            "Model Number: 83 of 119 with model MultivariateRegression for Validation 1\n",
            "83 - MultivariateRegression with avg smape 2.71: \n",
            "Model Number: 84 of 119 with model WindowRegression for Validation 1\n",
            "84 - WindowRegression with avg smape 1.63: \n",
            "Model Number: 85 of 119 with model MetricMotif for Validation 1\n",
            "85 - MetricMotif with avg smape 3.26: \n",
            "Model Number: 86 of 119 with model GLS for Validation 1\n",
            "86 - GLS with avg smape 1.94: \n",
            "Model Number: 87 of 119 with model ETS for Validation 1\n",
            "87 - ETS with avg smape 2.76: \n",
            "Model Number: 88 of 119 with model GLS for Validation 1\n",
            "88 - GLS with avg smape 3.02: \n",
            "Model Number: 89 of 119 with model UnivariateRegression for Validation 1\n",
            "89 - UnivariateRegression with avg smape 1.27: \n",
            "Model Number: 90 of 119 with model ETS for Validation 1\n",
            "90 - ETS with avg smape 6.42: \n",
            "Model Number: 91 of 119 with model SeasonalNaive for Validation 1\n",
            "91 - SeasonalNaive with avg smape 2.54: \n",
            "Model Number: 92 of 119 with model GLS for Validation 1\n",
            "92 - GLS with avg smape 7.18: \n",
            "Model Number: 93 of 119 with model MetricMotif for Validation 1\n",
            "93 - MetricMotif with avg smape 2.31: \n",
            "Model Number: 94 of 119 with model WindowRegression for Validation 1\n",
            "94 - WindowRegression with avg smape 2.26: \n",
            "Model Number: 95 of 119 with model GLM for Validation 1\n",
            "95 - GLM with avg smape 2.67: \n",
            "Model Number: 96 of 119 with model DatepartRegression for Validation 1\n",
            "Epoch 1/200\n",
            "6/6 [==============================] - 5s 35ms/step - loss: 0.3491\n",
            "Epoch 2/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.3365\n",
            "Epoch 3/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3402\n",
            "Epoch 4/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.3374\n",
            "Epoch 5/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.3354\n",
            "Epoch 6/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3356\n",
            "Epoch 7/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.3358\n",
            "Epoch 8/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3383\n",
            "Epoch 9/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.3367\n",
            "Epoch 10/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.3355\n",
            "Epoch 11/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.3362\n",
            "Epoch 12/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.3366\n",
            "Epoch 13/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3370\n",
            "Epoch 14/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3361\n",
            "Epoch 15/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3361\n",
            "Epoch 16/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3375\n",
            "Epoch 17/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3356\n",
            "Epoch 18/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.3358\n",
            "Epoch 19/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3363\n",
            "Epoch 20/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3352\n",
            "Epoch 21/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.3362\n",
            "Epoch 22/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.3373\n",
            "Epoch 23/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.3359\n",
            "Epoch 24/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.3372\n",
            "Epoch 25/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.3352\n",
            "Epoch 26/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.3355\n",
            "Epoch 27/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3390\n",
            "Epoch 28/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3362\n",
            "Epoch 29/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3369\n",
            "Epoch 30/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3352\n",
            "Epoch 31/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3351\n",
            "Epoch 32/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.3354\n",
            "Epoch 33/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3367\n",
            "Epoch 34/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.3363\n",
            "Epoch 35/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.3361\n",
            "Epoch 36/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3359\n",
            "Epoch 37/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3378\n",
            "Epoch 38/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.3363\n",
            "Epoch 39/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3357\n",
            "Epoch 40/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.3359\n",
            "Epoch 41/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.3357\n",
            "Epoch 42/200\n",
            "6/6 [==============================] - 0s 42ms/step - loss: 0.3361\n",
            "Epoch 43/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.3353\n",
            "Epoch 44/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.3366\n",
            "Epoch 45/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.3355\n",
            "Epoch 46/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3361\n",
            "Epoch 47/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.3366\n",
            "Epoch 48/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.3354\n",
            "Epoch 49/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.3358\n",
            "Epoch 50/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3345\n",
            "Epoch 51/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.3340\n",
            "Epoch 52/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.3348\n",
            "Epoch 53/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.3358\n",
            "Epoch 54/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.3333\n",
            "Epoch 55/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3293\n",
            "Epoch 56/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.3369\n",
            "Epoch 57/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.3265\n",
            "Epoch 58/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.3146\n",
            "Epoch 59/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.3113\n",
            "Epoch 60/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.3193\n",
            "Epoch 61/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.2833\n",
            "Epoch 62/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.2872\n",
            "Epoch 63/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.3047\n",
            "Epoch 64/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.2637\n",
            "Epoch 65/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.2442\n",
            "Epoch 66/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.2551\n",
            "Epoch 67/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.2550\n",
            "Epoch 68/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.2550\n",
            "Epoch 69/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.2042\n",
            "Epoch 70/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.2523\n",
            "Epoch 71/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.2018\n",
            "Epoch 72/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.2391\n",
            "Epoch 73/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.1767\n",
            "Epoch 74/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.2102\n",
            "Epoch 75/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.2395\n",
            "Epoch 76/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1588\n",
            "Epoch 77/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.1411\n",
            "Epoch 78/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.1884\n",
            "Epoch 79/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.2304\n",
            "Epoch 80/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1412\n",
            "Epoch 81/200\n",
            "6/6 [==============================] - 0s 41ms/step - loss: 0.1968\n",
            "Epoch 82/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.1275\n",
            "Epoch 83/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.1413\n",
            "Epoch 84/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1639\n",
            "Epoch 85/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1636\n",
            "Epoch 86/200\n",
            "6/6 [==============================] - 0s 41ms/step - loss: 0.1418\n",
            "Epoch 87/200\n",
            "6/6 [==============================] - 0s 41ms/step - loss: 0.1302\n",
            "Epoch 88/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.1249\n",
            "Epoch 89/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.1185\n",
            "Epoch 90/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.1581\n",
            "Epoch 91/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.1628\n",
            "Epoch 92/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1267\n",
            "Epoch 93/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1121\n",
            "Epoch 94/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.1154\n",
            "Epoch 95/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0941\n",
            "Epoch 96/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.1525\n",
            "Epoch 97/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1327\n",
            "Epoch 98/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1249\n",
            "Epoch 99/200\n",
            "6/6 [==============================] - 0s 43ms/step - loss: 0.1069\n",
            "Epoch 100/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1100\n",
            "Epoch 101/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.1414\n",
            "Epoch 102/200\n",
            "6/6 [==============================] - 0s 41ms/step - loss: 0.0983\n",
            "Epoch 103/200\n",
            "6/6 [==============================] - 0s 65ms/step - loss: 0.1032\n",
            "Epoch 104/200\n",
            "6/6 [==============================] - 0s 57ms/step - loss: 0.1044\n",
            "Epoch 105/200\n",
            "6/6 [==============================] - 0s 53ms/step - loss: 0.1373\n",
            "Epoch 106/200\n",
            "6/6 [==============================] - 0s 67ms/step - loss: 0.0925\n",
            "Epoch 107/200\n",
            "6/6 [==============================] - 0s 74ms/step - loss: 0.1097\n",
            "Epoch 108/200\n",
            "6/6 [==============================] - 0s 60ms/step - loss: 0.1327\n",
            "Epoch 109/200\n",
            "6/6 [==============================] - 0s 56ms/step - loss: 0.0935\n",
            "Epoch 110/200\n",
            "6/6 [==============================] - 0s 56ms/step - loss: 0.0973\n",
            "Epoch 111/200\n",
            "6/6 [==============================] - 0s 65ms/step - loss: 0.0924\n",
            "Epoch 112/200\n",
            "6/6 [==============================] - 0s 60ms/step - loss: 0.1304\n",
            "Epoch 113/200\n",
            "6/6 [==============================] - 0s 56ms/step - loss: 0.0939\n",
            "Epoch 114/200\n",
            "6/6 [==============================] - 0s 51ms/step - loss: 0.1060\n",
            "Epoch 115/200\n",
            "6/6 [==============================] - 0s 60ms/step - loss: 0.1412\n",
            "Epoch 116/200\n",
            "6/6 [==============================] - 0s 56ms/step - loss: 0.0802\n",
            "Epoch 117/200\n",
            "6/6 [==============================] - 0s 42ms/step - loss: 0.0793\n",
            "Epoch 118/200\n",
            "6/6 [==============================] - 0s 42ms/step - loss: 0.1305\n",
            "Epoch 119/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.1028\n",
            "Epoch 120/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0884\n",
            "Epoch 121/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0759\n",
            "Epoch 122/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1131\n",
            "Epoch 123/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1066\n",
            "Epoch 124/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1015\n",
            "Epoch 125/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0781\n",
            "Epoch 126/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0958\n",
            "Epoch 127/200\n",
            "6/6 [==============================] - 0s 42ms/step - loss: 0.0907\n",
            "Epoch 128/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0709\n",
            "Epoch 129/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0865\n",
            "Epoch 130/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0920\n",
            "Epoch 131/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0980\n",
            "Epoch 132/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0808\n",
            "Epoch 133/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1017\n",
            "Epoch 134/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0787\n",
            "Epoch 135/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0653\n",
            "Epoch 136/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.1222\n",
            "Epoch 137/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0658\n",
            "Epoch 138/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0956\n",
            "Epoch 139/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0665\n",
            "Epoch 140/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0785\n",
            "Epoch 141/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.0805\n",
            "Epoch 142/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0965\n",
            "Epoch 143/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0739\n",
            "Epoch 144/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.1139\n",
            "Epoch 145/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.0632\n",
            "Epoch 146/200\n",
            "6/6 [==============================] - 0s 42ms/step - loss: 0.0733\n",
            "Epoch 147/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0864\n",
            "Epoch 148/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.0755\n",
            "Epoch 149/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.1036\n",
            "Epoch 150/200\n",
            "6/6 [==============================] - 0s 42ms/step - loss: 0.0632\n",
            "Epoch 151/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.1052\n",
            "Epoch 152/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.0693\n",
            "Epoch 153/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0613\n",
            "Epoch 154/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0821\n",
            "Epoch 155/200\n",
            "6/6 [==============================] - 0s 41ms/step - loss: 0.0857\n",
            "Epoch 156/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0715\n",
            "Epoch 157/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0651\n",
            "Epoch 158/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1026\n",
            "Epoch 159/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0655\n",
            "Epoch 160/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0623\n",
            "Epoch 161/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.1035\n",
            "Epoch 162/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0621\n",
            "Epoch 163/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0626\n",
            "Epoch 164/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0759\n",
            "Epoch 165/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1026\n",
            "Epoch 166/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0611\n",
            "Epoch 167/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0874\n",
            "Epoch 168/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0648\n",
            "Epoch 169/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0713\n",
            "Epoch 170/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0882\n",
            "Epoch 171/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0621\n",
            "Epoch 172/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0718\n",
            "Epoch 173/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0812\n",
            "Epoch 174/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.0843\n",
            "Epoch 175/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0576\n",
            "Epoch 176/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0588\n",
            "Epoch 177/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0693\n",
            "Epoch 178/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0555\n",
            "Epoch 179/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0946\n",
            "Epoch 180/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0561\n",
            "Epoch 181/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0724\n",
            "Epoch 182/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0593\n",
            "Epoch 183/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0700\n",
            "Epoch 184/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0880\n",
            "Epoch 185/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0573\n",
            "Epoch 186/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0585\n",
            "Epoch 187/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0952\n",
            "Epoch 188/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0530\n",
            "Epoch 189/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0591\n",
            "Epoch 190/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0713\n",
            "Epoch 191/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.0581\n",
            "Epoch 192/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0643\n",
            "Epoch 193/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0771\n",
            "Epoch 194/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0624\n",
            "Epoch 195/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0536\n",
            "Epoch 196/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0699\n",
            "Epoch 197/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0520\n",
            "Epoch 198/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0737\n",
            "Epoch 199/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0547\n",
            "Epoch 200/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0636\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "96 - DatepartRegression with avg smape 8.27: \n",
            "Model Number: 97 of 119 with model GLM for Validation 1\n",
            "97 - GLM with avg smape 2.03: \n",
            "Model Number: 98 of 119 with model DatepartRegression for Validation 1\n",
            "98 - DatepartRegression with avg smape 2.54: \n",
            "Model Number: 99 of 119 with model GLM for Validation 1\n",
            "99 - GLM with avg smape 2.0: \n",
            "Model Number: 100 of 119 with model GLS for Validation 1\n",
            "100 - GLS with avg smape 3.48: \n",
            "Model Number: 101 of 119 with model MetricMotif for Validation 1\n",
            "101 - MetricMotif with avg smape 2.58: \n",
            "Model Number: 102 of 119 with model DatepartRegression for Validation 1\n",
            "Epoch 1/50\n",
            "6/6 [==============================] - 5s 15ms/step - loss: nan\n",
            "Epoch 2/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.5674\n",
            "Epoch 3/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.7692\n",
            "Epoch 4/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.6475\n",
            "Epoch 5/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.4953\n",
            "Epoch 6/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.4697\n",
            "Epoch 7/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.4014\n",
            "Epoch 8/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.3385\n",
            "Epoch 9/50\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.5719\n",
            "Epoch 10/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.4998\n",
            "Epoch 11/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1480\n",
            "Epoch 12/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 13/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1377\n",
            "Epoch 14/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 15/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.3323\n",
            "Epoch 16/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.2923\n",
            "Epoch 17/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.3731\n",
            "Epoch 18/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.3290\n",
            "Epoch 19/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.2598\n",
            "Epoch 20/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1781\n",
            "Epoch 21/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0107\n",
            "Epoch 22/50\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.7216\n",
            "Epoch 23/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.2618\n",
            "Epoch 24/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.3480\n",
            "Epoch 25/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1924\n",
            "Epoch 26/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0505\n",
            "Epoch 27/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 28/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: nan\n",
            "Epoch 29/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 30/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 31/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: nan\n",
            "Epoch 32/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 33/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: nan\n",
            "Epoch 34/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: nan\n",
            "Epoch 35/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 36/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 37/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 38/50\n",
            "6/6 [==============================] - 0s 13ms/step - loss: nan\n",
            "Epoch 39/50\n",
            "6/6 [==============================] - 0s 13ms/step - loss: nan\n",
            "Epoch 40/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: nan\n",
            "Epoch 41/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: nan\n",
            "Epoch 42/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: nan\n",
            "Epoch 43/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 44/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 45/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 46/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: nan\n",
            "Epoch 47/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 48/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 49/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 50/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "102 - DatepartRegression with avg smape 4.84: \n",
            "Model Number: 103 of 119 with model ConstantNaive for Validation 1\n",
            "103 - ConstantNaive with avg smape 2.45: \n",
            "Model Number: 104 of 119 with model ConstantNaive for Validation 1\n",
            "104 - ConstantNaive with avg smape 2.1: \n",
            "Model Number: 105 of 119 with model MetricMotif for Validation 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/0qcwf0nj.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/ozn1u5yv.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=63540', 'data', 'file=/tmp/tmp11_dwuur/0qcwf0nj.json', 'init=/tmp/tmp11_dwuur/ozn1u5yv.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_modelcpef9jvw/prophet_model-20221223064155.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "06:41:55 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "105 - MetricMotif with avg smape 2.09: \n",
            "Model Number: 106 of 119 with model FBProphet for Validation 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "06:41:55 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/068o5pj3.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/09jfw0b2.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=37005', 'data', 'file=/tmp/tmp11_dwuur/068o5pj3.json', 'init=/tmp/tmp11_dwuur/09jfw0b2.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_modelvhspse21/prophet_model-20221223064156.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "06:41:56 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "06:41:56 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "106 - FBProphet with avg smape 2.74: \n",
            "Model Number: 107 of 119 with model FBProphet for Validation 1\n",
            "107 - FBProphet with avg smape 2.71: \n",
            "Model Number: 108 of 119 with model WindowRegression for Validation 1\n",
            "108 - WindowRegression with avg smape 1.91: \n",
            "Model Number: 109 of 119 with model GLM for Validation 1\n",
            "109 - GLM with avg smape 7.11: \n",
            "Model Number: 110 of 119 with model MetricMotif for Validation 1\n",
            "110 - MetricMotif with avg smape 2.71: \n",
            "Model Number: 111 of 119 with model GLM for Validation 1\n",
            "111 - GLM with avg smape 7.04: \n",
            "Model Number: 112 of 119 with model MetricMotif for Validation 1\n",
            "112 - MetricMotif with avg smape 1.79: \n",
            "Model Number: 113 of 119 with model ConstantNaive for Validation 1\n",
            "113 - ConstantNaive with avg smape 2.29: \n",
            "Model Number: 114 of 119 with model GLM for Validation 1\n",
            "114 - GLM with avg smape 4.11: \n",
            "Model Number: 115 of 119 with model ConstantNaive for Validation 1\n",
            "115 - ConstantNaive with avg smape 7.16: \n",
            "Model Number: 116 of 119 with model ConstantNaive for Validation 1\n",
            "116 - ConstantNaive with avg smape 8.26: \n",
            "Model Number: 117 of 119 with model FBProphet for Validation 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/g6vr1nk5.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/tugcxodi.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=11608', 'data', 'file=/tmp/tmp11_dwuur/g6vr1nk5.json', 'init=/tmp/tmp11_dwuur/tugcxodi.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_modelh08tzi9a/prophet_model-20221223064157.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "06:41:57 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "06:41:58 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "117 - FBProphet with avg smape 16.6: \n",
            "Model Number: 118 of 119 with model ConstantNaive for Validation 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/oad8a9tc.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/h0kf3n_e.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=53179', 'data', 'file=/tmp/tmp11_dwuur/oad8a9tc.json', 'init=/tmp/tmp11_dwuur/h0kf3n_e.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_models_u73z9k/prophet_model-20221223064158.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "06:41:58 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "06:41:58 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "118 - ConstantNaive with avg smape 2.81: \n",
            "Model Number: 119 of 119 with model FBProphet for Validation 1\n",
            "119 - FBProphet with avg smape 2.0: \n",
            "Validation Round: 2\n",
            "Model Number: 1 of 119 with model Ensemble for Validation 2\n",
            "📈 1 - Ensemble with avg smape 7.29: \n",
            "Model Number: 2 of 119 with model Ensemble for Validation 2\n",
            "📈 2 - Ensemble with avg smape 7.19: \n",
            "Model Number: 3 of 119 with model Ensemble for Validation 2\n",
            "3 - Ensemble with avg smape 7.8: \n",
            "Model Number: 4 of 119 with model NVAR for Validation 2\n",
            "4 - NVAR with avg smape 7.28: \n",
            "Model Number: 5 of 119 with model NVAR for Validation 2\n",
            "5 - NVAR with avg smape 7.28: \n",
            "Model Number: 6 of 119 with model NVAR for Validation 2\n",
            "6 - NVAR with avg smape 7.3: \n",
            "Model Number: 7 of 119 with model NVAR for Validation 2\n",
            "📈 7 - NVAR with avg smape 6.48: \n",
            "Model Number: 8 of 119 with model Ensemble for Validation 2\n",
            "8 - Ensemble with avg smape 7.19: \n",
            "Model Number: 9 of 119 with model Ensemble for Validation 2\n",
            "9 - Ensemble with avg smape 7.18: \n",
            "Model Number: 10 of 119 with model NVAR for Validation 2\n",
            "📈 10 - NVAR with avg smape 6.47: \n",
            "Model Number: 11 of 119 with model Ensemble for Validation 2\n",
            "11 - Ensemble with avg smape 6.94: \n",
            "Model Number: 12 of 119 with model NVAR for Validation 2\n",
            "12 - NVAR with avg smape 8.65: \n",
            "Model Number: 13 of 119 with model UnivariateMotif for Validation 2\n",
            "13 - UnivariateMotif with avg smape 7.38: \n",
            "Model Number: 14 of 119 with model UnivariateMotif for Validation 2\n",
            "14 - UnivariateMotif with avg smape 7.49: \n",
            "Model Number: 15 of 119 with model UnivariateMotif for Validation 2\n",
            "15 - UnivariateMotif with avg smape 7.36: \n",
            "Model Number: 16 of 119 with model UnivariateMotif for Validation 2\n",
            "16 - UnivariateMotif with avg smape 9.75: \n",
            "Model Number: 17 of 119 with model UnivariateMotif for Validation 2\n",
            "17 - UnivariateMotif with avg smape 6.83: \n",
            "Model Number: 18 of 119 with model UnivariateMotif for Validation 2\n",
            "18 - UnivariateMotif with avg smape 8.77: \n",
            "Model Number: 19 of 119 with model MultivariateMotif for Validation 2\n",
            "📈 19 - MultivariateMotif with avg smape 6.17: \n",
            "Model Number: 20 of 119 with model ARIMA for Validation 2\n",
            "20 - ARIMA with avg smape 6.9: \n",
            "Model Number: 21 of 119 with model MultivariateRegression for Validation 2\n",
            "21 - MultivariateRegression with avg smape 6.95: \n",
            "Model Number: 22 of 119 with model ARIMA for Validation 2\n",
            "22 - ARIMA with avg smape 8.87: \n",
            "Model Number: 23 of 119 with model ARIMA for Validation 2\n",
            "23 - ARIMA with avg smape 8.79: \n",
            "Model Number: 24 of 119 with model LastValueNaive for Validation 2\n",
            "24 - LastValueNaive with avg smape 7.74: \n",
            "Model Number: 25 of 119 with model LastValueNaive for Validation 2\n",
            "25 - LastValueNaive with avg smape 9.62: \n",
            "Model Number: 26 of 119 with model LastValueNaive for Validation 2\n",
            "26 - LastValueNaive with avg smape 7.55: \n",
            "Model Number: 27 of 119 with model LastValueNaive for Validation 2\n",
            "27 - LastValueNaive with avg smape 9.87: \n",
            "Model Number: 28 of 119 with model MultivariateMotif for Validation 2\n",
            "📈 28 - MultivariateMotif with avg smape 2.57: \n",
            "Model Number: 29 of 119 with model MultivariateMotif for Validation 2\n",
            "29 - MultivariateMotif with avg smape 6.1: \n",
            "Model Number: 30 of 119 with model AverageValueNaive for Validation 2\n",
            "30 - AverageValueNaive with avg smape 11.21: \n",
            "Model Number: 31 of 119 with model SectionalMotif for Validation 2\n",
            "31 - SectionalMotif with avg smape 7.22: \n",
            "Model Number: 32 of 119 with model LastValueNaive for Validation 2\n",
            "32 - LastValueNaive with avg smape 9.55: \n",
            "Model Number: 33 of 119 with model LastValueNaive for Validation 2\n",
            "33 - LastValueNaive with avg smape 9.26: \n",
            "Model Number: 34 of 119 with model AverageValueNaive for Validation 2\n",
            "34 - AverageValueNaive with avg smape 10.23: \n",
            "Model Number: 35 of 119 with model MultivariateMotif for Validation 2\n",
            "35 - MultivariateMotif with avg smape 5.58: \n",
            "Model Number: 36 of 119 with model MultivariateMotif for Validation 2\n",
            "36 - MultivariateMotif with avg smape 6.1: \n",
            "Model Number: 37 of 119 with model MultivariateMotif for Validation 2\n",
            "37 - MultivariateMotif with avg smape 6.31: \n",
            "Model Number: 38 of 119 with model ETS for Validation 2\n",
            "ETS failed on Close with ValueError('could not broadcast input array from shape (165,) into shape (420,)')\n",
            "38 - ETS with avg smape 5.38: \n",
            "Model Number: 39 of 119 with model SeasonalNaive for Validation 2\n",
            "39 - SeasonalNaive with avg smape 10.86: \n",
            "Model Number: 40 of 119 with model ARIMA for Validation 2\n",
            "40 - ARIMA with avg smape 9.06: \n",
            "Model Number: 41 of 119 with model ARIMA for Validation 2\n",
            "41 - ARIMA with avg smape 8.38: \n",
            "Model Number: 42 of 119 with model ARIMA for Validation 2\n",
            "42 - ARIMA with avg smape 6.02: \n",
            "Model Number: 43 of 119 with model ETS for Validation 2\n",
            "ETS failed on Close with ValueError('could not broadcast input array from shape (170,) into shape (364,)')\n",
            "43 - ETS with avg smape 5.81: \n",
            "Model Number: 44 of 119 with model Theta for Validation 2\n",
            "44 - Theta with avg smape 6.33: \n",
            "Model Number: 45 of 119 with model SectionalMotif for Validation 2\n",
            "45 - SectionalMotif with avg smape 5.77: \n",
            "Model Number: 46 of 119 with model MultivariateRegression for Validation 2\n",
            "46 - MultivariateRegression with avg smape 4.57: \n",
            "Model Number: 47 of 119 with model SeasonalNaive for Validation 2\n",
            "47 - SeasonalNaive with avg smape 11.68: \n",
            "Model Number: 48 of 119 with model AverageValueNaive for Validation 2\n",
            "48 - AverageValueNaive with avg smape 12.01: \n",
            "Model Number: 49 of 119 with model Theta for Validation 2\n",
            "49 - Theta with avg smape 5.59: \n",
            "Model Number: 50 of 119 with model Theta for Validation 2\n",
            "50 - Theta with avg smape 6.89: \n",
            "Model Number: 51 of 119 with model SectionalMotif for Validation 2\n",
            "51 - SectionalMotif with avg smape 5.85: \n",
            "Model Number: 52 of 119 with model AverageValueNaive for Validation 2\n",
            "52 - AverageValueNaive with avg smape 11.76: \n",
            "Model Number: 53 of 119 with model SectionalMotif for Validation 2\n",
            "53 - SectionalMotif with avg smape 4.71: \n",
            "Model Number: 54 of 119 with model AverageValueNaive for Validation 2\n",
            "54 - AverageValueNaive with avg smape 5.66: \n",
            "Model Number: 55 of 119 with model Theta for Validation 2\n",
            "55 - Theta with avg smape 4.39: \n",
            "Model Number: 56 of 119 with model ETS for Validation 2\n",
            "56 - ETS with avg smape 7.44: \n",
            "Model Number: 57 of 119 with model AverageValueNaive for Validation 2\n",
            "57 - AverageValueNaive with avg smape 5.24: \n",
            "Model Number: 58 of 119 with model GLS for Validation 2\n",
            "58 - GLS with avg smape 5.58: \n",
            "Model Number: 59 of 119 with model Theta for Validation 2\n",
            "59 - Theta with avg smape 5.33: \n",
            "Model Number: 60 of 119 with model Theta for Validation 2\n",
            "60 - Theta with avg smape 5.4: \n",
            "Model Number: 61 of 119 with model SeasonalNaive for Validation 2\n",
            "61 - SeasonalNaive with avg smape 10.98: \n",
            "Model Number: 62 of 119 with model UnobservedComponents for Validation 2\n",
            "62 - UnobservedComponents with avg smape 10.72: \n",
            "Model Number: 63 of 119 with model SectionalMotif for Validation 2\n",
            "63 - SectionalMotif with avg smape 5.33: \n",
            "Model Number: 64 of 119 with model WindowRegression for Validation 2\n",
            "64 - WindowRegression with avg smape 6.98: \n",
            "Model Number: 65 of 119 with model DatepartRegression for Validation 2\n",
            "65 - DatepartRegression with avg smape 4.13: \n",
            "Model Number: 66 of 119 with model ETS for Validation 2\n",
            "66 - ETS with avg smape 6.56: \n",
            "Model Number: 67 of 119 with model UnobservedComponents for Validation 2\n",
            "67 - UnobservedComponents with avg smape 5.58: \n",
            "Model Number: 68 of 119 with model GLS for Validation 2\n",
            "68 - GLS with avg smape 13.76: \n",
            "Model Number: 69 of 119 with model DatepartRegression for Validation 2\n",
            "Epoch 1/200\n",
            "6/6 [==============================] - 5s 39ms/step - loss: 0.2102\n",
            "Epoch 2/200\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 0.0906\n",
            "Epoch 3/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0885\n",
            "Epoch 4/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0900\n",
            "Epoch 5/200\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 0.0964\n",
            "Epoch 6/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.0881\n",
            "Epoch 7/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0923\n",
            "Epoch 8/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0849\n",
            "Epoch 9/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0859\n",
            "Epoch 10/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0855\n",
            "Epoch 11/200\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 0.0970\n",
            "Epoch 12/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0919\n",
            "Epoch 13/200\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 0.0961\n",
            "Epoch 14/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0894\n",
            "Epoch 15/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0888\n",
            "Epoch 16/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0859\n",
            "Epoch 17/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0874\n",
            "Epoch 18/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0848\n",
            "Epoch 19/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0926\n",
            "Epoch 20/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0914\n",
            "Epoch 21/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.0849\n",
            "Epoch 22/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0868\n",
            "Epoch 23/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0922\n",
            "Epoch 24/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0869\n",
            "Epoch 25/200\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 0.0883\n",
            "Epoch 26/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0894\n",
            "Epoch 27/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0864\n",
            "Epoch 28/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0894\n",
            "Epoch 29/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0848\n",
            "Epoch 30/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0907\n",
            "Epoch 31/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0885\n",
            "Epoch 32/200\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 0.0859\n",
            "Epoch 33/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0849\n",
            "Epoch 34/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0892\n",
            "Epoch 35/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0873\n",
            "Epoch 36/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0864\n",
            "Epoch 37/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0845\n",
            "Epoch 38/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0854\n",
            "Epoch 39/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0867\n",
            "Epoch 40/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0866\n",
            "Epoch 41/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0884\n",
            "Epoch 42/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0964\n",
            "Epoch 43/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0866\n",
            "Epoch 44/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0889\n",
            "Epoch 45/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0896\n",
            "Epoch 46/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0855\n",
            "Epoch 47/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0848\n",
            "Epoch 48/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0873\n",
            "Epoch 49/200\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 0.0924\n",
            "Epoch 50/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0848\n",
            "Epoch 51/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0915\n",
            "Epoch 52/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0880\n",
            "Epoch 53/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0870\n",
            "Epoch 54/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0850\n",
            "Epoch 55/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0867\n",
            "Epoch 56/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0874\n",
            "Epoch 57/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0857\n",
            "Epoch 58/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0891\n",
            "Epoch 59/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0850\n",
            "Epoch 60/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0868\n",
            "Epoch 61/200\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 0.0898\n",
            "Epoch 62/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.0860\n",
            "Epoch 63/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0833\n",
            "Epoch 64/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0840\n",
            "Epoch 65/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0835\n",
            "Epoch 66/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0847\n",
            "Epoch 67/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0876\n",
            "Epoch 68/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0859\n",
            "Epoch 69/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0828\n",
            "Epoch 70/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0797\n",
            "Epoch 71/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0786\n",
            "Epoch 72/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0795\n",
            "Epoch 73/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0724\n",
            "Epoch 74/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0758\n",
            "Epoch 75/200\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 0.0590\n",
            "Epoch 76/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0701\n",
            "Epoch 77/200\n",
            "6/6 [==============================] - 0s 41ms/step - loss: 0.0580\n",
            "Epoch 78/200\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 0.0666\n",
            "Epoch 79/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0602\n",
            "Epoch 80/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0543\n",
            "Epoch 81/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.0606\n",
            "Epoch 82/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0483\n",
            "Epoch 83/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0556\n",
            "Epoch 84/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0542\n",
            "Epoch 85/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.0413\n",
            "Epoch 86/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0579\n",
            "Epoch 87/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0475\n",
            "Epoch 88/200\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 0.0540\n",
            "Epoch 89/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0586\n",
            "Epoch 90/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0396\n",
            "Epoch 91/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0478\n",
            "Epoch 92/200\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 0.0507\n",
            "Epoch 93/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0478\n",
            "Epoch 94/200\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 0.0570\n",
            "Epoch 95/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0412\n",
            "Epoch 96/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0449\n",
            "Epoch 97/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0419\n",
            "Epoch 98/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0417\n",
            "Epoch 99/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0364\n",
            "Epoch 100/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0696\n",
            "Epoch 101/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0345\n",
            "Epoch 102/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0429\n",
            "Epoch 103/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0358\n",
            "Epoch 104/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0490\n",
            "Epoch 105/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0375\n",
            "Epoch 106/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0388\n",
            "Epoch 107/200\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 0.0461\n",
            "Epoch 108/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0297\n",
            "Epoch 109/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0504\n",
            "Epoch 110/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0296\n",
            "Epoch 111/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0271\n",
            "Epoch 112/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0634\n",
            "Epoch 113/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0326\n",
            "Epoch 114/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0361\n",
            "Epoch 115/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0378\n",
            "Epoch 116/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0271\n",
            "Epoch 117/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0318\n",
            "Epoch 118/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0333\n",
            "Epoch 119/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0398\n",
            "Epoch 120/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0374\n",
            "Epoch 121/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0263\n",
            "Epoch 122/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0493\n",
            "Epoch 123/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0321\n",
            "Epoch 124/200\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 0.0229\n",
            "Epoch 125/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0344\n",
            "Epoch 126/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0484\n",
            "Epoch 127/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0341\n",
            "Epoch 128/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0262\n",
            "Epoch 129/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0364\n",
            "Epoch 130/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0296\n",
            "Epoch 131/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0345\n",
            "Epoch 132/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0336\n",
            "Epoch 133/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0301\n",
            "Epoch 134/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0294\n",
            "Epoch 135/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0283\n",
            "Epoch 136/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0265\n",
            "Epoch 137/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0235\n",
            "Epoch 138/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0262\n",
            "Epoch 139/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0366\n",
            "Epoch 140/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0275\n",
            "Epoch 141/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0271\n",
            "Epoch 142/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0243\n",
            "Epoch 143/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0280\n",
            "Epoch 144/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0348\n",
            "Epoch 145/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0311\n",
            "Epoch 146/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0298\n",
            "Epoch 147/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0245\n",
            "Epoch 148/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0244\n",
            "Epoch 149/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0292\n",
            "Epoch 150/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0279\n",
            "Epoch 151/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0325\n",
            "Epoch 152/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0219\n",
            "Epoch 153/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0222\n",
            "Epoch 154/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0272\n",
            "Epoch 155/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0233\n",
            "Epoch 156/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0277\n",
            "Epoch 157/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0259\n",
            "Epoch 158/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0217\n",
            "Epoch 159/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0433\n",
            "Epoch 160/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0183\n",
            "Epoch 161/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0249\n",
            "Epoch 162/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0270\n",
            "Epoch 163/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0232\n",
            "Epoch 164/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0221\n",
            "Epoch 165/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0238\n",
            "Epoch 166/200\n",
            "6/6 [==============================] - 0s 42ms/step - loss: 0.0222\n",
            "Epoch 167/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0195\n",
            "Epoch 168/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0236\n",
            "Epoch 169/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0232\n",
            "Epoch 170/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.0174\n",
            "Epoch 171/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0330\n",
            "Epoch 172/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0185\n",
            "Epoch 173/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0184\n",
            "Epoch 174/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0162\n",
            "Epoch 175/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0165\n",
            "Epoch 176/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0249\n",
            "Epoch 177/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0307\n",
            "Epoch 178/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0156\n",
            "Epoch 179/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0196\n",
            "Epoch 180/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0335\n",
            "Epoch 181/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0190\n",
            "Epoch 182/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0163\n",
            "Epoch 183/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0179\n",
            "Epoch 184/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0213\n",
            "Epoch 185/200\n",
            "6/6 [==============================] - 0s 41ms/step - loss: 0.0168\n",
            "Epoch 186/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0180\n",
            "Epoch 187/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.0214\n",
            "Epoch 188/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0217\n",
            "Epoch 189/200\n",
            "6/6 [==============================] - 0s 41ms/step - loss: 0.0193\n",
            "Epoch 190/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0192\n",
            "Epoch 191/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0191\n",
            "Epoch 192/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0215\n",
            "Epoch 193/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0253\n",
            "Epoch 194/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0164\n",
            "Epoch 195/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0189\n",
            "Epoch 196/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0237\n",
            "Epoch 197/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0205\n",
            "Epoch 198/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0156\n",
            "Epoch 199/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0229\n",
            "Epoch 200/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0233\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "69 - DatepartRegression with avg smape 5.9: \n",
            "Model Number: 70 of 119 with model SeasonalNaive for Validation 2\n",
            "70 - SeasonalNaive with avg smape 11.89: \n",
            "Model Number: 71 of 119 with model SectionalMotif for Validation 2\n",
            "71 - SectionalMotif with avg smape 4.64: \n",
            "Model Number: 72 of 119 with model MultivariateRegression for Validation 2\n",
            "72 - MultivariateRegression with avg smape 6.53: \n",
            "Model Number: 73 of 119 with model DatepartRegression for Validation 2\n",
            "73 - DatepartRegression with avg smape 5.4: \n",
            "Model Number: 74 of 119 with model MultivariateRegression for Validation 2\n",
            "74 - MultivariateRegression with avg smape 5.58: \n",
            "Model Number: 75 of 119 with model UnobservedComponents for Validation 2\n",
            "75 - UnobservedComponents with avg smape 6.02: \n",
            "Model Number: 76 of 119 with model SeasonalNaive for Validation 2\n",
            "76 - SeasonalNaive with avg smape 6.68: \n",
            "Model Number: 77 of 119 with model MultivariateRegression for Validation 2\n",
            "77 - MultivariateRegression with avg smape 7.46: \n",
            "Model Number: 78 of 119 with model UnobservedComponents for Validation 2\n",
            "78 - UnobservedComponents with avg smape 5.73: \n",
            "Model Number: 79 of 119 with model UnobservedComponents for Validation 2\n",
            "79 - UnobservedComponents with avg smape 5.73: \n",
            "Model Number: 80 of 119 with model UnobservedComponents for Validation 2\n",
            "80 - UnobservedComponents with avg smape 5.58: \n",
            "Model Number: 81 of 119 with model WindowRegression for Validation 2\n",
            "81 - WindowRegression with avg smape 5.58: \n",
            "Model Number: 82 of 119 with model WindowRegression for Validation 2\n",
            "82 - WindowRegression with avg smape 7.02: \n",
            "Model Number: 83 of 119 with model MultivariateRegression for Validation 2\n",
            "83 - MultivariateRegression with avg smape 5.59: \n",
            "Model Number: 84 of 119 with model WindowRegression for Validation 2\n",
            "84 - WindowRegression with avg smape 8.5: \n",
            "Model Number: 85 of 119 with model MetricMotif for Validation 2\n",
            "85 - MetricMotif with avg smape 4.9: \n",
            "Model Number: 86 of 119 with model GLS for Validation 2\n",
            "86 - GLS with avg smape 6.7: \n",
            "Model Number: 87 of 119 with model ETS for Validation 2\n",
            "87 - ETS with avg smape 5.97: \n",
            "Model Number: 88 of 119 with model GLS for Validation 2\n",
            "88 - GLS with avg smape 8.01: \n",
            "Model Number: 89 of 119 with model UnivariateRegression for Validation 2\n",
            "89 - UnivariateRegression with avg smape 2.84: \n",
            "Model Number: 90 of 119 with model ETS for Validation 2\n",
            "📈 90 - ETS with avg smape 2.12: \n",
            "Model Number: 91 of 119 with model SeasonalNaive for Validation 2\n",
            "91 - SeasonalNaive with avg smape 8.76: \n",
            "Model Number: 92 of 119 with model GLS for Validation 2\n",
            "92 - GLS with avg smape 8.52: \n",
            "Model Number: 93 of 119 with model MetricMotif for Validation 2\n",
            "93 - MetricMotif with avg smape 6.35: \n",
            "Model Number: 94 of 119 with model WindowRegression for Validation 2\n",
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 94: WindowRegression\n",
            "Model Number: 95 of 119 with model GLM for Validation 2\n",
            "95 - GLM with avg smape 7.32: \n",
            "Model Number: 96 of 119 with model DatepartRegression for Validation 2\n",
            "Epoch 1/200\n",
            "6/6 [==============================] - 8s 64ms/step - loss: 0.3619\n",
            "Epoch 2/200\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.3530\n",
            "Epoch 3/200\n",
            "6/6 [==============================] - 0s 67ms/step - loss: 0.3402\n",
            "Epoch 4/200\n",
            "6/6 [==============================] - 0s 68ms/step - loss: 0.3422\n",
            "Epoch 5/200\n",
            "6/6 [==============================] - 0s 63ms/step - loss: 0.3421\n",
            "Epoch 6/200\n",
            "6/6 [==============================] - 1s 111ms/step - loss: 0.3400\n",
            "Epoch 7/200\n",
            "6/6 [==============================] - 1s 75ms/step - loss: 0.3400\n",
            "Epoch 8/200\n",
            "6/6 [==============================] - 0s 76ms/step - loss: 0.3400\n",
            "Epoch 9/200\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.3404\n",
            "Epoch 10/200\n",
            "6/6 [==============================] - 0s 71ms/step - loss: 0.3391\n",
            "Epoch 11/200\n",
            "6/6 [==============================] - 0s 60ms/step - loss: 0.3412\n",
            "Epoch 12/200\n",
            "6/6 [==============================] - 0s 66ms/step - loss: 0.3408\n",
            "Epoch 13/200\n",
            "6/6 [==============================] - 0s 67ms/step - loss: 0.3402\n",
            "Epoch 14/200\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.3415\n",
            "Epoch 15/200\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.3396\n",
            "Epoch 16/200\n",
            "6/6 [==============================] - 0s 67ms/step - loss: 0.3390\n",
            "Epoch 17/200\n",
            "6/6 [==============================] - 0s 72ms/step - loss: 0.3394\n",
            "Epoch 18/200\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 0.3396\n",
            "Epoch 19/200\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.3405\n",
            "Epoch 20/200\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.3400\n",
            "Epoch 21/200\n",
            "6/6 [==============================] - 0s 71ms/step - loss: 0.3387\n",
            "Epoch 22/200\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.3389\n",
            "Epoch 23/200\n",
            "6/6 [==============================] - 0s 73ms/step - loss: 0.3393\n",
            "Epoch 24/200\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.3415\n",
            "Epoch 25/200\n",
            "6/6 [==============================] - 0s 64ms/step - loss: 0.3392\n",
            "Epoch 26/200\n",
            "6/6 [==============================] - 0s 70ms/step - loss: 0.3395\n",
            "Epoch 27/200\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.3388\n",
            "Epoch 28/200\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.3407\n",
            "Epoch 29/200\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.3392\n",
            "Epoch 30/200\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 0.3399\n",
            "Epoch 31/200\n",
            "6/6 [==============================] - 0s 77ms/step - loss: 0.3398\n",
            "Epoch 32/200\n",
            "6/6 [==============================] - 0s 74ms/step - loss: 0.3388\n",
            "Epoch 33/200\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 0.3388\n",
            "Epoch 34/200\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.3393\n",
            "Epoch 35/200\n",
            "6/6 [==============================] - 0s 74ms/step - loss: 0.3387\n",
            "Epoch 36/200\n",
            "6/6 [==============================] - 0s 70ms/step - loss: 0.3396\n",
            "Epoch 37/200\n",
            "6/6 [==============================] - 0s 71ms/step - loss: 0.3385\n",
            "Epoch 38/200\n",
            "6/6 [==============================] - 0s 61ms/step - loss: 0.3389\n",
            "Epoch 39/200\n",
            "6/6 [==============================] - 0s 77ms/step - loss: 0.3390\n",
            "Epoch 40/200\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.3406\n",
            "Epoch 41/200\n",
            "6/6 [==============================] - 1s 114ms/step - loss: 0.3397\n",
            "Epoch 42/200\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.3411\n",
            "Epoch 43/200\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.3394\n",
            "Epoch 44/200\n",
            "6/6 [==============================] - 0s 68ms/step - loss: 0.3390\n",
            "Epoch 45/200\n",
            "6/6 [==============================] - 1s 82ms/step - loss: 0.3391\n",
            "Epoch 46/200\n",
            "6/6 [==============================] - 1s 82ms/step - loss: 0.3382\n",
            "Epoch 47/200\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.3390\n",
            "Epoch 48/200\n",
            "6/6 [==============================] - 0s 66ms/step - loss: 0.3383\n",
            "Epoch 49/200\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 0.3396\n",
            "Epoch 50/200\n",
            "6/6 [==============================] - 0s 65ms/step - loss: 0.3378\n",
            "Epoch 51/200\n",
            "6/6 [==============================] - 0s 75ms/step - loss: 0.3397\n",
            "Epoch 52/200\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 0.3372\n",
            "Epoch 53/200\n",
            "6/6 [==============================] - 0s 69ms/step - loss: 0.3386\n",
            "Epoch 54/200\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.3357\n",
            "Epoch 55/200\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.3353\n",
            "Epoch 56/200\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.3348\n",
            "Epoch 57/200\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.3269\n",
            "Epoch 58/200\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.3296\n",
            "Epoch 59/200\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.3170\n",
            "Epoch 60/200\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.3001\n",
            "Epoch 61/200\n",
            "6/6 [==============================] - 0s 69ms/step - loss: 0.3160\n",
            "Epoch 62/200\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.2847\n",
            "Epoch 63/200\n",
            "6/6 [==============================] - 0s 64ms/step - loss: 0.2725\n",
            "Epoch 64/200\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.2404\n",
            "Epoch 65/200\n",
            "6/6 [==============================] - 0s 71ms/step - loss: 0.2805\n",
            "Epoch 66/200\n",
            "6/6 [==============================] - 0s 72ms/step - loss: 0.2123\n",
            "Epoch 67/200\n",
            "6/6 [==============================] - 0s 67ms/step - loss: 0.2326\n",
            "Epoch 68/200\n",
            "6/6 [==============================] - 0s 74ms/step - loss: 0.2175\n",
            "Epoch 69/200\n",
            "6/6 [==============================] - 0s 76ms/step - loss: 0.2013\n",
            "Epoch 70/200\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.1724\n",
            "Epoch 71/200\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.2460\n",
            "Epoch 72/200\n",
            "6/6 [==============================] - 0s 50ms/step - loss: 0.1960\n",
            "Epoch 73/200\n",
            "6/6 [==============================] - 0s 45ms/step - loss: 0.1521\n",
            "Epoch 74/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.2029\n",
            "Epoch 75/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1611\n",
            "Epoch 76/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1905\n",
            "Epoch 77/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1577\n",
            "Epoch 78/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1798\n",
            "Epoch 79/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.1442\n",
            "Epoch 80/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1379\n",
            "Epoch 81/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1576\n",
            "Epoch 82/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1288\n",
            "Epoch 83/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.1353\n",
            "Epoch 84/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.1306\n",
            "Epoch 85/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1771\n",
            "Epoch 86/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.1204\n",
            "Epoch 87/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1418\n",
            "Epoch 88/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1466\n",
            "Epoch 89/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1334\n",
            "Epoch 90/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.1304\n",
            "Epoch 91/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1348\n",
            "Epoch 92/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.1151\n",
            "Epoch 93/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1488\n",
            "Epoch 94/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.1296\n",
            "Epoch 95/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1186\n",
            "Epoch 96/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1094\n",
            "Epoch 97/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.1293\n",
            "Epoch 98/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.1303\n",
            "Epoch 99/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.1037\n",
            "Epoch 100/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1137\n",
            "Epoch 101/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1113\n",
            "Epoch 102/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1287\n",
            "Epoch 103/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1123\n",
            "Epoch 104/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1338\n",
            "Epoch 105/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1161\n",
            "Epoch 106/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.1084\n",
            "Epoch 107/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1205\n",
            "Epoch 108/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.1119\n",
            "Epoch 109/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0915\n",
            "Epoch 110/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1442\n",
            "Epoch 111/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0904\n",
            "Epoch 112/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.1383\n",
            "Epoch 113/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1043\n",
            "Epoch 114/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.1234\n",
            "Epoch 115/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.1157\n",
            "Epoch 116/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0885\n",
            "Epoch 117/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1519\n",
            "Epoch 118/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0819\n",
            "Epoch 119/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1083\n",
            "Epoch 120/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1310\n",
            "Epoch 121/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1029\n",
            "Epoch 122/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.1530\n",
            "Epoch 123/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0963\n",
            "Epoch 124/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0983\n",
            "Epoch 125/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1017\n",
            "Epoch 126/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1169\n",
            "Epoch 127/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1365\n",
            "Epoch 128/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0906\n",
            "Epoch 129/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1026\n",
            "Epoch 130/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0869\n",
            "Epoch 131/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1068\n",
            "Epoch 132/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0930\n",
            "Epoch 133/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1279\n",
            "Epoch 134/200\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.1040\n",
            "Epoch 135/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1021\n",
            "Epoch 136/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0846\n",
            "Epoch 137/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1062\n",
            "Epoch 138/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1050\n",
            "Epoch 139/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1119\n",
            "Epoch 140/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0848\n",
            "Epoch 141/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0871\n",
            "Epoch 142/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.1124\n",
            "Epoch 143/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0721\n",
            "Epoch 144/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1196\n",
            "Epoch 145/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0773\n",
            "Epoch 146/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0758\n",
            "Epoch 147/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0887\n",
            "Epoch 148/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0974\n",
            "Epoch 149/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.0880\n",
            "Epoch 150/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0773\n",
            "Epoch 151/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0959\n",
            "Epoch 152/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0747\n",
            "Epoch 153/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0859\n",
            "Epoch 154/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0863\n",
            "Epoch 155/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0628\n",
            "Epoch 156/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1087\n",
            "Epoch 157/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0685\n",
            "Epoch 158/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.0865\n",
            "Epoch 159/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.1132\n",
            "Epoch 160/200\n",
            "6/6 [==============================] - 0s 41ms/step - loss: 0.0669\n",
            "Epoch 161/200\n",
            "6/6 [==============================] - 0s 43ms/step - loss: 0.0990\n",
            "Epoch 162/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0596\n",
            "Epoch 163/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1242\n",
            "Epoch 164/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.0722\n",
            "Epoch 165/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.0864\n",
            "Epoch 166/200\n",
            "6/6 [==============================] - 0s 41ms/step - loss: 0.0720\n",
            "Epoch 167/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0669\n",
            "Epoch 168/200\n",
            "6/6 [==============================] - 0s 41ms/step - loss: 0.0968\n",
            "Epoch 169/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0751\n",
            "Epoch 170/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0995\n",
            "Epoch 171/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0619\n",
            "Epoch 172/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0827\n",
            "Epoch 173/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0749\n",
            "Epoch 174/200\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0647\n",
            "Epoch 175/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0838\n",
            "Epoch 176/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0563\n",
            "Epoch 177/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0794\n",
            "Epoch 178/200\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.0638\n",
            "Epoch 179/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0744\n",
            "Epoch 180/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.1064\n",
            "Epoch 181/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0619\n",
            "Epoch 182/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0665\n",
            "Epoch 183/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0772\n",
            "Epoch 184/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0575\n",
            "Epoch 185/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0789\n",
            "Epoch 186/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0716\n",
            "Epoch 187/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0847\n",
            "Epoch 188/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0510\n",
            "Epoch 189/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0595\n",
            "Epoch 190/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0877\n",
            "Epoch 191/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0548\n",
            "Epoch 192/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0672\n",
            "Epoch 193/200\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0654\n",
            "Epoch 194/200\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0630\n",
            "Epoch 195/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0683\n",
            "Epoch 196/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.0740\n",
            "Epoch 197/200\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.0699\n",
            "Epoch 198/200\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0503\n",
            "Epoch 199/200\n",
            "6/6 [==============================] - 0s 42ms/step - loss: 0.0815\n",
            "Epoch 200/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0554\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "96 - DatepartRegression with avg smape 6.63: \n",
            "Model Number: 97 of 119 with model GLM for Validation 2\n",
            "97 - GLM with avg smape 5.43: \n",
            "Model Number: 98 of 119 with model DatepartRegression for Validation 2\n",
            "98 - DatepartRegression with avg smape 7.97: \n",
            "Model Number: 99 of 119 with model GLM for Validation 2\n",
            "99 - GLM with avg smape 6.1: \n",
            "Model Number: 100 of 119 with model GLS for Validation 2\n",
            "100 - GLS with avg smape 9.51: \n",
            "Model Number: 101 of 119 with model MetricMotif for Validation 2\n",
            "101 - MetricMotif with avg smape 7.39: \n",
            "Model Number: 102 of 119 with model DatepartRegression for Validation 2\n",
            "Epoch 1/50\n",
            "6/6 [==============================] - 6s 20ms/step - loss: nan\n",
            "Epoch 2/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: nan\n",
            "Epoch 3/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: nan\n",
            "Epoch 4/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: nan\n",
            "Epoch 5/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 6/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 7/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 8/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 9/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: nan\n",
            "Epoch 10/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 11/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: nan\n",
            "Epoch 12/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: nan\n",
            "Epoch 13/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: nan\n",
            "Epoch 14/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 15/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 16/50\n",
            "6/6 [==============================] - 0s 13ms/step - loss: nan\n",
            "Epoch 17/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 18/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 19/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: nan\n",
            "Epoch 20/50\n",
            "6/6 [==============================] - 0s 13ms/step - loss: nan\n",
            "Epoch 21/50\n",
            "6/6 [==============================] - 0s 13ms/step - loss: nan\n",
            "Epoch 22/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 23/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: nan\n",
            "Epoch 24/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 25/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 26/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: nan\n",
            "Epoch 27/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: nan\n",
            "Epoch 28/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: nan\n",
            "Epoch 29/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 30/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: nan\n",
            "Epoch 31/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: nan\n",
            "Epoch 32/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: nan\n",
            "Epoch 33/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: nan\n",
            "Epoch 34/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 35/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 36/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: nan\n",
            "Epoch 37/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: nan\n",
            "Epoch 38/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 39/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 40/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: nan\n",
            "Epoch 41/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 42/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: nan\n",
            "Epoch 43/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: nan\n",
            "Epoch 44/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: nan\n",
            "Epoch 45/50\n",
            "6/6 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 46/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: nan\n",
            "Epoch 47/50\n",
            "6/6 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 48/50\n",
            "6/6 [==============================] - 0s 13ms/step - loss: nan\n",
            "Epoch 49/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: nan\n",
            "Epoch 50/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: nan\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "102 - DatepartRegression with avg smape 9.71: \n",
            "Model Number: 103 of 119 with model ConstantNaive for Validation 2\n",
            "103 - ConstantNaive with avg smape 5.87: \n",
            "Model Number: 104 of 119 with model ConstantNaive for Validation 2\n",
            "104 - ConstantNaive with avg smape 6.82: \n",
            "Model Number: 105 of 119 with model MetricMotif for Validation 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "105 - MetricMotif with avg smape 7.22: \n",
            "Model Number: 106 of 119 with model FBProphet for Validation 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/m9y8m2mp.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/zs1kox3o.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=33793', 'data', 'file=/tmp/tmp11_dwuur/m9y8m2mp.json', 'init=/tmp/tmp11_dwuur/zs1kox3o.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_modelpj1_6czk/prophet_model-20221223064433.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "06:44:33 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "06:44:33 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/wmkm0j00.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/dr7jjnx3.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=51910', 'data', 'file=/tmp/tmp11_dwuur/wmkm0j00.json', 'init=/tmp/tmp11_dwuur/dr7jjnx3.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_modelmtxbz2nj/prophet_model-20221223064434.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "06:44:34 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "06:44:34 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "106 - FBProphet with avg smape 9.71: \n",
            "Model Number: 107 of 119 with model FBProphet for Validation 2\n",
            "107 - FBProphet with avg smape 5.58: \n",
            "Model Number: 108 of 119 with model WindowRegression for Validation 2\n",
            "📈 108 - WindowRegression with avg smape 2.08: \n",
            "Model Number: 109 of 119 with model GLM for Validation 2\n",
            "109 - GLM with avg smape 10.03: \n",
            "Model Number: 110 of 119 with model MetricMotif for Validation 2\n",
            "110 - MetricMotif with avg smape 5.58: \n",
            "Model Number: 111 of 119 with model GLM for Validation 2\n",
            "111 - GLM with avg smape 9.15: \n",
            "Model Number: 112 of 119 with model MetricMotif for Validation 2\n",
            "112 - MetricMotif with avg smape 6.09: \n",
            "Model Number: 113 of 119 with model ConstantNaive for Validation 2\n",
            "113 - ConstantNaive with avg smape 4.64: \n",
            "Model Number: 114 of 119 with model GLM for Validation 2\n",
            "114 - GLM with avg smape 5.62: \n",
            "Model Number: 115 of 119 with model ConstantNaive for Validation 2\n",
            "115 - ConstantNaive with avg smape 8.09: \n",
            "Model Number: 116 of 119 with model ConstantNaive for Validation 2\n",
            "116 - ConstantNaive with avg smape 9.43: \n",
            "Model Number: 117 of 119 with model FBProphet for Validation 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/whb57jrd.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/pfam0scp.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=68633', 'data', 'file=/tmp/tmp11_dwuur/whb57jrd.json', 'init=/tmp/tmp11_dwuur/pfam0scp.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_modeleguvijel/prophet_model-20221223064436.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "06:44:36 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "06:44:36 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "117 - FBProphet with avg smape 8.65: \n",
            "Model Number: 118 of 119 with model ConstantNaive for Validation 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/jxhfdq40.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/85i7nm13.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=46537', 'data', 'file=/tmp/tmp11_dwuur/jxhfdq40.json', 'init=/tmp/tmp11_dwuur/85i7nm13.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_model1i837c9t/prophet_model-20221223064437.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "06:44:37 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "06:44:37 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "118 - ConstantNaive with avg smape 7.65: \n",
            "Model Number: 119 of 119 with model FBProphet for Validation 2\n",
            "119 - FBProphet with avg smape 6.93: \n",
            "Validation Round: 3\n",
            "Model Number: 1 of 119 with model Ensemble for Validation 3\n",
            "📈 1 - Ensemble with avg smape 66.84: \n",
            "Model Number: 2 of 119 with model Ensemble for Validation 3\n",
            "📈 2 - Ensemble with avg smape 4.18: \n",
            "Model Number: 3 of 119 with model Ensemble for Validation 3\n",
            "📈 3 - Ensemble with avg smape 3.88: \n",
            "Model Number: 4 of 119 with model NVAR for Validation 3\n",
            "4 - NVAR with avg smape 70.15: \n",
            "Model Number: 5 of 119 with model NVAR for Validation 3\n",
            "5 - NVAR with avg smape 70.09: \n",
            "Model Number: 6 of 119 with model NVAR for Validation 3\n",
            "📈 6 - NVAR with avg smape 3.54: \n",
            "Model Number: 7 of 119 with model NVAR for Validation 3\n",
            "7 - NVAR with avg smape 4.43: \n",
            "Model Number: 8 of 119 with model Ensemble for Validation 3\n",
            "8 - Ensemble with avg smape 64.5: \n",
            "Model Number: 9 of 119 with model Ensemble for Validation 3\n",
            "9 - Ensemble with avg smape 64.37: \n",
            "Model Number: 10 of 119 with model NVAR for Validation 3\n",
            "10 - NVAR with avg smape 4.44: \n",
            "Model Number: 11 of 119 with model Ensemble for Validation 3\n",
            "11 - Ensemble with avg smape 64.17: \n",
            "Model Number: 12 of 119 with model NVAR for Validation 3\n",
            "12 - NVAR with avg smape 4.07: \n",
            "Model Number: 13 of 119 with model UnivariateMotif for Validation 3\n",
            "📈 13 - UnivariateMotif with avg smape 2.89: \n",
            "Model Number: 14 of 119 with model UnivariateMotif for Validation 3\n",
            "📈 14 - UnivariateMotif with avg smape 2.46: \n",
            "Model Number: 15 of 119 with model UnivariateMotif for Validation 3\n",
            "15 - UnivariateMotif with avg smape 5.59: \n",
            "Model Number: 16 of 119 with model UnivariateMotif for Validation 3\n",
            "16 - UnivariateMotif with avg smape 3.17: \n",
            "Model Number: 17 of 119 with model UnivariateMotif for Validation 3\n",
            "17 - UnivariateMotif with avg smape 5.39: \n",
            "Model Number: 18 of 119 with model UnivariateMotif for Validation 3\n",
            "18 - UnivariateMotif with avg smape 3.78: \n",
            "Model Number: 19 of 119 with model MultivariateMotif for Validation 3\n",
            "19 - MultivariateMotif with avg smape 3.6: \n",
            "Model Number: 20 of 119 with model ARIMA for Validation 3\n",
            "20 - ARIMA with avg smape 4.24: \n",
            "Model Number: 21 of 119 with model MultivariateRegression for Validation 3\n",
            "21 - MultivariateRegression with avg smape 2.65: \n",
            "Model Number: 22 of 119 with model ARIMA for Validation 3\n",
            "22 - ARIMA with avg smape 3.61: \n",
            "Model Number: 23 of 119 with model ARIMA for Validation 3\n",
            "23 - ARIMA with avg smape 3.62: \n",
            "Model Number: 24 of 119 with model LastValueNaive for Validation 3\n",
            "24 - LastValueNaive with avg smape 2.74: \n",
            "Model Number: 25 of 119 with model LastValueNaive for Validation 3\n",
            "25 - LastValueNaive with avg smape 3.39: \n",
            "Model Number: 26 of 119 with model LastValueNaive for Validation 3\n",
            "26 - LastValueNaive with avg smape 2.46: \n",
            "Model Number: 27 of 119 with model LastValueNaive for Validation 3\n",
            "27 - LastValueNaive with avg smape 3.2: \n",
            "Model Number: 28 of 119 with model MultivariateMotif for Validation 3\n",
            "28 - MultivariateMotif with avg smape 7.31: \n",
            "Model Number: 29 of 119 with model MultivariateMotif for Validation 3\n",
            "29 - MultivariateMotif with avg smape 4.16: \n",
            "Model Number: 30 of 119 with model AverageValueNaive for Validation 3\n",
            "📈 30 - AverageValueNaive with avg smape 1.94: \n",
            "Model Number: 31 of 119 with model SectionalMotif for Validation 3\n",
            "31 - SectionalMotif with avg smape 3.06: \n",
            "Model Number: 32 of 119 with model LastValueNaive for Validation 3\n",
            "32 - LastValueNaive with avg smape 3.42: \n",
            "Model Number: 33 of 119 with model LastValueNaive for Validation 3\n",
            "33 - LastValueNaive with avg smape 3.04: \n",
            "Model Number: 34 of 119 with model AverageValueNaive for Validation 3\n",
            "34 - AverageValueNaive with avg smape 2.63: \n",
            "Model Number: 35 of 119 with model MultivariateMotif for Validation 3\n",
            "35 - MultivariateMotif with avg smape 3.25: \n",
            "Model Number: 36 of 119 with model MultivariateMotif for Validation 3\n",
            "36 - MultivariateMotif with avg smape 4.09: \n",
            "Model Number: 37 of 119 with model MultivariateMotif for Validation 3\n",
            "37 - MultivariateMotif with avg smape 3.85: \n",
            "Model Number: 38 of 119 with model ETS for Validation 3\n",
            "ETS failed on Close with ValueError('could not broadcast input array from shape (155,) into shape (420,)')\n",
            "38 - ETS with avg smape 3.65: \n",
            "Model Number: 39 of 119 with model SeasonalNaive for Validation 3\n",
            "39 - SeasonalNaive with avg smape 2.27: \n",
            "Model Number: 40 of 119 with model ARIMA for Validation 3\n",
            "40 - ARIMA with avg smape 3.03: \n",
            "Model Number: 41 of 119 with model ARIMA for Validation 3\n",
            "41 - ARIMA with avg smape 3.03: \n",
            "Model Number: 42 of 119 with model ARIMA for Validation 3\n",
            "42 - ARIMA with avg smape 3.32: \n",
            "Model Number: 43 of 119 with model ETS for Validation 3\n",
            "ETS failed on Close with ValueError('could not broadcast input array from shape (160,) into shape (364,)')\n",
            "43 - ETS with avg smape 3.04: \n",
            "Model Number: 44 of 119 with model Theta for Validation 3\n",
            "44 - Theta with avg smape 4.71: \n",
            "Model Number: 45 of 119 with model SectionalMotif for Validation 3\n",
            "45 - SectionalMotif with avg smape 3.51: \n",
            "Model Number: 46 of 119 with model MultivariateRegression for Validation 3\n",
            "46 - MultivariateRegression with avg smape 3.27: \n",
            "Model Number: 47 of 119 with model SeasonalNaive for Validation 3\n",
            "47 - SeasonalNaive with avg smape 2.82: \n",
            "Model Number: 48 of 119 with model AverageValueNaive for Validation 3\n",
            "48 - AverageValueNaive with avg smape 2.53: \n",
            "Model Number: 49 of 119 with model Theta for Validation 3\n",
            "49 - Theta with avg smape 3.24: \n",
            "Model Number: 50 of 119 with model Theta for Validation 3\n",
            "50 - Theta with avg smape 4.19: \n",
            "Model Number: 51 of 119 with model SectionalMotif for Validation 3\n",
            "51 - SectionalMotif with avg smape 3.21: \n",
            "Model Number: 52 of 119 with model AverageValueNaive for Validation 3\n",
            "52 - AverageValueNaive with avg smape 2.48: \n",
            "Model Number: 53 of 119 with model SectionalMotif for Validation 3\n",
            "53 - SectionalMotif with avg smape 4.31: \n",
            "Model Number: 54 of 119 with model AverageValueNaive for Validation 3\n",
            "54 - AverageValueNaive with avg smape 3.14: \n",
            "Model Number: 55 of 119 with model Theta for Validation 3\n",
            "55 - Theta with avg smape 6.87: \n",
            "Model Number: 56 of 119 with model ETS for Validation 3\n",
            "56 - ETS with avg smape 2.43: \n",
            "Model Number: 57 of 119 with model AverageValueNaive for Validation 3\n",
            "57 - AverageValueNaive with avg smape 6.44: \n",
            "Model Number: 58 of 119 with model GLS for Validation 3\n",
            "58 - GLS with avg smape 3.17: \n",
            "Model Number: 59 of 119 with model Theta for Validation 3\n",
            "59 - Theta with avg smape 3.36: \n",
            "Model Number: 60 of 119 with model Theta for Validation 3\n",
            "60 - Theta with avg smape 3.25: \n",
            "Model Number: 61 of 119 with model SeasonalNaive for Validation 3\n",
            "61 - SeasonalNaive with avg smape 2.66: \n",
            "Model Number: 62 of 119 with model UnobservedComponents for Validation 3\n",
            "62 - UnobservedComponents with avg smape 3.91: \n",
            "Model Number: 63 of 119 with model SectionalMotif for Validation 3\n",
            "63 - SectionalMotif with avg smape 3.32: \n",
            "Model Number: 64 of 119 with model WindowRegression for Validation 3\n",
            "📈 64 - WindowRegression with avg smape 1.79: \n",
            "Model Number: 65 of 119 with model DatepartRegression for Validation 3\n",
            "65 - DatepartRegression with avg smape 7.01: \n",
            "Model Number: 66 of 119 with model ETS for Validation 3\n",
            "66 - ETS with avg smape 2.72: \n",
            "Model Number: 67 of 119 with model UnobservedComponents for Validation 3\n",
            "67 - UnobservedComponents with avg smape 3.17: \n",
            "Model Number: 68 of 119 with model GLS for Validation 3\n",
            "68 - GLS with avg smape 5.37: \n",
            "Model Number: 69 of 119 with model DatepartRegression for Validation 3\n",
            "Epoch 1/200\n",
            "5/5 [==============================] - 4s 31ms/step - loss: 0.1841\n",
            "Epoch 2/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0885\n",
            "Epoch 3/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0982\n",
            "Epoch 4/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0898\n",
            "Epoch 5/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0932\n",
            "Epoch 6/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0864\n",
            "Epoch 7/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0918\n",
            "Epoch 8/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0995\n",
            "Epoch 9/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0890\n",
            "Epoch 10/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0896\n",
            "Epoch 11/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0851\n",
            "Epoch 12/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0876\n",
            "Epoch 13/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0916\n",
            "Epoch 14/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0869\n",
            "Epoch 15/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0864\n",
            "Epoch 16/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0884\n",
            "Epoch 17/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0947\n",
            "Epoch 18/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0855\n",
            "Epoch 19/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0868\n",
            "Epoch 20/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0855\n",
            "Epoch 21/200\n",
            "5/5 [==============================] - 0s 37ms/step - loss: 0.0881\n",
            "Epoch 22/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0973\n",
            "Epoch 23/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0879\n",
            "Epoch 24/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0852\n",
            "Epoch 25/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0884\n",
            "Epoch 26/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0887\n",
            "Epoch 27/200\n",
            "5/5 [==============================] - 0s 35ms/step - loss: 0.0862\n",
            "Epoch 28/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0889\n",
            "Epoch 29/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0901\n",
            "Epoch 30/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0863\n",
            "Epoch 31/200\n",
            "5/5 [==============================] - 0s 35ms/step - loss: 0.0842\n",
            "Epoch 32/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0874\n",
            "Epoch 33/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0897\n",
            "Epoch 34/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0957\n",
            "Epoch 35/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0869\n",
            "Epoch 36/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0880\n",
            "Epoch 37/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0856\n",
            "Epoch 38/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0858\n",
            "Epoch 39/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0863\n",
            "Epoch 40/200\n",
            "5/5 [==============================] - 0s 39ms/step - loss: 0.0918\n",
            "Epoch 41/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0859\n",
            "Epoch 42/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0883\n",
            "Epoch 43/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0887\n",
            "Epoch 44/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0872\n",
            "Epoch 45/200\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.0875\n",
            "Epoch 46/200\n",
            "5/5 [==============================] - 0s 38ms/step - loss: 0.0833\n",
            "Epoch 47/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0865\n",
            "Epoch 48/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0899\n",
            "Epoch 49/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0894\n",
            "Epoch 50/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0870\n",
            "Epoch 51/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0867\n",
            "Epoch 52/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0848\n",
            "Epoch 53/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0906\n",
            "Epoch 54/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0873\n",
            "Epoch 55/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0905\n",
            "Epoch 56/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0847\n",
            "Epoch 57/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0836\n",
            "Epoch 58/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0932\n",
            "Epoch 59/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0910\n",
            "Epoch 60/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0896\n",
            "Epoch 61/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0847\n",
            "Epoch 62/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0878\n",
            "Epoch 63/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0905\n",
            "Epoch 64/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0844\n",
            "Epoch 65/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0855\n",
            "Epoch 66/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0902\n",
            "Epoch 67/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0857\n",
            "Epoch 68/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0847\n",
            "Epoch 69/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0847\n",
            "Epoch 70/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0869\n",
            "Epoch 71/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0846\n",
            "Epoch 72/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0917\n",
            "Epoch 73/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0849\n",
            "Epoch 74/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0843\n",
            "Epoch 75/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0878\n",
            "Epoch 76/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0834\n",
            "Epoch 77/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0856\n",
            "Epoch 78/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0814\n",
            "Epoch 79/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0839\n",
            "Epoch 80/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0826\n",
            "Epoch 81/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0815\n",
            "Epoch 82/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0834\n",
            "Epoch 83/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0789\n",
            "Epoch 84/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0759\n",
            "Epoch 85/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0770\n",
            "Epoch 86/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0719\n",
            "Epoch 87/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0707\n",
            "Epoch 88/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0915\n",
            "Epoch 89/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0640\n",
            "Epoch 90/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0556\n",
            "Epoch 91/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0696\n",
            "Epoch 92/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0524\n",
            "Epoch 93/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0580\n",
            "Epoch 94/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0665\n",
            "Epoch 95/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0547\n",
            "Epoch 96/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0571\n",
            "Epoch 97/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0565\n",
            "Epoch 98/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0569\n",
            "Epoch 99/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0426\n",
            "Epoch 100/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0710\n",
            "Epoch 101/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0468\n",
            "Epoch 102/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0464\n",
            "Epoch 103/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0491\n",
            "Epoch 104/200\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.0546\n",
            "Epoch 105/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0527\n",
            "Epoch 106/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0354\n",
            "Epoch 107/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0524\n",
            "Epoch 108/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0375\n",
            "Epoch 109/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0697\n",
            "Epoch 110/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0401\n",
            "Epoch 111/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0362\n",
            "Epoch 112/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0507\n",
            "Epoch 113/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0405\n",
            "Epoch 114/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0488\n",
            "Epoch 115/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0386\n",
            "Epoch 116/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0553\n",
            "Epoch 117/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0433\n",
            "Epoch 118/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0511\n",
            "Epoch 119/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0364\n",
            "Epoch 120/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0698\n",
            "Epoch 121/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0342\n",
            "Epoch 122/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0342\n",
            "Epoch 123/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0417\n",
            "Epoch 124/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0433\n",
            "Epoch 125/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0351\n",
            "Epoch 126/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0541\n",
            "Epoch 127/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0404\n",
            "Epoch 128/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0421\n",
            "Epoch 129/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0310\n",
            "Epoch 130/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0399\n",
            "Epoch 131/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0381\n",
            "Epoch 132/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0415\n",
            "Epoch 133/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0382\n",
            "Epoch 134/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0357\n",
            "Epoch 135/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0280\n",
            "Epoch 136/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0333\n",
            "Epoch 137/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0446\n",
            "Epoch 138/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0346\n",
            "Epoch 139/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0383\n",
            "Epoch 140/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0487\n",
            "Epoch 141/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0306\n",
            "Epoch 142/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0290\n",
            "Epoch 143/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0357\n",
            "Epoch 144/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0322\n",
            "Epoch 145/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0322\n",
            "Epoch 146/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0325\n",
            "Epoch 147/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0446\n",
            "Epoch 148/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0329\n",
            "Epoch 149/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0244\n",
            "Epoch 150/200\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.0384\n",
            "Epoch 151/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0348\n",
            "Epoch 152/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0256\n",
            "Epoch 153/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0296\n",
            "Epoch 154/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0276\n",
            "Epoch 155/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0230\n",
            "Epoch 156/200\n",
            "5/5 [==============================] - 0s 37ms/step - loss: 0.0448\n",
            "Epoch 157/200\n",
            "5/5 [==============================] - 0s 35ms/step - loss: 0.0220\n",
            "Epoch 158/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0290\n",
            "Epoch 159/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0326\n",
            "Epoch 160/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0229\n",
            "Epoch 161/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0215\n",
            "Epoch 162/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0325\n",
            "Epoch 163/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0314\n",
            "Epoch 164/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0226\n",
            "Epoch 165/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0237\n",
            "Epoch 166/200\n",
            "5/5 [==============================] - 0s 40ms/step - loss: 0.0384\n",
            "Epoch 167/200\n",
            "5/5 [==============================] - 0s 35ms/step - loss: 0.0200\n",
            "Epoch 168/200\n",
            "5/5 [==============================] - 0s 39ms/step - loss: 0.0287\n",
            "Epoch 169/200\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.0222\n",
            "Epoch 170/200\n",
            "5/5 [==============================] - 0s 37ms/step - loss: 0.0188\n",
            "Epoch 171/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0273\n",
            "Epoch 172/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0267\n",
            "Epoch 173/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0254\n",
            "Epoch 174/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0288\n",
            "Epoch 175/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0197\n",
            "Epoch 176/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0300\n",
            "Epoch 177/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0233\n",
            "Epoch 178/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0286\n",
            "Epoch 179/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0256\n",
            "Epoch 180/200\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.0203\n",
            "Epoch 181/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0198\n",
            "Epoch 182/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0281\n",
            "Epoch 183/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0177\n",
            "Epoch 184/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0399\n",
            "Epoch 185/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0176\n",
            "Epoch 186/200\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0176\n",
            "Epoch 187/200\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.0154\n",
            "Epoch 188/200\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0326\n",
            "Epoch 189/200\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0194\n",
            "Epoch 190/200\n",
            "5/5 [==============================] - 0s 48ms/step - loss: 0.0220\n",
            "Epoch 191/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0266\n",
            "Epoch 192/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0216\n",
            "Epoch 193/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0388\n",
            "Epoch 194/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0167\n",
            "Epoch 195/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0161\n",
            "Epoch 196/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0286\n",
            "Epoch 197/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0172\n",
            "Epoch 198/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0163\n",
            "Epoch 199/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0262\n",
            "Epoch 200/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0211\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "69 - DatepartRegression with avg smape 2.95: \n",
            "Model Number: 70 of 119 with model SeasonalNaive for Validation 3\n",
            "70 - SeasonalNaive with avg smape 2.43: \n",
            "Model Number: 71 of 119 with model SectionalMotif for Validation 3\n",
            "71 - SectionalMotif with avg smape 4.32: \n",
            "Model Number: 72 of 119 with model MultivariateRegression for Validation 3\n",
            "72 - MultivariateRegression with avg smape 4.68: \n",
            "Model Number: 73 of 119 with model DatepartRegression for Validation 3\n",
            "📈 73 - DatepartRegression with avg smape 1.71: \n",
            "Model Number: 74 of 119 with model MultivariateRegression for Validation 3\n",
            "74 - MultivariateRegression with avg smape 3.17: \n",
            "Model Number: 75 of 119 with model UnobservedComponents for Validation 3\n",
            "75 - UnobservedComponents with avg smape 3.41: \n",
            "Model Number: 76 of 119 with model SeasonalNaive for Validation 3\n",
            "76 - SeasonalNaive with avg smape 2.56: \n",
            "Model Number: 77 of 119 with model MultivariateRegression for Validation 3\n",
            "77 - MultivariateRegression with avg smape 3.17: \n",
            "Model Number: 78 of 119 with model UnobservedComponents for Validation 3\n",
            "78 - UnobservedComponents with avg smape 3.82: \n",
            "Model Number: 79 of 119 with model UnobservedComponents for Validation 3\n",
            "79 - UnobservedComponents with avg smape 3.82: \n",
            "Model Number: 80 of 119 with model UnobservedComponents for Validation 3\n",
            "80 - UnobservedComponents with avg smape 3.17: \n",
            "Model Number: 81 of 119 with model WindowRegression for Validation 3\n",
            "81 - WindowRegression with avg smape 3.17: \n",
            "Model Number: 82 of 119 with model WindowRegression for Validation 3\n",
            "82 - WindowRegression with avg smape 2.79: \n",
            "Model Number: 83 of 119 with model MultivariateRegression for Validation 3\n",
            "83 - MultivariateRegression with avg smape 3.17: \n",
            "Model Number: 84 of 119 with model WindowRegression for Validation 3\n",
            "84 - WindowRegression with avg smape 3.05: \n",
            "Model Number: 85 of 119 with model MetricMotif for Validation 3\n",
            "85 - MetricMotif with avg smape 3.84: \n",
            "Model Number: 86 of 119 with model GLS for Validation 3\n",
            "86 - GLS with avg smape 2.54: \n",
            "Model Number: 87 of 119 with model ETS for Validation 3\n",
            "87 - ETS with avg smape 3.24: \n",
            "Model Number: 88 of 119 with model GLS for Validation 3\n",
            "88 - GLS with avg smape 4.31: \n",
            "Model Number: 89 of 119 with model UnivariateRegression for Validation 3\n",
            "89 - UnivariateRegression with avg smape 5.26: \n",
            "Model Number: 90 of 119 with model ETS for Validation 3\n",
            "90 - ETS with avg smape 1.96: \n",
            "Model Number: 91 of 119 with model SeasonalNaive for Validation 3\n",
            "91 - SeasonalNaive with avg smape 2.06: \n",
            "Model Number: 92 of 119 with model GLS for Validation 3\n",
            "92 - GLS with avg smape 1.96: \n",
            "Model Number: 93 of 119 with model MetricMotif for Validation 3\n",
            "93 - MetricMotif with avg smape 6.73: \n",
            "Model Number: 94 of 119 with model WindowRegression for Validation 3\n",
            "94 - WindowRegression with avg smape 4.31: \n",
            "Model Number: 95 of 119 with model GLM for Validation 3\n",
            "95 - GLM with avg smape 2.24: \n",
            "Model Number: 96 of 119 with model DatepartRegression for Validation 3\n",
            "Epoch 1/200\n",
            "5/5 [==============================] - 5s 30ms/step - loss: 0.3513\n",
            "Epoch 2/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.3380\n",
            "Epoch 3/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.3402\n",
            "Epoch 4/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.3376\n",
            "Epoch 5/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.3378\n",
            "Epoch 6/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.3395\n",
            "Epoch 7/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.3382\n",
            "Epoch 8/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.3389\n",
            "Epoch 9/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.3378\n",
            "Epoch 10/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.3379\n",
            "Epoch 11/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.3381\n",
            "Epoch 12/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.3383\n",
            "Epoch 13/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.3385\n",
            "Epoch 14/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.3372\n",
            "Epoch 15/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.3380\n",
            "Epoch 16/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.3380\n",
            "Epoch 17/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.3383\n",
            "Epoch 18/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.3375\n",
            "Epoch 19/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.3380\n",
            "Epoch 20/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.3379\n",
            "Epoch 21/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.3381\n",
            "Epoch 22/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.3400\n",
            "Epoch 23/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.3381\n",
            "Epoch 24/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.3372\n",
            "Epoch 25/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.3378\n",
            "Epoch 26/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.3380\n",
            "Epoch 27/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.3376\n",
            "Epoch 28/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.3377\n",
            "Epoch 29/200\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.3375\n",
            "Epoch 30/200\n",
            "5/5 [==============================] - 0s 38ms/step - loss: 0.3379\n",
            "Epoch 31/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.3383\n",
            "Epoch 32/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.3379\n",
            "Epoch 33/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.3380\n",
            "Epoch 34/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.3385\n",
            "Epoch 35/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.3379\n",
            "Epoch 36/200\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.3373\n",
            "Epoch 37/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.3374\n",
            "Epoch 38/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.3375\n",
            "Epoch 39/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.3376\n",
            "Epoch 40/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.3379\n",
            "Epoch 41/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.3372\n",
            "Epoch 42/200\n",
            "5/5 [==============================] - 0s 35ms/step - loss: 0.3382\n",
            "Epoch 43/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.3386\n",
            "Epoch 44/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.3374\n",
            "Epoch 45/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.3377\n",
            "Epoch 46/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.3380\n",
            "Epoch 47/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.3372\n",
            "Epoch 48/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.3376\n",
            "Epoch 49/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.3381\n",
            "Epoch 50/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.3381\n",
            "Epoch 51/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.3376\n",
            "Epoch 52/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.3370\n",
            "Epoch 53/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.3385\n",
            "Epoch 54/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.3370\n",
            "Epoch 55/200\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.3378\n",
            "Epoch 56/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.3387\n",
            "Epoch 57/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.3365\n",
            "Epoch 58/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.3357\n",
            "Epoch 59/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.3361\n",
            "Epoch 60/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.3332\n",
            "Epoch 61/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.3227\n",
            "Epoch 62/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.3189\n",
            "Epoch 63/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.3100\n",
            "Epoch 64/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.2619\n",
            "Epoch 65/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.2594\n",
            "Epoch 66/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.2154\n",
            "Epoch 67/200\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.1884\n",
            "Epoch 68/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.2107\n",
            "Epoch 69/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.1723\n",
            "Epoch 70/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.1846\n",
            "Epoch 71/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.1605\n",
            "Epoch 72/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.1603\n",
            "Epoch 73/200\n",
            "5/5 [==============================] - 0s 38ms/step - loss: 0.1598\n",
            "Epoch 74/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.1424\n",
            "Epoch 75/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.2000\n",
            "Epoch 76/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.1283\n",
            "Epoch 77/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.1253\n",
            "Epoch 78/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.1490\n",
            "Epoch 79/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.1364\n",
            "Epoch 80/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.1153\n",
            "Epoch 81/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.1281\n",
            "Epoch 82/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.1616\n",
            "Epoch 83/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.1282\n",
            "Epoch 84/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.1159\n",
            "Epoch 85/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.1354\n",
            "Epoch 86/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.1204\n",
            "Epoch 87/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.1099\n",
            "Epoch 88/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.1161\n",
            "Epoch 89/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.1239\n",
            "Epoch 90/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.1071\n",
            "Epoch 91/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.1550\n",
            "Epoch 92/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.1027\n",
            "Epoch 93/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.1013\n",
            "Epoch 94/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.1075\n",
            "Epoch 95/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.1151\n",
            "Epoch 96/200\n",
            "5/5 [==============================] - 0s 37ms/step - loss: 0.0985\n",
            "Epoch 97/200\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.1198\n",
            "Epoch 98/200\n",
            "5/5 [==============================] - 0s 35ms/step - loss: 0.1021\n",
            "Epoch 99/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.1052\n",
            "Epoch 100/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.1025\n",
            "Epoch 101/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0870\n",
            "Epoch 102/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.1205\n",
            "Epoch 103/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0929\n",
            "Epoch 104/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.1006\n",
            "Epoch 105/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0983\n",
            "Epoch 106/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.1007\n",
            "Epoch 107/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.1150\n",
            "Epoch 108/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0942\n",
            "Epoch 109/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0977\n",
            "Epoch 110/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0959\n",
            "Epoch 111/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0870\n",
            "Epoch 112/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.1186\n",
            "Epoch 113/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0831\n",
            "Epoch 114/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.1315\n",
            "Epoch 115/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.1011\n",
            "Epoch 116/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0997\n",
            "Epoch 117/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0859\n",
            "Epoch 118/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.1114\n",
            "Epoch 119/200\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.0755\n",
            "Epoch 120/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.1178\n",
            "Epoch 121/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0847\n",
            "Epoch 122/200\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.0843\n",
            "Epoch 123/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0795\n",
            "Epoch 124/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0958\n",
            "Epoch 125/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0693\n",
            "Epoch 126/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0802\n",
            "Epoch 127/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.1022\n",
            "Epoch 128/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0939\n",
            "Epoch 129/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0721\n",
            "Epoch 130/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.1029\n",
            "Epoch 131/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0691\n",
            "Epoch 132/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.1119\n",
            "Epoch 133/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0806\n",
            "Epoch 134/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0724\n",
            "Epoch 135/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0672\n",
            "Epoch 136/200\n",
            "5/5 [==============================] - 0s 35ms/step - loss: 0.0681\n",
            "Epoch 137/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0596\n",
            "Epoch 138/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0955\n",
            "Epoch 139/200\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.0649\n",
            "Epoch 140/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0768\n",
            "Epoch 141/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0657\n",
            "Epoch 142/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0560\n",
            "Epoch 143/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0761\n",
            "Epoch 144/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0668\n",
            "Epoch 145/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0663\n",
            "Epoch 146/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0560\n",
            "Epoch 147/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0640\n",
            "Epoch 148/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0934\n",
            "Epoch 149/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0645\n",
            "Epoch 150/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0706\n",
            "Epoch 151/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0611\n",
            "Epoch 152/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0538\n",
            "Epoch 153/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0757\n",
            "Epoch 154/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0503\n",
            "Epoch 155/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0649\n",
            "Epoch 156/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0575\n",
            "Epoch 157/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0619\n",
            "Epoch 158/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0662\n",
            "Epoch 159/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0483\n",
            "Epoch 160/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0708\n",
            "Epoch 161/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0492\n",
            "Epoch 162/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0682\n",
            "Epoch 163/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0681\n",
            "Epoch 164/200\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.0499\n",
            "Epoch 165/200\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.0574\n",
            "Epoch 166/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0715\n",
            "Epoch 167/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0485\n",
            "Epoch 168/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0732\n",
            "Epoch 169/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0511\n",
            "Epoch 170/200\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.0609\n",
            "Epoch 171/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0541\n",
            "Epoch 172/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0415\n",
            "Epoch 173/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0859\n",
            "Epoch 174/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0449\n",
            "Epoch 175/200\n",
            "5/5 [==============================] - 0s 35ms/step - loss: 0.0548\n",
            "Epoch 176/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0622\n",
            "Epoch 177/200\n",
            "5/5 [==============================] - 0s 35ms/step - loss: 0.0490\n",
            "Epoch 178/200\n",
            "5/5 [==============================] - 0s 37ms/step - loss: 0.0703\n",
            "Epoch 179/200\n",
            "5/5 [==============================] - 0s 107ms/step - loss: 0.0481\n",
            "Epoch 180/200\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.0464\n",
            "Epoch 181/200\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 0.0454\n",
            "Epoch 182/200\n",
            "5/5 [==============================] - 0s 49ms/step - loss: 0.0783\n",
            "Epoch 183/200\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.0466\n",
            "Epoch 184/200\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.0421\n",
            "Epoch 185/200\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.0404\n",
            "Epoch 186/200\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.0505\n",
            "Epoch 187/200\n",
            "5/5 [==============================] - 0s 47ms/step - loss: 0.0450\n",
            "Epoch 188/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0366\n",
            "Epoch 189/200\n",
            "5/5 [==============================] - 0s 37ms/step - loss: 0.0469\n",
            "Epoch 190/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0277\n",
            "Epoch 191/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0518\n",
            "Epoch 192/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0314\n",
            "Epoch 193/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0500\n",
            "Epoch 194/200\n",
            "5/5 [==============================] - 0s 35ms/step - loss: 0.0269\n",
            "Epoch 195/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0283\n",
            "Epoch 196/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0761\n",
            "Epoch 197/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0280\n",
            "Epoch 198/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0319\n",
            "Epoch 199/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0283\n",
            "Epoch 200/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0275\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "96 - DatepartRegression with avg smape 2.88: \n",
            "Model Number: 97 of 119 with model GLM for Validation 3\n",
            "97 - GLM with avg smape 3.29: \n",
            "Model Number: 98 of 119 with model DatepartRegression for Validation 3\n",
            "98 - DatepartRegression with avg smape 3.14: \n",
            "Model Number: 99 of 119 with model GLM for Validation 3\n",
            "99 - GLM with avg smape 2.87: \n",
            "Model Number: 100 of 119 with model GLS for Validation 3\n",
            "100 - GLS with avg smape 3.78: \n",
            "Model Number: 101 of 119 with model MetricMotif for Validation 3\n",
            "101 - MetricMotif with avg smape 6.32: \n",
            "Model Number: 102 of 119 with model DatepartRegression for Validation 3\n",
            "Epoch 1/50\n",
            "5/5 [==============================] - 5s 11ms/step - loss: nan\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.4814\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.4536\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.3328\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.3992\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.4268\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.4348\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.2719\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.2760\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.2824\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.3502\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.2234\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.3549\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.3453\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.3438\n",
            "Epoch 17/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.2721\n",
            "Epoch 18/50\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.2760\n",
            "Epoch 19/50\n",
            "5/5 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 20/50\n",
            "5/5 [==============================] - 0s 10ms/step - loss: nan\n",
            "Epoch 21/50\n",
            "5/5 [==============================] - 0s 10ms/step - loss: nan\n",
            "Epoch 22/50\n",
            "5/5 [==============================] - 0s 10ms/step - loss: nan\n",
            "Epoch 23/50\n",
            "5/5 [==============================] - 0s 10ms/step - loss: nan\n",
            "Epoch 24/50\n",
            "5/5 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 25/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 26/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 27/50\n",
            "5/5 [==============================] - 0s 10ms/step - loss: nan\n",
            "Epoch 28/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 29/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 30/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 31/50\n",
            "5/5 [==============================] - 0s 10ms/step - loss: nan\n",
            "Epoch 32/50\n",
            "5/5 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 33/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 34/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 35/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 36/50\n",
            "5/5 [==============================] - 0s 10ms/step - loss: nan\n",
            "Epoch 37/50\n",
            "5/5 [==============================] - 0s 10ms/step - loss: nan\n",
            "Epoch 38/50\n",
            "5/5 [==============================] - 0s 10ms/step - loss: nan\n",
            "Epoch 39/50\n",
            "5/5 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 40/50\n",
            "5/5 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 41/50\n",
            "5/5 [==============================] - 0s 10ms/step - loss: nan\n",
            "Epoch 42/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 43/50\n",
            "5/5 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 44/50\n",
            "5/5 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 45/50\n",
            "5/5 [==============================] - 0s 10ms/step - loss: nan\n",
            "Epoch 46/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 47/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 48/50\n",
            "5/5 [==============================] - 0s 14ms/step - loss: nan\n",
            "Epoch 49/50\n",
            "5/5 [==============================] - 0s 12ms/step - loss: nan\n",
            "Epoch 50/50\n",
            "5/5 [==============================] - 0s 10ms/step - loss: nan\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "102 - DatepartRegression with avg smape 3.02: \n",
            "Model Number: 103 of 119 with model ConstantNaive for Validation 3\n",
            "103 - ConstantNaive with avg smape 3.14: \n",
            "Model Number: 104 of 119 with model ConstantNaive for Validation 3\n",
            "104 - ConstantNaive with avg smape 2.61: \n",
            "Model Number: 105 of 119 with model MetricMotif for Validation 3\n",
            "105 - MetricMotif with avg smape 6.1: \n",
            "Model Number: 106 of 119 with model FBProphet for Validation 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/p7lywy1b.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/kwxxmsxd.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=14342', 'data', 'file=/tmp/tmp11_dwuur/p7lywy1b.json', 'init=/tmp/tmp11_dwuur/kwxxmsxd.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_model7kpzffjn/prophet_model-20221223064625.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "06:46:25 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "06:46:25 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/l0ar4mfi.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/4no0gxy_.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=20484', 'data', 'file=/tmp/tmp11_dwuur/l0ar4mfi.json', 'init=/tmp/tmp11_dwuur/4no0gxy_.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_modely1yazu26/prophet_model-20221223064626.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "06:46:26 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "06:46:26 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "106 - FBProphet with avg smape 5.49: \n",
            "Model Number: 107 of 119 with model FBProphet for Validation 3\n",
            "107 - FBProphet with avg smape 3.17: \n",
            "Model Number: 108 of 119 with model WindowRegression for Validation 3\n",
            "108 - WindowRegression with avg smape 2.46: \n",
            "Model Number: 109 of 119 with model GLM for Validation 3\n",
            "109 - GLM with avg smape 2.81: \n",
            "Model Number: 110 of 119 with model MetricMotif for Validation 3\n",
            "110 - MetricMotif with avg smape 3.17: \n",
            "Model Number: 111 of 119 with model GLM for Validation 3\n",
            "111 - GLM with avg smape 1.94: \n",
            "Model Number: 112 of 119 with model MetricMotif for Validation 3\n",
            "112 - MetricMotif with avg smape 7.55: \n",
            "Model Number: 113 of 119 with model ConstantNaive for Validation 3\n",
            "113 - ConstantNaive with avg smape 4.31: \n",
            "Model Number: 114 of 119 with model GLM for Validation 3\n",
            "114 - GLM with avg smape 7.01: \n",
            "Model Number: 115 of 119 with model ConstantNaive for Validation 3\n",
            "115 - ConstantNaive with avg smape 2.25: \n",
            "Model Number: 116 of 119 with model ConstantNaive for Validation 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "116 - ConstantNaive with avg smape 2.37: \n",
            "Model Number: 117 of 119 with model FBProphet for Validation 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/6hvxg7f5.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/7h0abs96.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=80450', 'data', 'file=/tmp/tmp11_dwuur/6hvxg7f5.json', 'init=/tmp/tmp11_dwuur/7h0abs96.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_model_l73tgj_/prophet_model-20221223064627.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "06:46:27 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "06:46:28 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "117 - FBProphet with avg smape 7.8: \n",
            "Model Number: 118 of 119 with model ConstantNaive for Validation 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/yc6vm405.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp11_dwuur/u3wmthue.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=60945', 'data', 'file=/tmp/tmp11_dwuur/yc6vm405.json', 'init=/tmp/tmp11_dwuur/u3wmthue.json', 'output', 'file=/tmp/tmp11_dwuur/prophet_modelz_c_lacv/prophet_model-20221223064629.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "06:46:29 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "06:46:29 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "118 - ConstantNaive with avg smape 4.93: \n",
            "Model Number: 119 of 119 with model FBProphet for Validation 3\n",
            "119 - FBProphet with avg smape 2.89: \n"
          ]
        }
      ],
      "source": [
        "model = AutoTS(forecast_length=10, frequency='infer', \n",
        "               ensemble='simple', drop_data_older_than_periods=200,max_generations=5,validation_method=\"backwards\")\n",
        "model = model.fit(df1, date_col='Date', value_col='Close', id_col=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sByPu2zIWvoG",
        "outputId": "06016cac-9f3f-44a8-ac4c-472e45e9b11a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stock Price Prediction of Apple\n",
            "                 Close\n",
            "2022-05-02  162.368367\n",
            "2022-05-03  161.306558\n",
            "2022-05-04  160.354943\n",
            "2022-05-05  159.655655\n",
            "2022-05-06  159.200251\n",
            "2022-05-09  158.904396\n",
            "2022-05-10  158.748387\n",
            "2022-05-11  158.751987\n",
            "2022-05-12  158.958218\n",
            "2022-05-13  159.384752\n"
          ]
        }
      ],
      "source": [
        "prediction = model.predict()\n",
        "forecast = prediction.forecast\n",
        "print(\"Stock Price Prediction of Apple\")\n",
        "print(forecast)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YodB2YNkxW9Q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
